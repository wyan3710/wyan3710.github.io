[{"title":"MySQL存储引擎如何选择","url":"/2019/12/31/pout/数据库/mysql/修改MySQL引擎/","content":"\n#### 定义以及作用\n\n> 数据库引擎是用于存储、处理和保护数据的核心服务。\n>\n> 利用数据库引擎可控制访问权限并快速处理事务，从而满足企业内大多数需要处理大量数据的应用程序的要求。 \n>\n> 使用数据库引擎创建用于联机(客户端与服务端能够实时通信。由客户机发起，直到服务器确认。)事务处理或联机分析处理数据的关系数据库。这包括创建用于存储数据的表和用于查看、管理和保护数据安全的数据库对象（如索引、视图和存储过程）。\n\n\n\n\n#### Mysql的存储引擎有哪些：\n\n> InnoDB\n>>\n>> 这是MySQL 5.5或更高版本的默认存储引擎。它提供了事务安全(ACID兼容)表，支持外键引用完整性约束。它支持提交、回滚和紧急恢复功能来保护数据。它还支持行级锁定。当在多用户环境中使用时，它的“一致非锁定读取”提高了性能。它将数据存储在集群索引中，从而减少了基于主键的查询的I/O。\n\n> MyISAM\n>>\n>> 该存储引擎管理非事务性表，提供高速存储和检索，支持全文搜索。\n\n> MEMORY\n>>\n>> 提供内存中的表，以前称为堆。它在RAM中处理所有数据，以便比在磁盘上存储数据更快地访问。用于快速查找引用和其他相同的数据。\n\n\n#### 修改数据库引擎\n\n> 方式壹：\n>> \n>> 修改配置文件my.ini\n>>\n>> 将mysql.ini另存为my.ini，在[mysqld]后面添加default-storage-engine=Innodb,重启服务，数据库默认的引擎修改为Innodb\n\n> 方式贰：\n>> \n>> 在建表得时候指定\n>>\n>> create table table_name(你的各个字段名)type=MyISAM;\n\n> 方式叁：\n>> \n>> 建表后更改\n>>\n>> alert table table_name type=Innodb;\n\n#### 如何查看是否修改成功(查看当前数据库的引擎)\n\n> 方式壹：\n>>\n>> show table status from table_name;\n\n> 方拾贰：\n>>\n>> show create table table_name;\n\n> 方式叁：\n>>\n>> 使用数据库管理工具(具体自己去问度娘)\n注意：不同版本之间有可能命令有些不同\n\n#### MyISAM、InnoDB和MEMORY引擎之间的区别:\n\n> InnoDB存储引擎\n>>\n>> InnoDB给MySQL的表提供了事务处理、回滚、崩溃修复能力和多版本并发控制的事务安全。在MySQL从3.23.34a开始包含InnnoDB。它是MySQL上第一个提供外键约束的表引擎。而且InnoDB对事务处理的能力，也是其他存储引擎不能比拟的。靠后版本的MySQL的默认存储引擎就是InnoDB。\n>>\n>>InnoDB存储引擎总支持AUTO_INCREMENT。自动增长列的值不能为空，并且值必须唯一。MySQL中规定自增列必须为主键。在插入值的时候，如果自动增长列不输入值，则插入的值为自动增长后的值；如果输入的值为0或空（NULL），则插入的值也是自动增长后的值；如果插入某个确定的值，且该值在前面没有出现过，就可以直接插入。\n>>\n>>InnoDB还支持外键（FOREIGN KEY）。外键所在的表叫做子表，外键所依赖（REFERENCES）的表叫做父表。父表中被字表外键关联的字段必须为主键。当删除、更新父表中的某条信息时，子表也必须有相应的改变，这是数据库的参照完整性规则。\n>>\n>> InnoDB中，创建的表的表结构存储在.frm文件中（我觉得是frame的缩写吧）。数据和索引存储在innodb_data_home_dir和innodb_data_file_path定义的表空间中。\n>>\n>> InnoDB的优势在于提供了良好的事务处理、崩溃修复能力和并发控制。缺点是读写效率较差，占用的数据空间相对较大。\n\n> MyISAM存储引擎\n>>\n>> MyISAM是MySQL中常见的存储引擎，曾经是MySQL的默认存储引擎。MyISAM是基于ISAM引擎发展起来的，增加了许多有用的扩展。\n>> \n>> MyISAM的表存储成3个文件。文件的名字与表名相同。拓展名为frm、MYD、MYI。其实，frm文件存储表的结构；MYD文件存储数据，是MYData的缩写；MYI文件存储索引，是MYIndex的缩写。\n>>\n>> 基于MyISAM存储引擎的表支持3种不同的存储格式。包括静态型、动态型和压缩型。其中，静态型是MyISAM的默认存储格式，它的字段是固定长度的；动态型包含变长字段，记录的长度不是固定的；压缩型需要用到myisampack工具，占用的磁盘空间较小。\n>>\n>> MyISAM的优势在于占用空间小，处理速度快。缺点是不支持事务的完整性和并发性。\n\n\n> MEMORY存储引擎\n>>\n>> MEMORY是MySQL中一类特殊的存储引擎。它使用存储在内存中的内容来创建表，而且数据全部放在内存中。这些特性与前面的两个很不同。\n>>\n>> 每个基于MEMORY存储引擎的表实际对应一个磁盘文件。该文件的文件名与表名相同，类型为frm类型。该文件中只存储表的结构。而其数据文件，都是存储在内存中，这样有利于数据的快速处理，提高整个表的效率。值得注意的是，服务器需要有足够的内存来维持MEMORY存储引擎的表的使用。如果不需要了，可以释放内存，甚至删除不需要的表。\n>>\n>> MEMORY默认使用哈希索引。速度比使用B型树索引快。当然如果你想用B型树索引，可以在创建索引时指定。\n>>\n>> 注意，MEMORY用到的很少，因为它是把数据存到内存中，如果内存出现异常就会影响数据。如果重启或者关机，所有数据都会消失。因此，基于MEMORY的表的生命周期很短，一般是一次性的\n\n#### 怎样选择合理的存储引擎\n\n> InnoDB：支持事务处理，支持外键，支持崩溃修复能力和并发控制。如果需要对事务的完整性要求比较高（比如银行），要求实现并发控制（比如售票），那选择InnoDB有很大的优势。如果需要频繁的更新、删除操作的数据库，也可以选择InnoDB，因为支持事务的提交（commit）和回滚（rollback）。 \n>\n> MyISAM：插入数据快，空间和内存使用比较低。如果表主要是用于插入新记录和读出记录，那么选择MyISAM能实现处理高效率。如果应用的完整性、并发性要求比 较低，也可以使用。\n>\n> MEMORY：所有的数据都在内存中，数据的处理速度快，但是安全性不高。如果需要很快的读写速度，对数据的安全性要求较低，可以选择MEMOEY。它对表的大小有要求，不能建立太大的表。所以，这类数据库只使用在相对较小的数据库表。\n>\n> 注意，同一个数据库也可以使用多种存储引擎的表。如果一个表要求比较高的事务处理，可以选择InnoDB。这个数据库中可以将查询要求比较高的表选择MyISAM存储。如果该数据库需要一个用于查询的临时表，可以选择MEMORY存储引擎。\n","tags":["数据库"]},{"title":"高阶函数","url":"/2019/12/24/pout/python中高级面试题/高阶函数/","content":"\n\n<!-- toc -->\n\n# 高阶函数\n\n## 1、map\n\n一般情况map()函数接收两个参数，一个函数（该函数接收一个参数），一个序列，将传入的函数依次作用到序列的每个元素，并返回一个新的Iterator（迭代器）。 例如有这样一个list：['pYthon', 'jaVa', 'kOtlin'],现在要把list中每个元素首字母改为大写，其它的改为小写，可以这样操作：\n\n```\n>>> def f(s):\n...    return s.title()\n... \n>>> l = map(f, ['pYthon', 'jaVa', 'kOtlin'])\n>>> list(l)\n['Python', 'Java', 'Kotlin']\n```\n\n## 2、reduce\n\n和map()用法类似，reduce把传入的函数作用在一个序列上，但传入的函数需要接收两个参数，传入函数的计算结果继续和序列的下一个元素做累积计算。\n\n例如有一个list，里边的元素都是字符串，要把它拼接成一个字符串：\n\n```\n>>> from functools import reduce\n>>> def f(x, y):\n...    return x + y\n... \n>>> reduce(f, ['ab', 'c', 'de', 'f'])\n'abcdef'\n```\n\n## 3、filter\n\nfilter()同样接收一个函数和一个序列，然后把传入的函数依次作用于序列的每个元素，如果传入的函数返回true则保留元素，否则丢弃，最终返回一个Iterator。\n\n例如一个list中元素有纯字母、纯数字、字母数字组合的，我们要保留纯字母的：\n\n```\n>>> def f(s):\n...    return s.isalpha()\n... \n>>> l = filter(f, ['abc', 'xyz', '123kg', '666'])\n>>> list(l)\n['abc', 'xyz']\n```\n\n## 4、sorted\n\nsorted()函数就是用来排序的，同时可以自己定义排序的规则。\n\n```\n>>> sorted([6, -2, 4, -1])\n[-2, -1, 4, 6]\n\n>>> sorted([6, -2, 4, -1], key=abs)\n[-1, -2, 4, 6]\n\n>>> sorted([6, -2, 4, -1], key=abs, reverse=True)\n[6, 4, -2, -1]\n\n>>> sorted(['Windows', 'iOS', 'Android'])\n['Android', 'Windows', 'iOS']\n\n>>> d = [('Tom', 170), ('Jim', 175), ('Andy', 168), ('Bob', 185)]\n>>> def by_height(t):\n...     return t[1]\n... \n>>> sorted(d, key=by_height)\n [('Andy', 168), ('Tom', 170), ('Jim', 175),  ('Bob', 185)]\n```\n","tags":["python中高级面试题"]},{"title":"Linux四剑客","url":"/2019/12/23/pout/Linux/Linux四剑客/","content":"\n\n<!-- toc -->\n\n\n## Find\n\n> 查询命令，就是`find`\n\n```\nusage: find [-H | -L | -P] [-EXdsx] [-f path] path ... [expression]\n       find [-H | -L | -P] [-EXdsx] -f path [path ...] [expression]\n```\n\n- 常用参数\n\n```\n-name   \tfile\t\t    \t\t\t\t#查找名为file的文件；\n-type   \tb/d/c/p/l/f\t\t\t    #查是块设备、目录、字符设备、管道、符号链接、普通文件；\n-size     n[c]     \t\t    \t\t#查长度为n块[或n字节]的文件；\n-perm               \t\t\t\t\t#按执行权限来查找；\n-user    \tusername   \t\t\t\t\t#按文件属主来查找；\n-group   \tgroupname  \t\t\t\t\t#按组来查找；\n-mtime    -n +n     \t\t\t\t\t#按文件更改时间来查找文件，-n指n天以内，+n指n天以前；\n-atime    -n +n     \t\t\t    #按文件访问时间来查找文件；\n-ctime    -n +n     \t\t\t    #按文件创建时间来查找文件；\n-mmin     -n +n     \t\t\t    #按文件更改时间来查找文件，-n指n分钟以内，+n指n分钟以前；\n-amin     -n +n     \t\t\t    #按文件访问时间来查找文件；\n-cmin     -n +n     \t\t\t    #按文件创建时间来查找文件；\n-nogroup            \t\t\t    #查无有效属组的文件；\n-nouser             \t\t\t    #查无有效属主的文件；\n-newer   \tf1 !f2     \t\t\t    #找文件，-n指n天以内，+n指n天以前；\n-depth              \t\t\t    #使查找在进入子目录前先行查找完本目录；\n-fstype             \t\t\t    #查更改时间比f1新但比f2旧的文件；\n-mount              \t\t\t    #查文件时不跨越文件系统mount点；\n-follow             \t\t\t    #如果遇到符号链接文件，就跟踪链接所指的文件；\n-cpio              \t\t\t\t\t\t#查位于某一类型文件系统中的文件；\n-prune              \t\t\t    #忽略某个目录；\n-maxdepth\t\t\t\t\t\t\t\t\t\t\t#查找目录级别深度\n```\n\n- `-name`\n\n```\nfind /home/ -name 'file'\n# 查询home目录下文件名为file的文件\nfind /home/ -name '[a-z]*'\n# 查找home目录下文件名以小写字母开头的文件\n```\n\n- `-type`\n\n```\nfind /home/ -type d \n# 查找home目录下的目录\nfind /home/ ! -type d\n# 查找home目录下的非目录\nfind /home -type d | xargs chmod 755 -R\n# 查找home目录下的目录，并将这个目录里的文件的权限设置为755\n```\n\n- `-size`\n\n```\nfind /home/ -size +1K\n# 查找home目录下文件大于1K的文件\nfind /home/ -size -1M\n# 查找home目录下文件小于1M的文件\nfind /home/ -size 10K\n# 查找home目录下文件等于10K的文件\n```\n\n- `-perm`\n\n```\nfind /home/ -perm -775\n# 减号代表的意思是 比当前775更充足的权限 所有1的部分必须被匹配\nfind /home/ -perm +001\n# 加号代表的意思是 只要有权限位置符合查询条件即可\nfind /home/ -perm 644\n# 查询 644的文件及目录\n```\n\n## Grep\n\n> `Global search regular expression`\n\n```\nusage: grep [-abcDEFGHhIiJLlmnOoqRSsUVvwxZ] [-A num] [-B num] [-C[num]]\n\t[-e pattern] [-f file] [--binary-files=value] [--color=when]\n\t[--context[=num]] [--directories=action] [--label] [--line-buffered]\n\t[--null] [pattern] [file ...]\n```\n\n- 常用参数\n\n```\n-a \t\t\t\t\t\t# 以文本文件方式搜索；\n-c \t\t\t\t\t\t# 计算找到的符合行的次数；\n-i \t\t\t\t\t\t# 忽略大小写；\n-n \t\t\t\t\t\t# 顺便输出行号；\n-v \t\t\t\t\t\t# 反向选择，即显示不包含匹配文本的所有行；\n-h \t\t\t\t\t\t# 查询多文件时不显示文件名；\n-l \t\t\t\t\t\t# 查询多文件时只输出包含匹配字符的文件名；\n-s \t\t\t\t\t\t# 不显示不存在或无匹配文本的错误信息；\n-E \t\t\t\t\t\t# 允许使用egrep扩展模式匹配。\n```\n\n- 常用通配符\n\n```\n*\t\t\t\t\t\t\t# 0个或者多个字符、数字；\n?\t\t\t\t\t\t\t# 匹配任意一个字符；\n#\t\t\t\t\t\t\t# 表示注解；\n|\t\t\t\t\t\t\t# 管道符号；\n;\t\t\t\t\t\t\t# 多个命令连续执行；\n&\t\t\t\t\t\t\t# 后台运行指令；\n!\t\t\t\t\t\t\t# 逻辑运算非；\n[]\t\t\t\t\t\t# 内容范围，匹配括号中内容；\n{}\t\t\t\t\t\t# 命令块，多个命令匹配\n```\n\n- 正则表达式\n\n```\n*\t\t\t\t\t\t\t# 前一个字符匹配0次或多次\n+\t\t\t\t\t\t\t# 前面的正则表达式1次或多次\n?\t\t\t\t\t\t\t# 前面的正则表达式出现0次或多次\n```\n\n```\n^\t\t\t\t\t\t\t# 匹配行首，即以某个字符开头；\n$\t\t\t\t\t\t\t# 匹配行尾，即以某个字符结尾；\n\\(..\\) \t\t\t\t# 标记匹配字符；\n[]\t\t\t\t\t\t# 匹配中括号里的任意指定字符，但只匹配一个字符；\n[^]\t\t\t\t\t\t# 匹配除中括号以外的任意一个字符；\n\\\t\t\t\t\t\t\t# 转义符，取消特殊含义；\n\\< \t\t\t\t\t\t# 锚定单词的开始；\n\\> \t\t\t\t\t\t# 锚定单词的结束；\n{n}\t\t\t\t\t\t# 匹配字符出现n次；\n{n,}\t\t\t\t\t# 匹配字符出现大于等于n次；\n{n,m}\t\t\t\t\t# 匹配字符至少出现n次，最多出现m次；\n\\w \t\t\t\t\t\t# 匹配文字和数字字符；\n\\W \t\t\t\t\t\t# \\w的反置形式，匹配一个或多个非单词字符；\n\\b \t\t\t\t\t\t# 单词锁定符；\n\\s\t\t\t\t\t\t# 匹配任何空白字符；\n\\d\t\t\t\t\t\t# 匹配一个数字字符，等价于[0-9]\n```\n\n- 练一个\n\n```\n# 查询文件ip地址\n```\n\n## Awk\n\n> `AWK`是一个优良的文本处理工具，[Linux](http://baike.baidu.com/item/Linux)及[Unix](http://baike.baidu.com/item/Unix)环境中现有的功能最强大的数据处理引擎之一\n>\n> 以`Aho`、`Weinberger`、`Kernighan`三位发明者名字首字母命名为`AWK`，`AWK`是一个行级文本高效处理工具\n>\n> `AWK`经过改进生成的新的版本有`Nawk`、`Gawk`，一般Linux默认为`Gawk`，`Gawk`是 `AWK`的`GNU`开源免费版本，也就是我们现在所使用的版本\n\n> `AWK`基本原理是逐行处理文件中的数据，查找与命令行中所给定内容相匹配的模式\n>\n> 如果发现匹配内容，则进行下一个编程步骤，如果找不到匹配内容，则 继续处理下一行\n\n```\nusage: awk [-F fs] [-v var=value] [-f progfile | 'prog'] [file ...]\n```\n\n- 内置变量\n\n| 变量    | 解释                    |\n| ------- | ----------------------- |\n| `FS`    | 分隔符                  |\n| `OFS`   | 输出分隔符              |\n| `NR`    | 当前行数，从`-1`开始    |\n| `NF`    | 当前记录字段个数        |\n| `$0`    | 当前记录                |\n| `$1~$n` | 当前记录第n个字段（列） |\n\n- 内置函数\n\n| 函数              | 解释                     |\n| ----------------- | ------------------------ |\n| `gsub(r, s)`      | 在`$0`中用`s`代替`r`     |\n| `index(s, t)`     | 返回`s`中`t`的第一个位置 |\n| `length(s)`       | `s`的长度                |\n| `match(s, r)`     | `s`是否匹配`r`           |\n| `split(s, a, fs)` | 在`fs`                   |\n| `substr(s, p`     | 返回`s`从`p`开始的子串   |\n\n- 操作符\n\n| 操作符                          | 解释            |\n| ------------------------------- | --------------- |\n| ++、–                           | 增加或减少      |\n| ^、**                           | 指数            |\n| !、+、-                         | 非、一元加减    |\n| +、-、*、/、%、                 | 四则运算、取余  |\n| <<、<=、==、!=、>=、>           | 比较大小        |\n| &&、\\|\\|                        | 逻辑and、逻辑or |\n| =、+=、-=、*=、/=、%=、^=、\\**= | 赋值            |\n\n## Sed\n\n> 在处理文本时把当前处理的行存储在临时缓冲区中，称为：模式空间，**pattern space**\n>\n> 然后SED命令处理缓冲区中的内容，处理完成后将缓冲区的内容输出至屏幕或者写入文件\n\n```\nx                   \t\t\t\t# x为指定行号；\nx,y                 \t\t    # 指定从x到y的行号范围；\n/pattern/           \t\t    # 查询包含模式的行；\n/pattern/pattern/   \t\t    # 查询包含两个模式的行；\n/pattern/,x         \t\t    # 从与pattern的匹配行到x号行之间的行；\nx,/pattern/         \t\t    # 从x号行到与pattern的匹配行之间的行；\nx,y!                \t\t    # 查询不包括x和y行号的行；\nr                \t\t\t\t\t\t# 从另一个文件中读文件；\nw                \t\t\t\t\t\t# 将文本写入到一个文件；\ny                \t\t\t\t\t\t# 变换字符；\nq             \t\t\t\t\t\t\t# 第一个模式匹配完成后退出；\nl                \t\t\t\t\t\t# 显示与八进制ASCII码等价的控制字符；\n{}              \t\t\t    \t# 在定位行执行的命令组；\np                \t\t\t\t\t\t# 打印匹配行；\n=                \t\t\t\t\t\t# 打印文件行号；\na\\              \t\t\t    \t# 在定位行号之后追加文本信息；\ni\\              \t\t\t    \t# 在定位行号之前插入文本信息；\nd                \t\t\t\t\t\t# 删除定位行；\nc\\              \t\t\t    \t# 用新文本替换定位文本；\ns                \t\t\t\t\t\t# 使用替换模式替换相应模式；\nn                \t\t\t\t\t\t# 读取下一个输入行，用下一个命令处理新的行；\nN                           # 将当前读入行的下一行读取到当前的模式空间。\nh                \t\t\t\t\t\t# 将模式缓冲区的文本复制到保持缓冲区；\nH                \t\t\t\t\t\t# 将模式缓冲区的文本追加到保持缓冲区；\nx                \t\t\t\t\t\t# 互换模式缓冲区和保持缓冲区的内容；\ng                \t\t\t\t\t\t# 将保持缓冲区的内容复制到模式缓冲区；\nG                \t\t\t\t\t\t# 将保持缓冲区的内容追加到模式缓冲区。\n```\n\n> **sed**工具默认处理文本，文本内容输出屏幕已经修改，但是文件内容其实没有修改\n>\n> 需要加**-i**参数即对文件彻底修改；\n\n- 查看文件\n\n```\ncat -n file | sed -n \"p\"\n# 查看file文件 并且阅读其中所有行，因p前无规则\ncat -n file | sed -n \"1,3p\"\n# 查看file文件 1-3行\nsed \"1p;\\$p\" file\nsed '1p;$p' file\n# 查看文件第一行和最后一行\n```\n\n- 删除文件内容\n\n```\nsed '1,3d' file\n# 删除文件第一到第三行\nsed '$d' file\n# 删除文件最后一行\nsed -i '/#*/d' file\n# 删除文件中以#号开头的行\n```\n\n- 替换文本内容\n\n```\nsed 's/old/new/g' file\nsed -i 's/old/new/g' file # 加了-i那么修改会影响到原本的文件对象\n# 替换file文件中的old内容为new\n#\ts:使用替换模式替换相应模式\n# g:将保持缓冲区的内容复制到模式缓冲区；\n```\n\n- 追加文本内容\n\n```\nsed \"/###/a 123123123\" file\n# 在file的符合###匹配的 后面加一行123123123\nsed \"1,3a 123123123\" file\n# 在文件的1，3行追加123123123\n```\n\n- 插入文本内容\n\n```\nsed \"/###/a 123123123\" file\n# 在file的符合###匹配的 前面加一行123123123\nsed \"1,3a 123123123\" file\n# 在文件的1，3行前面加123123123\n```\n","tags":["Linux"]},{"title":"虚拟化、云计算","url":"/2019/12/23/pout/虚拟化、云计算概念：/","content":"\n虚拟化是指通过虚拟化技术奖一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间运行而互不影响，从而显著提高计算机的工作效率。\n    \n\n<!--more-->\n\n### 虚拟化、云计算概念：\n\n美国环境保护署（EPA）报告的一组有趣的统计数据。\n\nEPA研究服务器和数据中心的能源效率时发现，实际上服务器只有5%的时间在工作。在其他时间，服务器都处于 “休眠” 状态。就是说只有5%的消耗属于服务性能的消耗，其他都属于自己的无用消耗。\n\n\n####什么是虚拟化：\n\n虚拟化是指通过虚拟化技术奖一台计算机虚拟为多台逻辑计算机。在一台计算机上同时运行多个逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序都可以在相互独立的空间运行而互不影响，从而显著提高计算机的工作效率。\n\n虚拟化使用软件的方法重新定义划分IT资源，可以实现IT资源的鼎泰分配、灵活调度、跨域共享，提高IT资源利用率，使IT资源能够真正成为社会基础设施，服务于各行各业中灵活多变的应用需求。\n\n\n\n#### 虚拟化技术的应用价值：\n\n**虚拟化前**\n\n![1576409487(1)](C:\\Users\\lenovo\\Desktop\\1576409487(1).png)\n\n- **资源浪费**\n\n  系统资源的利用率不高，\n\n  有的服务器长期空闲，\n\n  有的服务器超负荷为运转\n\n- **管理难度大**（设备多）\n\n  服务器、路由、防火墙等设备数量众多，管理难度大。\n\n- **重复劳动**\n\n  经常性的重装、重做系统，调试网络设备等\n\n- **参数配置繁琐**\n\n  调整单台服务器CPU内存、硬盘大小等流程繁琐\n\n- **安全性差**\n\n  每台主机一个独立的操作系统，当安装一个完整的LAMP环境时，apache和mysql的资源是共享的，会造成安全性的问题。当Apache爆发漏洞时，可能会导致mysql的数据泄露。\n\n**虚拟化后：**\n\n- **高利用率**\n\n  将分散、独立的服务器资源整合成虚拟资源池后，资源利用率大大提高\n\n- **自由配置**\n\n  在资源池范围内，可以自行添加虚拟机，更改虚拟机内存、存储空间等参数\n\n- **统一管理**\n\n  通过虚拟化平台能够清晰的查看服务器运行情况、硬件健康状况等信息\n\n- **更稳定**\n\n  虚拟化本身就是一个安全技术，通过虚拟化技术手段，提高系统稳定性，保证数据安全\n\n\n\n\n\n#### 虚拟化过程：\n\n给每个服务器上装一个(VMware)虚拟卡，通过虚拟化软件把孤立的、分散的服务器资源连接在一起，形成一个虚拟化资源池，将资源集中起来，然后相对的虚拟出多台服务器，通过虚拟化软件将虚拟化任务自动的分配在多台服务器上\n\n![1576411355(1)](C:\\Users\\lenovo\\Desktop\\1576411355(1).png)\n\n\n\n\n\n#### 虚拟化技术的分类：\n\n- 全虚拟化技术\n\n  完全虚拟化技术又叫硬件辅助虚拟化技术，最初所使用的的虚拟化技术就是全虚拟化技术，它在虚拟机（VM）和硬件之间加了一个软件层——Hyperyisor，或者叫做虚拟机监控器（VMM）\n\n  - hypervisor（虚拟机软件层/虚拟机监控机）\n\n  ![1576467239(1)](C:\\Users\\lenovo\\Desktop\\1576467239(1).png)\n\n- 半虚拟化技术/准虚拟化技术（使用比较少）\n\n  半虚拟化技术，也叫准虚拟化技术。它就是在全虚拟化的基础上，把客户操作系统进行了修改，增加了一个专门的API，这个API可以将客户操作系统发出的指令进行最优化，即不需要Hypervisor耗费一定的资源进行翻译操作，因此Hypervisor的工作负担变得非常的小，因此整体的性能也有很大的提高。\n\n![1576467741(1)](C:\\Users\\lenovo\\Desktop\\1576467741(1).png)\n\n\n\n\n\n#### openstack云计算概念：\n\n**云计算**就是通过网络访问服务的一种模式。\n\n**“云计算”**可以理解为：通过互联网可以使用足够强大的计算机为用户提供的服务，这种服务的使用像可以统一的单位来描述。\n\n\n\n#### 虚拟化和云计算比较\n\n**虚拟化：** 是一种技术存在，从1个物理硬件系统创建多个模拟环境\n\n**云计算：** 是一种服务模式存在，汇聚并自动化分配虚拟资源以供按需使用\n","tags":["虚拟化"]},{"title":"Git相关知识","url":"/2019/12/23/pout/Git/","content":"\n\n<!-- toc -->\n\n# Git相关知识\n\n## 常用命令\n\ngit init 在本地新建一个repo,进入一个项目目录,执行git init,会初始化一个repo,并在当前文件夹下创建一个.git文件夹.\n\ngit clone 获取一个url对应的远程Git repo, 创建一个local copy. 一般的格式是git clone [url]. clone下来的repo会以url最后一个斜线后面的名称命名,创建一个文件夹,如果想要指定特定的名称,可以git clone [url] newname指定.\n\ngit status 查询repo的状态. git status -s: -s表示short, -s的输出标记会有两列,第一列是对staging区域而言,第二列是对working目录而言.\n\ngit log show commit history of a branch. git log --oneline --number: 每条log只显示一行,显示number条. git log --oneline --graph:可以图形化地表示出分支合并历史. git log branchname可以显示特定分支的log. git log --oneline branch1 ^branch2,可以查看在分支1,却不在分支2中的提交.^表示排除这个分支(Window下可能要给^branch2加上引号). git log --decorate会显示出tag信息. git log --author=[author name] 可以指定作者的提交历史. git log --since --before --until --after 根据提交时间筛选log. --no-merges可以将merge的commits排除在外. git log --grep 根据commit信息过滤log: git log --grep=keywords 默认情况下, git log --grep --author是OR的关系,即满足一条即被返回,如果你想让它们是AND的关系,可以加上--all-match的option. git log -S: filter by introduced diff. 比如: git log -SmethodName (注意S和后面的词之间没有等号分隔). git log -p: show patch introduced at each commit. 每一个提交都是一个快照(snapshot),Git会把每次提交的diff计算出来,作为一个patch显示给你看. 另一种方法是git show [SHA]. git log --stat: show diffstat of changes introduced at each commit. 同样是用来看改动的相对信息的,--stat比-p的输出更简单一些.\n\ngit add 在提交之前,Git有一个暂存区(staging area),可以放入新添加的文件或者加入新的改动. commit时提交的改动是上一次加入到staging area中的改动,而不是我们disk上的改动. git add . 会递归地添加当前工作目录中的所有文件.\n\ngit diff 不加参数的git diff: show diff of unstaged changes. 此命令比较的是工作目录中当前文件和暂存区域快照之间的差异,也就是修改之后还没有暂存起来的变化内容.\n\n```\n 若要看已经暂存起来的文件和上次提交时的快照之间的差异,可以用:\n git diff --cached 命令.\n show diff of staged changes.\n (Git 1.6.1 及更高版本还允许使用 git diff --staged，效果是相同的).\n\n git diff HEAD\n show diff of all staged or unstated changes.\n 也即比较woking directory和上次提交之间所有的改动.\n\n 如果想看自从某个版本之后都改动了什么,可以用:\n git diff [version tag]\n 跟log命令一样,diff也可以加上--stat参数来简化输出.\n\n git diff [branchA] [branchB]可以用来比较两个分支.\n 它实际上会返回一个由A到B的patch,不是我们想要的结果.\n 一般我们想要的结果是两个分支分开以后各自的改动都是什么,是由命令:\n git diff [branchA]…[branchB]给出的.\n 实际上它是:git diff $(git merge-base [branchA] [branchB]) [branchB]的结果.\n```\n\ngit commit 提交已经被add进来的改动. git commit -m “the commit message\" git commit -a 会先把所有已经track的文件的改动add进来,然后提交(有点像svn的一次提交,不用先暂存). 对于没有track的文件,还是需要git add一下. git commit --amend 增补提交. 会使用与当前提交节点相同的父节点进行一次新的提交,旧的提交将会被取消.\n\ngit reset undo changes and commits. 这里的HEAD关键字指的是当前分支最末梢最新的一个提交.也就是版本库中该分支上的最新版本. git reset HEAD: unstage files from index and reset pointer to HEAD 这个命令用来把不小心add进去的文件从staged状态取出来,可以单独针对某一个文件操作: git reset HEAD - - filename, 这个- - 也可以不加. git reset --soft move HEAD to specific commit reference, index and staging are untouched. git reset --hard unstage files AND undo any changes in the working directory since last commit. 使用git reset —hard HEAD进行reset,即上次提交之后,所有staged的改动和工作目录的改动都会消失,还原到上次提交的状态. 这里的HEAD可以被写成任何一次提交的SHA-1. 不带soft和hard参数的git reset,实际上带的是默认参数mixed.\n\n```\n 总结:\n git reset --mixed id,是将git的HEAD变了(也就是提交记录变了),但文件并没有改变，(也就是working tree并没有改变). 取消了commit和add的内容.\n git reset --soft id. 实际上，是git reset –mixed id 后,又做了一次git add.即取消了commit的内容.\n git reset --hard id.是将git的HEAD变了,文件也变了.\n 按改动范围排序如下:\n soft (commit) < mixed (commit + add) < hard (commit + add + local working)\n```\n\ngit revert 反转撤销提交.只要把出错的提交(commit)的名字(reference)作为参数传给命令就可以了. git revert HEAD: 撤销最近的一个提交. git revert会创建一个反向的新提交,可以通过参数-n来告诉Git先不要提交.\n\ngit rm git rm file: 从staging区移除文件,同时也移除出工作目录. git rm --cached: 从staging区移除文件,但留在工作目录中. git rm --cached从功能上等同于git reset HEAD,清除了缓存区,但不动工作目录树.\n\ngit clean git clean是从工作目录中移除没有track的文件. 通常的参数是git clean -df: -d表示同时移除目录,-f表示force,因为在git的配置文件中, clean.requireForce=true,如果不加-f,clean将会拒绝执行.\n\ngit mv git rm - - cached orig; mv orig new; git add new\n\ngit stash 把当前的改动压入一个栈. git stash将会把当前目录和index中的所有改动(但不包括未track的文件)压入一个栈,然后留给你一个clean的工作状态,即处于上一次最新提交处. git stash list会显示这个栈的list. git stash apply:取出stash中的上一个项目(stash@{0}),并且应用于当前的工作目录. 也可以指定别的项目,比如git stash apply stash@{1}. 如果你在应用stash中项目的同时想要删除它,可以用git stash pop\n\n```\n 删除stash中的项目:\n git stash drop: 删除上一个,也可指定参数删除指定的一个项目.\n git stash clear: 删除所有项目.\n```\n\ngit branch git branch可以用来列出分支,创建分支和删除分支. git branch -v可以看见每一个分支的最后一次提交. git branch: 列出本地所有分支,当前分支会被星号标示出. git branch (branchname): 创建一个新的分支(当你用这种方式创建分支的时候,分支是基于你的上一次提交建立的). git branch -d (branchname): 删除一个分支. 删除remote的分支: git push (remote-name) :(branch-name): delete a remote branch. 这个是因为完整的命令形式是: git push remote-name local-branch:remote-branch 而这里local-branch的部分为空,就意味着删除了remote-branch\n\ngit checkout 　　git checkout (branchname)\n\n切换到一个分支. git checkout -b (branchname): 创建并切换到新的分支. 这个命令是将git branch newbranch和git checkout newbranch合在一起的结果. checkout还有另一个作用:替换本地改动: git checkout -- 此命令会使用HEAD中的最新内容替换掉你的工作目录中的文件.已添加到暂存区的改动以及新文件都不会受到影响. 注意:git checkout filename会删除该文件中所有没有暂存和提交的改动,这个操作是不可逆的.\n\ngit merge 把一个分支merge进当前的分支. git merge [alias]/[branch] 把远程分支merge到当前分支.\n\n```\n 如果出现冲突,需要手动修改,可以用git mergetool.\n 解决冲突的时候可以用到git diff,解决完之后用git add添加,即表示冲突已经被resolved.\n```\n\ngit tag tag a point in history as import. 会在一个提交上建立永久性的书签,通常是发布一个release版本或者ship了什么东西之后加tag. 比如: git tag v1.0 git tag -a v1.0, -a参数会允许你添加一些信息,即make an annotated tag. 当你运行git tag -a命令的时候,Git会打开一个编辑器让你输入tag信息.\n\n```\n 我们可以利用commit SHA来给一个过去的提交打tag:\n git tag -a v0.9 XXXX\n\n push的时候是不包含tag的,如果想包含,可以在push时加上--tags参数.\n fetch的时候,branch HEAD可以reach的tags是自动被fetch下来的, tags that aren’t reachable from branch heads will be skipped.如果想确保所有的tags都被包含进来,需要加上--tags选项.\n```\n\ngit remote list, add and delete remote repository aliases. 因为不需要每次都用完整的url,所以Git为每一个remote repo的url都建立一个别名,然后用git remote来管理这个list. git remote: 列出remote aliases. 如果你clone一个project,Git会自动将原来的url添加进来,别名就叫做:origin. git remote -v:可以看见每一个别名对应的实际url. git remote add [alias] [url]: 添加一个新的remote repo. git remote rm [alias]: 删除一个存在的remote alias. git remote rename [old-alias] [new-alias]: 重命名. git remote set-url [alias] [url]:更新url. 可以加上—push和fetch参数,为同一个别名set不同的存取地址.\n\ngit fetch download new branches and data from a remote repository. 可以git fetch [alias]取某一个远程repo,也可以git fetch --all取到全部repo fetch将会取到所有你本地没有的数据,所有取下来的分支可以被叫做remote branches,它们和本地分支一样(可以看diff,log等,也可以merge到其他分支),但是Git不允许你checkout到它们.\n\ngit pull fetch from a remote repo and try to merge into the current branch. pull == fetch + merge FETCH_HEAD git pull会首先执行git fetch,然后执行git merge,把取来的分支的head merge到当前分支.这个merge操作会产生一个新的commit.\n如果使用--rebase参数,它会执行git rebase来取代原来的git merge.\n\ngit rebase --rebase不会产生合并的提交,它会将本地的所有提交临时保存为补丁(patch),放在”.git/rebase”目录中,然后将当前分支更新到最新的分支尖端,最后把保存的补丁应用到分支上. rebase的过程中,也许会出现冲突,Git会停止rebase并让你解决冲突,在解决完冲突之后,用git add去更新这些内容,然后无需执行commit,只需要: git rebase --continue就会继续打余下的补丁. git rebase --abort将会终止rebase,当前分支将会回到rebase之前的状态.\n\ngit push push your new branches and data to a remote repository. git push [alias] [branch] 将会把当前分支merge到alias上的[branch]分支.如果分支已经存在,将会更新,如果不存在,将会添加这个分支. 如果有多个人向同一个remote repo push代码, Git会首先在你试图push的分支上运行git log,检查它的历史中是否能看到server上的branch现在的tip,如果本地历史中不能看到server的tip,说明本地的代码不是最新的,Git会拒绝你的push,让你先fetch,merge,之后再push,这样就保证了所有人的改动都会被考虑进来.\n\ngit reflog git reflog是对reflog进行管理的命令,reflog是git用来记录引用变化的一种机制,比如记录分支的变化或者是HEAD引用的变化. 当git reflog不指定引用的时候,默认列出HEAD的reflog. HEAD@{0}代表HEAD当前的值,HEAD@{3}代表HEAD在3次变化之前的值. git会将变化记录到HEAD对应的reflog文件中,其路径为.git/logs/HEAD, 分支的reflog文件都放在.git/logs/refs目录下的子目录中.\n\n## 分支\n\n如果团队中有多个人再开发一下项目，一同事再开发一个新的功能，需要一周时间完成，他写了其中的30%还没有写完，如果他提交了这个版本，那么团队中的其它人就不能继续开发了。但是等到他全部写完再全部提交，大家又看不到他的开发进度，也不能继续干活，这如何是好呢？\n\n对于上面的这个问题，我们就可以用分支管理的办法来解决，一同事开发新功能他可以创建一个属于他自己的分支，其它同事暂时看不到，继续在开发分支（一般都 有多个分支）上干活，他在自己的分支上干活，等他全部开发完成，再一次性的合并到开发分支上，这样我们既可知道他的开发进度，又不影响大家干活，是不是很方便呢？\n\n分支本质上其实就是一个指向某次提交的可变指针。Git 的默认分支名字为 master 。而我们是怎么知道当前处于哪个分支当中呢？答案就是在于 HEAD 这个十分特殊的指针，它专门用于指向于本地分支中的当前分支。我们可以简单理解为：commit <- branch <- HEAD\n\n下面我们来创建分支。\n\n## 解决冲突\n\n1、git冲突的场景\n\n情景一：多个分支代码合并到一个分支时； 情景二：多个分支向同一个远端分支推送代码时； 实际上，push操作即是将本地代码merge到远端库分支上。\n\n关于push和pull其实就分别是用本地分支合并到远程分支 和 将远程分支合并到本地分支\n\n所以这两个过程中也可能存在冲突。\n\ngit的合并中产生冲突的具体情况：\n\n　　<1>两个分支中修改了同一个文件（不管什么地方） 　　<2>两个分支中修改了同一个文件的名称\n\n两个分支中分别修改了不同文件中的部分，不会产生冲突，可以直接将两部分合并。\n\n2、冲突解决方法\n\n情景一：在当前分支上，直接修改冲突代码--->add--->commit。 情景二：在本地当前分支上，修改冲突代码--->add--->commit--->push\n","tags":["Git"]},{"title":"Docker 存储","url":"/2019/12/23/pout/Docker/docker存储/","content":"\n\n<!-- toc -->\n\n#Docker 存储\n\n### Docek 镜像层的镜像分层结构\n\n* docker的镜像分层结构，如下所示：\n\n![基于Ubuntu映像的容器层](https://docs.docker.com/storage/storagedriver/images/container-layers.jpg)\n\n* docker镜像中引入层layer概念，镜像的制作过程中的每一步都会生产一个新的镜像层\n\n* 容器读写层的工作原理\n\n  > 我们刚刚在说镜像的分层特性的时候说到镜像是只读的。而事实上当我们使用镜像启动一个容器的时候，我们其实是可以在容器里随意读写的，从结果上看，似乎与镜像的只读特性相悖。\n  >\n  > 我们继续看上面的图，其实可以看到在镜像的最上层，还有一个读写层。而这个读写层，即在容器启动时为当前容器单独挂载。每一个容器在运行时，都会基于当前镜像在其最上层挂载一个读写层。而用户针对容器的所有操作都在读写层中完成。一旦容器销毁，这个读写层也随之销毁。\n  >\n  > > 知识点： 容器=镜像+读写层\n  >\n  > 而我们针对这个读写层的操作，主要基于两种方式：写时复制和用时分配。\n\n---\n\n### 容器\n\n![容器共享相同的图像](https://docs.docker.com/storage/storagedriver/images/sharing-layers.jpg)\n\n* 容器由最上面一个可写的容器层和若干个只读的镜像层组成，容器的数据就存在这些层中。这种分层结构最大的特点是Copy-on-Write。\n\n  1. 新数据会直接存放在最上面的容器层\n\n  2. 修改现有数据会从镜像层复制文件到容器中，再在容器层修改并保存，镜像层的数据不会发生改变\n\n  3. 若多个层中有命名相同的文件，用户只能看到最上面一层的文件\n\n     \n\n* 分层结构使镜像和容器的创建、共享以及分发变得非常高效，而这些都要归功于 Docerk stoage driver。**正是 storage driver 实现了多层数据的堆叠并为用户提供一个单一的合并之后的统一视图**。\n\n  \n\n---\n\n\n### Docker 为容器提供了两种存放数据的资源：\n\n*  由storage driver（存储驱动） 管理的镜像层和容器层\n   *  用来放一些无状态的数据\n      *  **对于某些容器，直接将数据放在由** storage driver **维护的层中是很好的选择，比如那些无状态的应用。无状态意味着容器没有需要持久化的数据，随时可以从镜像直接创建。即存在与否依赖镜像的存在。**\n*  Data Volume。（数据卷）\n   *  用来放一些有状态的数据，例如数据库\n      * **本质上是** Docker Host （主机）**文件系统中的目录或文件，能够直接被 ** mount （挂载）**到容器的文件系统中**。\n\n#### 关于docker镜像的三问\n\n* 基于镜像A创建镜像B时是否会拷贝A镜像中的所有文件：`是不会的`\n* 基于镜像创建容器时是否会拷贝镜像中的所有文件至文件层：`不会的`\n* 容器与镜像在结构上有什么区别：`没有区别容器会比镜像多了一个` `merged`文件\n\n\n\n> 在讲原理前，先讲下写时复制和写时分配\n\n#### 写时复制（CoW）\n\n> 所有驱动都用到的技术------写时复制（CoW）。CoW就是copy-on-write，表示只在需要写时才去复制，这个是针对已有文件的修改场景比如基于一个image启动多个Container，如果为每个Container都去分配一个image一样的文件系统，那么将会占用大量的磁盘空间。而CoW技术可以让所有的容器共享image的文件系统，所有数据都从image中读取，只有当要对文件进行写操作时，才从image里把要写的文件复制到自己的文件系统进行修改。所以无论多少个容器共享同一个image，所作的写操作都是从image中复制到自己的文件系统中的复制本上进行，并不会修改image的源文件，且多个容器操作同一个文件，会在每个容器的文件系统里生成一个复本，每个容器修改的都是自己的复本，相互隔离的，相互不影响。使用CoW可以有效的提高磁盘的利用率。\n\n#### 用时分配（allocate-on-demand）\n\n> 而用时分配是用在原本没有这个文件的场景，只有在要新写入一个文件时才分配空间，这样可以提高存储资源的利用率。比如启动一个容器，并不会为这个容器预分配一些磁盘空间，而是当有新文件写入时，才按需分配新空间。\n\n#### Docker存储驱动的作用\n\n> 将这些分层的镜像文件堆叠起来，并且提供统一的视图.使container的文件系统看上去和我们普通的文件系统没什么区别。\n> 当创建一个新的容器的时候,实际上是在镜像的分层上新添加了一层container layer（容器层）.之后所有对容器产生的修改,实际都只影响这一层。\n>\n> 注意\n>\n> 容器层：读写层(可写层)\n> 镜像层：只读层\n\n>  Docker 支持多种 storage driver，有 AUFS 、Device Mapper 、Btrfs 、OverlayFS 、VFS 和ZFS。它们都能实现分层的架构，同时又有各自的特性。对于Docker 用户来说，具体选择使用哪个 storage driver 是一个难题，因为：\n\n​\t\t\t没有哪个driver 能够适应所有的场景。\n\n​\t\t\tdriver 本身在快速发展和迭代。\n\n> 优先使用 Linux 发行版默认的 storage driver。Docker 安装时会根据当前系统的配置选择默认的 driver。默认 driver 具有最好的稳定性，因为默认 driver 在发行版上经过了严格的测试。\n\n> 运行`docker info`可以查看可查看当前系统使用的`Storage driver`。\n>\n> > ```\n> > [root@izbp1dg6m4eebtcm77n0smz ~]# docker info\n> > Client:\n> > Debug Mode: false\n> > \n> > Server:\n> > Containers: 6\n> > Running: 4\n> > Paused: 0\n> > Stopped: 2\n> > Images: 4\n> > Server Version: 19.03.5\n> > Storage Driver: overlay2\n> > Backing Filesystem: extfs\n> > Supports d_type: true\n> > Native Overlay Diff: false\n> > Logging Driver: json-file\n> > Cgroup Driver: cgroupfs\n> > ```\n>\n> \n\n---\n\n\n> Ubuntu 用的 `AUFS`，底层文件系统是 `extfs`，各层数据存放在 `/var/lib/docker/aufs`。\n> centos默认的`driver`用的是`overlay2`，底层的文件系统是xfs,各层数据存放在`/var/lib/docker`\n\n\n> 而写时分配是用在原本没有这个文件的场景，只有在要新写入一个文件时才分配空间，这样可以提高存储资源的利用率。\n>\n> 比如启动一个容器，并不是为这个容器预分配一些磁盘空间，而是当有新文件写入时，才按需分配新空间。\n\n* docker提供了多种的存储驱动来实现不同的方式存储镜像\n\n##### Docker五种存储驱动原理及应用场景和性能测试对比\n\n> `Docker` 最开始采用AUFS作为文件系统，也得益于AUFS分层的概念，实现了多个Container可以共享同一个image。但由于`AUFS` 为并入 `Linux`内核，且只支持 `Ubuntu`，考虑到兼容的问题，在 `Docker 0.7` 版本中引入了存储驱动，就如Docker官网上说的，没有单一的驱动适应所有的应用场景，要根据不同的场景选择合适的存储驱动，才能有效的提高Docker 的性能。如何选择适合的存储驱动，要先了解存储驱动原理才能更好的判断。\n\n> 接下来我们说说这些分层的镜像是如何在磁盘中存储的。\n\n* `docker` 提供了多种存储驱动来实现不同的方式存储镜像\n\n  * 下列出了 `Docker` 中支持的存储驱动程序：\n  \n    |      技术       |      存储驱动成名称      |\n    | :-------------: | :----------------------: |\n    |   `OverlayFS`   | `overlay` 或  `overlay2` |\n  |     `AUFS`      |          `aufs`          |\n    |     `Btrfs`     |         `btrfs`          |\n  | `Device Mapper` |      `devicemapper`      |\n    |      `VFS`      |          `vfs`           |\n  |      `ZFS`      |          `zfs`           |\n  \n##### AUFS\n\n  > AUFS（AnotherUnionFS）是一种 Union FS ，是文件级的存储驱动。AUFS 是一个能透明覆盖一个或多个县有文件系统的层状文件系统，把多层合并成文件系统的单层表示。简单来说就是支持将不同目录挂载到同一个虚拟文件系统下的文件系统。这种文件可以一层一层地叠加修改文件。无论低下有多少层都是只读的，只有最上层的文件系统是可写的。当需要修改文件时，AUFS创建该文件的一个副本，使用CoW将文件从只读层复制到可写层进行修改，结果保存在可写层。在Docker中，低下的只读层就是image，可写层就是Container。结构如下图所示：\n\n  [![1.jpg](http://dockone.io/uploads/article/20190702/87af417e9f80a3eb8ae9716ae07b3dc1.jpg)](http://dockone.io/uploads/article/20190702/87af417e9f80a3eb8ae9716ae07b3dc1.jpg)\n\n  >**历史**：aufs驱动老早就在Docker中存在了！其实，他在使用`graphdriver`这个名字之前久存在了。如果你查看项目在那（即首次使用graphdriver名称）提交之前的历史，之前项目中当时只有一个aufs的实现。下边devicemapper部分会讲到更多关于graphdriver这个名称诞生的历史。\n  >\n  >**实现**：Aufs最初代表的意思“另一个联合文件系统（another union filesystem）”，试图对当时已经存在的UnionFS实现进行重写。正如你期望的那样，它是一个传统意义的上层覆盖，通过利用aufs称作为“分支（branch）”的特性，让堆叠的目录合并成一个堆叠内容单一挂载点视图。此驱动会将父级信息组合一个有序列表，并把它作为挂载参数，然后把重活移交给aufs来把这些分层组装成一个联合视图。更多的细节信息可以在aufs的[帮助文档](http://aufs.sourceforge.net/aufs3/man.html)上看到。\n  >\n  >**优点**：这可能是历史最久且测试最完善的graphdriver后端了。它拥有不错的性能，也比较稳定，适用于广泛的场景。尽管它只在Ubuntu或者Debian的内核上才可以启用（下边有说明），但是这两个发行版和Docker一起使用的场景已经非常多，这让它在广阔的环境中得到了验证。同时，通过让不同的容器从同一个分层里面加载相同的库（因为他们在磁盘上是相同的inode）达到了共享内存页的效果。\n  >\n  >**缺点**：Aufs从来没有被上游Linux内核社区接受。多年来Ubuntu和Debian都需要往内核集成一个历史久远的补丁包，且原作者已经放弃了让它被内核采纳的努力。可能与IPV4和IPv6的辩论有些类似，人们担心某一天内核更新后会出现难以整合aufs的补丁的情况，从而导致aufs没得玩。但是就如IPv6，替换aufs势在必行的决心讲了一年又一年。除此之外，它面临着很多其他比较棘手的问题。其中一个最麻烦的、也是比较有历史的问题（尽管某种程度上这是一个安全的特性），是关于在高层更改向上拷贝的文件的权限的，这个问题困扰了不少用户。最终在2015年早期的时候通过编号为[#11799](http://dockone.io/docker/docker#11799)的PR使用aufs的`dirperm1`特性修复了。自然，这需要内核中有具有`dirperm1`能力aufs，然而这在今天任何较新版本的Ubuntu或者Debian上都已经不成问题了。\n  >\n  >**总结**：如果你在使用Ubtuntu或者Debian，那默认的graphdriver就是aufs，它能满足你绝大多数需求。有人期望有一天它能被overlay的实现取代，但是考虑到overlay文件系统的诸多问题，以及在上游内核中的成熟程度等挑战，这尚未实现。最后，aufs中没有配额的支持。\n\n  ##### Overlay\n\n  > Overlay 是Linux内核3.18后支持的，也是一种Union FS，和AUFS的多层不同的是Overlay只有两层：一个upper文件系统和一个lower文件系统，分别代表Docekr的镜像层和容器层。当需要修改一个文件时，使用CoW将文件从只读的lower复制到可写的upper进行修改，结果也保存在upper层。在Docekr中，底下的只读层就是image，可写层就是Container。目前最新的OverlayFS为Overlay2。结构图如下所示：\n\n  [![2.jpg](http://dockone.io/uploads/article/20190702/c12e244abea02f7ed1eb42f0ccdbcf1d.jpg)](http://dockone.io/uploads/article/20190702/c12e244abea02f7ed1eb42f0ccdbcf1d.jpg)\n\n  > **历史**：**2014年8月**，Red Hat的 Alex Larsson在编号为[453552c8384929d8ae04dcf1c6954435c0111da0](https://github.com/docker/docker/commit/453552c8384929d8ae04dcf1c6954435c0111da0)的代码提交中添加了针对OverlayFS（最初的上游内核的名称）的graphdriver。\n  >\n  > **实现**：Overlay是一个联合文件系统，它的概念较之aufs的分支模型更为简单。Overlay通过三个概念来实现它的文件系统：一个“下层目录（lower-dir）”，一个“上层目录（upper-dir）”，和一个做为文件系统合并视图的“合并（merged）”目录。受限于只有一个“下层目录”，需要额外的工作来让“下层目录”递归嵌套（下层目录自己又是另外一个overlay的联合），或者按照Docker的实现，将所有位于下层的内容都硬链接到“下层目录”中。正是这种可能潜在的inode爆炸式增长（因为有大量的分层和硬连接）阻碍了很多人采用Overlay。Overlay2通过利用更高内核（4.0以及以上的版本）中提供了的更优雅处理多个位于下层分层的机制解决了这个问题。\n  >\n  > **优点**：Overlay作为一个合并进主线Linux内核的一个有完整支持的联合文件系统有望成为人们的焦点。与aufs类似，通过使用磁盘上相同的共享库，它也能让分散的容器实现内存共享。Overlay同时有很多的上游Linux内核基于现代的应用场景，如Docker，被持续开发（参看overlay2）。\n  >\n  > **缺点**：硬链接的实现方式已经引发了 [inode耗尽](http://dockone.io/docker/docker#10613)的问题，这阻碍了它的大规模采用。inode耗尽并不是唯一的问题，还有其他一些与用户命名空间、SELinux支持有关的问题，且整体的成熟状况不足也阻碍着overlay直接取代aufs成为Docker默认的graphdriver。随着很多问题的解决，特别是在最新的内核发新版中，overlay的可用度越来越高了。如今出现的Overlay2修复了inode耗尽的问题，应该是从Docker 1.12版本之后的焦点，成为overlay驱动的后续开发对象。出于向后兼容的原因，`overlay`驱动将会继续留在Docker引擎中继续支持现有的用户。\n  >\n  > **总结**：考虑到aufs没有足够多的发行版的支持，能有一个上游集成的联合文件系统且拥有Linux内核文件系统社区的支持，overlay驱动的加入是一个重大进步。Overlay在过去的18-24个月已经成熟了很多，并且随着overlay2的出现，它之前一些麻烦的问题已经解决了。希望overlay（或者更具可能性的overlay2）会成为未来默认的graphdriver。为了overlay最好的体验，上游内核社区在4.4.x的内核系列里面修复了很多overlay实现中存在的问题；选择该系列中更新的版本可以获得overlay更好的性能和稳定性。\n\n  ##### Overlay2\n\n  >**历史**：[Derek McGowan](https://github.com/dmcgowan)在编号为[#22126](https://github.com/docker/docker/pull/22126)的PR中添加了overlay2的graphdriver，在**2016年6月**被合并进Docker 1.12版本，正如该PR的标题注明的，要取代之前overlay的主要原因是它能“支持多个下层目录”，能解决原先驱动中inode耗尽的问题。\n  >\n  >**实现**：在上面的overlay部分已经讲述了Linux内核中的Overlay的框架。上面链接的PR中改进了原有的设计，基于Linux内核4.0和以后版本中overlay的特性，可以允许有多个下层的目录。\n  >\n  >**优点**：overlay2解决了一些因为最初驱动的设计而引发的inode耗尽和一些其他问题。Overlay2继续保留overlay已有的优点，包括在同一个引擎的多个容器间从同一个分层中加载内库从而达到内存共享。\n  >\n  >**缺点**：现在可能唯一能挑出overlay2的问题是代码库还比较年轻。很多早期的问题已经在早期测试过程中发现并被及时解决了。但是Docker 1.12是第一个提供overlay2的发行版本，随着使用量的增长，相信可能还会发现其他问题。\n  >\n  >**总结**：将Linux内核中的一个现代的、广受支持的联合文件系统，和一个和Docker中一个性能优秀的graphdriver结合起来，这应该是Docker引擎未来打造默认的graphdriver最好的道路，只有这样才能获得各种Linux发行版广泛的支持。\n\n  ##### Device mapper\n\n  > Device mapper 是Linux 内核 2.6.9 后支持的，提供的一种从逻辑设备到物理设备的映射框架机制，在该机制下，用户可以很方便的根据自己的需要制定实现存储资源的管理策略。前面讲的 AUFS 和 OverlayFS 都是文件级存储，而 Device mapper 是块级存储，所有的操作都是直接对块进行操作，而不是文件。Device mapper 驱动会先在块设备上创建一个资源池，然后在资源池上创建一个带有文件系统的基本设备，所有镜像都是这个基本设备的快照，而容器则是镜像的快照。所以在容器里看到文件系统是资源池上基本设备的文件系统的快照，并不有为容器分配空间。当要写入一个新文件时，在容器的镜像内为其分配新的块并写入数据，这个用时分配。当要修改已有文件时，再使用 CoW 为容器快照分配块空间，将要修改的数据复制在容器快照中新的块里在进行修改。Device mapper 驱动默认会创建一个 100 G 的文件包含镜像和容器。每个容器被限制在 10G 大小的卷内，可以自己设置调整。结构如下图所示：\n\n  [![3.jpg](http://dockone.io/uploads/article/20190702/0ef920a30190955999076f524229f321.jpg)](http://dockone.io/uploads/article/20190702/0ef920a30190955999076f524229f321.jpg)\n\n>  **历史**：Devicemapper很早就以Ｃ代码的包装器面貌存在了，用来和libdevmapper进行交互； 是2013的９月Alex Larsson在编号为[ 739af0a17f6a5a9956bbc9fd1e81e4d40bff8167](https://github.com/docker/docker/commit/739af0a17f6a5a9956bbc9fd1e81e4d40bff8167)的代码提交中添加的。几个月后的重构了才诞生了我们现在所知道的“graphdriver”这个词；Solomon Hykes在2013年10月份早期代码合并的注释中说：将devmapper和aufs整合进通用的“graphdriver”框架。\n>\n>   **实现**：devicemapper这个graphdriver利用了Linux中devicemapper代码中众多特性之一，“轻配置（thin provisioning）”，或者简称为“thinp”。*（译注：根据Wikipedia，“thin provisioning是利用虚拟化技术，让人觉得有比实际可用更多的物理资源。如果系统的资源足够，能同时满足所有的虚拟化的资源，那就不能叫做thin-provisioned。”）* 这与之前提到的联合文件系统不同，因为devicemapper是基于块设备的。这些“轻配置（thin-provisioned）”的块设备带来的是如联合文件系统所提供的一样轻量的行为，但是最重要的一点是，他们不是基于文件的（而是基于块设备的）。正如你能推测的，这让计算分层之间的差别变得不再容易，也丧失了通过在容器间使用同样的库片段而共享内存的能力。\n>\n>   **优点**：Devicemapper在过去的年间也被一些人感到不屑，但是它提供的一个非常重要的能力让红帽系（Fedora,RHEL，Project Atomic）也有了一个graphdriver。因为它是基于块设备而不是基于文件的，它有一些内置的能力如配额支持，而这在其他的实现中是不容易达到的。\n>\n>   **缺点**：使用devicemapper没有办法达到开箱立即唾手可得很好的性能。你必须遵循[安装和配置指示](https://docs.docker.com/engine/userguide/storagedriver/device-mapper-driver/#/configure-direct-lvm-mode-for-production)才能得到性能还可以的配置。并且最重要的是，在任何需要用Docke引擎来做点正事的地方，都不要使用“虚拟设备（loopback）”模式（对于运行有devicemapper且负载高的系统，如延迟删除（ deferred removal）这样的特性绝对有必要的，这能减少引擎看起来好似夯住了一样的悲剧。）。它的一些特性依赖libdevmaper特定的版本，并且需要比较高级的技能来验证系统上所有的设置。同时，如果Docker Engine的二进制是静态编译的话，devicemapper会完全无法工作，因为它需要[udev sync](http://dockone.io/docker/docker#11412)的支持，而这不能被静态编译进引擎中。\n>\n>   **总结**：对于红帽类发行版本来说，devicemapper已经成为“可以直接用”的选择，并且在过去几年间里得到了红帽团队的大力支持和改进。它质量上有优点也有缺点，如果安装/配置过程中没有特别格外注意的话，可能导致和其他选项比较起来性能低下、质量不高。鉴于overlay和overlay2受到了Fedora和RHEL最新的内核的支持，并且拥有SELinux的支持，除非在Red Hat场景中有某种必须使用devicemapper的需求，我想随着用户的成熟他们会转向overlay的怀抱。\n\n  ##### Btrfs\n\n  > Btrfs 被称为下一代写时复制文件系统，并入Linux内核，也是文件级存储，但可以向 Device mapper 一直操作底层设备。 Btrfs 把文件系统的一部分配置为一个完整的子文件系统，称为 subvolume。那么采用 subvolume ，一个大的文件系统可以被划分为很多个子文件系统，这些子文件系统共享底层的设备空间，在需要磁盘空间使用时便从底层设备中分配，类似应用程序调用 malloc（）分配内存一样。为了灵活利用设备空间， Btrfs 将磁盘空间划分为多个 chunk。每个 chunk 可以使用不同的磁盘空间分配策略。比如某些 chunk 只存放 metadata ，某些chunk 只存放数据。这种模型有很多优点，比如 Btrfs 支持动态添加设备。用户在系统中添加新的磁盘之后，可以使用 Btrfs 的命令将该设备添加到文件系统中。Btrfs 把一个大的文件系统当成一个资源池，配置成多个完整的子文件系统，还可以往资源池里加新的子文件系统，而基础镜像则是子文件系统的快照，每个子镜像和容器都有自己的快照，这些快照都是 subvolume 的快照。\n\n   [![4.jpg](http://dockone.io/uploads/article/20190702/99ab3acda52806a948625219d9e96a0b.jpg)](http://dockone.io/uploads/article/20190702/99ab3acda52806a948625219d9e96a0b.jpg)\n\n  > 当写入一个新文件时，为在容器的快照里为其分配一个新的数据块，文件写在这个空间里，这个叫做分配。而当要修改已有文件时，使用 CoW 复制分配一个新的原始数据和快照，在这个新分配的空间变更数据，变结束再跟新相关的数据结构指向新子文件系统和快照，原来的原始数据和快照没有指针指向，被覆盖。\n\n  > **历史**：**2013年12月**较晚的时候，Red Hat公司的Alex Larsson在编号为[e51af36a85126aca6bf6da5291eaf960fd82aa56](https://github.com/docker/docker/commit/e51af36a85126aca6bf6da5291eaf960fd82aa56)的提交中，让使用btrfs作为管理`/var/lib/docker`的文件系统成为可能。\n  >\n  > **实现**：Btrfs的原生特性中，有两个是“子卷（subvolumes）”和“快照（snapshots）”。*（译注：根据Wikipedia，“子卷在btrfs中不是一个块设备，也不应该被当做是一个块设备。相反，子卷可以被想象成POSIX文件的命名空间。这个命名空间可以通过顶层的子卷来访问到，也可以独立地被挂载。快照在Btrfs中实际上是一个子卷，通过使用Btrfs的写时复制来和其他的子卷共享数据，对快照的更改不会影响原先的子卷。” ）* graphdriver实现中主要结合了这两个能力，从而提供了堆叠和类似写时复制的特性。当然，graphdriver的根（默认情况下是：`/var/lib/docker`）需要是一个被btrfs文件系统格式化的磁盘。\n  >\n  > **优点**：Btrfs几年前发布的时候（2007-2009时代），它被视作一个未来的Linux文件系统并[受到了大量的关注](https://lwn.net/Articles/342892/)。如今在上游Linux内核中，该文件系统已经比较健壮，并受到良好的支持，是众多可选的文件系统之一。\n  >\n  > **缺点**：但是Btrfs并没有成为Linux发行版的主流选择，所以你不大可能已经有一个btrfs格式化的磁盘。因为这种在Linux发行版中采用不足的原因，它并没有受到类似其他graphdriver一样的关注和采用。\n  >\n  > **总结**：如果你正在使用btrfs，那很显然的这个graphdriver应该迎合了你的需求。在过去几年有过很多Bug，并且有一段时间缺乏对SELinux的支持，但是这已经[被修复](http://dockone.io/docker/docker#16452)了。同时，对btrfs配额的支持也直接加进了docker守护进程中，这是[Zhu Guihua](https://github.com/zhuguihua)在编号为[#19651](http://dockone.io/docker/docker#19651)的PR中添加的，这个特性包含在了Docker 1.12版本中。\n\n  ##### ZFS\n\n  > ZFS 文件系统是一个革命性的全新的文件系统，它从根本上改变了文件系统的管理方式， ZFS 完全抛弃了 “ 卷管理 ” ，不再创建虚拟的卷，而是把所有设备集中到一个存储池中进行管理，用 “ 存储池 ”  的概念来管理物理存储空间。过去，文件系统都是构建在物理设备之上的，为了管理这些物理设备，并为数据提供冗余，“ 卷管理 ” 的概念提供了一个单设备的映射。而 ZFS 创建在虚拟的，被称为 “ zpools ” 的存储池之上。每个存储池由若干虚拟设备（ virtual devices ，vdevs ）组成。这些虚拟设备可以是原始磁盘，也节能是一个RAID1 镜像设备，或是非标准 RAID 等级的多磁盘组。  于是 zpool 上的文件系统可以使用这些虚拟设备的总存储容量。\n\n  [![5.jpg](http://dockone.io/uploads/article/20190702/d6daba2b7adfe96daca62f9ed90bf0c4.jpg)](http://dockone.io/uploads/article/20190702/d6daba2b7adfe96daca62f9ed90bf0c4.jpg)\n\n  > 下面看一下Docker 里ZFS的使用。首先从 zpool里分配一个ZFS 文件系统给镜像的基础层，而其他镜像层则是这个 ZFS 文件系统快照的克隆，快照是只读的，而克隆是可写的，当容器启动时则在镜像的顶层生成一个可写层。如下图所示：\n\n  [![6.jpg](http://dockone.io/uploads/article/20190702/34cc4c9ea6c96b6f83dabb961ed8950e.jpg)](http://dockone.io/uploads/article/20190702/34cc4c9ea6c96b6f83dabb961ed8950e.jpg)\n\n  > d当要写一个新文件时，使用按需分配，一个新的数据块从 zpool 里生成新的数据写入这个块，而这个新空间存于容器（ ZFS 的克隆 ）里。\n  >\n  > 当要修改一个已存在的文件时，使用写时复制，分配一个新空间并把原始数据复制到新空间完成修改。\n\n  >**历史**：ZFS的graphdriver是由Arthur Gautier和Jörg Thalheim一起在[#9411](http://dockone.io/docker/docker#9411)的PR中实现的，在**2014年的5月**被合并进了Docker引擎里面，并且从Docker 1.7版本开始用户可以使用。该实现依赖Go的一个三方包[go-zfs](https://github.com/mistifyio/go-zfs)进行相关zfs命令的交互。\n  >\n  >**实现**：与btrfs和devicemapper类似，要使用zfs驱动必需要有一个ZFS格式化的块设备挂载到graphdriver路径（默认是/var/lib/docker）。同时也需要安装好zfs工具（在绝大多数的发行版上是一个名为zfs-utils的包）供zfs Go库调用来执行相关操作。ZFS有能力创建快照（与btrfs类似），然后以快照的克隆作为分享层的途径（在ZFS的实现中成了一个快照）。因为ZFS不是一个基于文件的实现，aufs和overlay中所拥有的内存共享能力在ZFS是没有的。\n  >\n  >**优点**：ZFS正在受到越来越多的欢迎，在Ubuntu 16.04中，在Ubuntu的LXC/LXD中已经被使用。最初由Sun创建，ZFS已经存在很长的时间了，并且在Solaris和很多BSD的衍生版中使用，并且它的Linux移植版实现看起来也比较稳定，对于容器文件系统的场景也有足够合理性能。`ZFS`graphdriver也很及时的在Dockr 1.12中通过PR [#21946](http://dockone.io/docker/docker#21946)添加了配额的支持，这让它在配额支持方面和btrfs、devicemapper站在了同一起跑线上。\n  >\n  >**缺点**：除了没有基于文件（inode）的共享达到内库共享之外，很难说ZFS和其它同样基于块设备的实现相比有什么缺点。通过比较，ZFS看起来欢迎程度越来越高。对于那些完全支持或者正在使用ZFS的Linux发行版或者UNIX衍生版而言，zfs graphdriver可以是一个非常好的选择。\n  >\n  >**总结**：ZFS的支持为Docker引擎中稳定的graphdriver加了分。对于那些ZFS的使用者，或者那些ZFS扮演了更要角色的发行版来说，Docker能直接支持该文件系统，对这些社区来说是一个好消息。对于那些默认文件系统是ext4和xfs的发行版，默认采用overlay驱动的用户来说，时间会告诉我们他们是否会对zfs驱动产生更多的兴趣。\n\n#### 存储驱动的对比及适应场景\n| **存储驱动** | **特点**                                 | **优点**                                                     | **缺点**                                                     | **适用场景**                       |\n| ------------ | ---------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------------- |\n| AUFS         | 联合文件系统、未并入内核主线、文件级存储 | 作为docker的第一个存储驱动，已经有很长的历史，比较稳定，且在大量的生产中实践过，有较强的社区支持 | 有多层，在做写时复制操作时，如果文件比较大且存在比较低的层，可能会慢一些 | 大并发但少IO的场景                 |\n| overlayFS    | 联合文件系统、并入内核主线、文件级存储   | 只有两层                                                     | 不管修改的内容大小都会复制整个文件，对大文件进行修改显示要比小文件消耗更多的时间 | 大并发但少IO的场景                 |\n| Devicemapper | 并入内核主线、块级存储                   | 块级无论是大文件还是小文件都只复制需要修改的块，并不是整个文件 | 不支持共享存储，当有多个容器读同一个文件时，需要生成多个复本，在很多容器启停的情况下可能会导致磁盘溢出 | 适合io密集的场景                   |\n| Btrfs        | 并入linux内核、文件级存储                | 可以像devicemapper一样直接操作底层设备，支持动态添加设备     | 不支持共享存储，当有多个容器读同一个文件时，需要生成多个复本 | 不适合在高密度容器的paas平台上使用 |\n| ZFS          | 把所有设备集中到一个存储池中来进行管理   | 支持多个容器共享一个缓存块，适合内存大的环境                 | COW使用碎片化问题更加严重，文件在硬盘上的物理地址会变的不再连续，顺序读会变的性能比较差 | 适合paas和高密度的场景             |\n\n\n\n[![7.jpg](http://dockone.io/uploads/article/20190702/747be895d53add6ea9ddf868f95ff8ec.jpg)](http://dockone.io/uploads/article/20190702/747be895d53add6ea9ddf868f95ff8ec.jpg)\n\n##### AUFS VS  Overlay\n\n> AUFS和Overlay都是联合文件系统，但AUFS有多层，而Overlay只有两层，所以在做写时复制操作时，如果文件比较大且存在比较低的层，则AUSF可能会慢一些。而且Overlay并入了linux kernel mainline，AUFS没有，所以可能会比AUFS快。但Overlay还太年轻，要谨慎在生产使用。而AUFS做为docker的第一个存储驱动，已经有很长的历史，比较的稳定，且在大量的生产中实践过，有较强的社区支持。目前开源的DC/OS指定使用Overlay。\n\n##### Overlay VS Device mapper\n\n> Overlay是文件级存储，Device mapper是块级存储，当文件特别大而修改的内容很小，Overlay不管修改的内容大小都会复制整个文件，对大文件进行修改显示要比小文件要消耗更多的时间，而块级无论是大文件还是小文件都只复制需要修改的块，并不是整个文件，在这种场景下，显然device mapper要快一些。因为块级的是直接访问逻辑盘，适合IO密集的场景。而对于程序内部复杂，大并发但少IO的场景，Overlay的性能相对要强一些。\n\n##### Device mapper VS Btrfs Driver VS ZFS\n\n> Device mapper和Btrfs都是直接对块操作，都不支持共享存储，表示当有多个容器读同一个文件时，需要生活多个复本，所以这种存储驱动不适合在高密度容器的PaaS平台上使用。而且在很多容器启停的情况下可能会导致磁盘溢出，造成主机不能工作。Device mapper不建议在生产使用。Btrfs在docker build可以很高效。\n> ZFS最初是为拥有大量内存的Salaris服务器设计的，所在在使用时对内存会有影响，适合内存大的环境。ZFS的COW使碎片化问题更加严重，对于顺序写生成的大文件，如果以后随机的对其中的一部分进行了更改，那么这个文件在硬盘上的物理地址就变得不再连续，未来的顺序读会变得性能比较差。ZFS支持多个容器共享一个缓存块，适合PaaS和高密度的用户场景。\n\n#### IO性能对比\n\n> 测试工具：IOzone（是一个文件系统的benchmark工具，可以测试不同的操作系统中文件系统的读写性能）\n> 测试场景：从4K到1G文件的顺序和随机IO性能\n> 测试方法：基于不同的存储驱动启动容器，在容器内安装IOzone，执行命令：\n\n```\n./iozone -a -n 4k -g 1g -i 0 -i 1 -i 2 -f /root/test.rar -Rb ./iozone.xls\n```\n\n##### 测试项的定义和解释\n\n> Write：测试向一个新文件写入的性能。\n> Re-write：测试向一个已存在的文件写入的性能。\n> Read：测试读一个已存在的文件的性能。\n> Re-Read：测试读一个最近读过的文件的性能。\n> Random Read：测试读一个文件中的随机偏移量的性能。\n> Random Write：测试写一个文件中的随机偏移量的性能。\n\n##### 测试数据对比\n\n> Write：\n>\n> [![8.jpg](http://dockone.io/uploads/article/20190702/f592fe0e47c24441541b3970f6775674.jpg)](http://dockone.io/uploads/article/20190702/f592fe0e47c24441541b3970f6775674.jpg)\n>\n> \n> Re-write:\n>\n> [![9.jpg](http://dockone.io/uploads/article/20190702/778f51a47542033e0ded1b1b1d0edd63.jpg)](http://dockone.io/uploads/article/20190702/778f51a47542033e0ded1b1b1d0edd63.jpg)\n>\n> \n> Read：\n>\n> [![10.jpg](http://dockone.io/uploads/article/20190702/3028c70ce9a0abcfa673459b199612a3.jpg)](http://dockone.io/uploads/article/20190702/3028c70ce9a0abcfa673459b199612a3.jpg)\n>\n> \n> Re-Read：\n>\n> [![11.jpg](http://dockone.io/uploads/article/20190702/fb9fe60305c941fbfbc564cb2351e588.jpg)](http://dockone.io/uploads/article/20190702/fb9fe60305c941fbfbc564cb2351e588.jpg)\n>\n> \n> Random Read：\n>\n> [![12.jpg](http://dockone.io/uploads/article/20190702/ef273f23ee51927344a224ef3798e75a.jpg)](http://dockone.io/uploads/article/20190702/ef273f23ee51927344a224ef3798e75a.jpg)\n>\n> \n> Random Write：\n>\n> [![13.jpg](http://dockone.io/uploads/article/20190702/3a07e8a8a9b4de99602d02dc849b771b.jpg)](http://dockone.io/uploads/article/20190702/3a07e8a8a9b4de99602d02dc849b771b.jpg)\n\n* 通过以上的性能数据可以看到：\n  * AUFS在读的方面性能相比Overlay要差一些，但在写的方面性能比Overlay要好。\n  * device mapper在512M以上文件的读写性能都非常的差，但在512M以下的文件读写性能都比较好。\n  * btrfs在512M以上的文件读写性能都非常好，但在512M以下的文件读写性能相比其他的存储驱动都比较差。\n  * ZFS整体的读写性能相比其他的存储驱动都要差一些。 简单的测试了一些数据，对测试出来的数据原理还需要进一步的解析。\n\n\n> `Docker` 提供了可插拔的存储驱动程序架构。它使我们能够灵活地 `插入` `Docker`中的存储驱动程序。他完全基于`Linux`文件系统 。\n\n> 要实现这一功能，我们必须 在`docker` 守护进程的开始时就设置驱动程序。 `Docker` 守护程序只能运行一个存储驱动程序，并且该守护程序实例创建的所有容器使用相同的存储驱动程序。\n\n\n\n* 当前存储驱动\n\n  * 查看守护程序使用哪个存储驱动程序，可以使用一下命令。\n\n  ~~~\n  $ docker info\n  ~~~\n\n  > 可以看到上面的命令显示了守护进程使用的存储驱动程序。备份文件系统 `extfs` 。 `extfs` 表示覆盖存储驱动程序在文件系统的顶部运行。\n  >\n  > 后备文件系统实质用于在 `/var/lib/docker` 录下创建 `Docker` 主机的本地存储区域的文件系统。\n\n  * 下表包含必须与主机备份文件系统相匹配的存储驱动程序。\n\n    |   存储驱动   |      常用      |              已禁用               |\n    | :----------: | :------------: | :-------------------------------: |\n    |   overlay    |    ext4xfs     | btrfs  aufs  overlayzfs  eCryptfs |\n    |   overlay2   |    ext4xfs     | btrfs  aufs  overlayzfs  eCryptfs |\n    |     aufs     |    ext4xfs     |       btrfs  aufs  eCryptfs       |\n    |     aufs     |   btrfsonly    |                N/A                |\n    | devicemapper |   Direct-lvm   |                N/A                |\n    |     vfs      | debugging only |                N/A                |\n    |              |                |                N/A                |\n\n  > 注意 ：- “已禁用/Disabled on” 表示某些存储驱动程序无法在某些后台文件系统上运行\n\n\n#### 设置存储驱动程序\n\n> 可以通过 `dockersd`命令按指定名称来设置存储驱动程序。以下命令启动守护程序并设置新的驱动程序。\n\n~~~\n$ dockerd --storage-driver=devicemapper\n~~~\n\n> 稍后，可以通过 `docker info` 命令检查 `docker` 服务驱动程序\n\n---\n\n**对于某些容器，直接将数据放在由** storage driver **维护的层中是很好的选择，比如那些无状态的应用。无状态意味着容器没有需要持久化的数据，随时可以从镜像直接创建。即存在与否依赖镜像的存在。**\n\n~~~\n\t# 如一些工具箱，启动是为了执行命令，不需要保存数据供以后使用，使用完直接退出，容器删除时存在容器层的工作数据也一起删除，这没问题，下次启动新容器即可。\n\t\n\t# 但对于另一类应用这种方式就不合适了，它们有持久化数据的需求，容器启动时需要加载已有的数据，容器销毁时希望保留产生的新数据，也就是说，这类容器是有状态的，例如数据库。\n\t这就要用到docker 的另一个存储机制：data volume\n~~~\n\n\n\n### Data Volume（数据卷）\n\n---\n\n> 对于有些容器，我们可能会持久化数据的需求，也就是容器启动时需要加载已有的数据，容器销毁时希望保留产生的数据，也就是说这类容器是有状态的。\n>\n> 这就需要用到 `Docker` 的 `Data Volume` 存储机制。`Data Volume`本质上是 `Docker host`文件系统中的目录或文件，能够直接被 `mount` 到容器的文件系统。\n>\n> 在具体的使用上，`Docekr` 提供了两种类型的Volume：bind mount 和docker managed volume。\n\n##### 附：bind mount 与 docker managed volume 的区别\n\n* 这两种 **data volume** 实际上都是使用 **host** 文件系统的中的某个路径作为 **mount** 源。它们不同之处在于：\n\n| **不同点**                 | **bind mount**                  | **docker managed volume**        |\n| -------------------------- | ------------------------------- | -------------------------------- |\n| **volume 位置**            | 可任意指定                      | **/var/lib/docker/volumes/...**  |\n| **对已有mount point 影响** | 隐藏并替换为 **volume**         | 原有数据复制到 **volume**        |\n| **是否支持单个文件**       | 支持                            | 不支持，只能是目录               |\n| **权限控制**               | 可设置为只读，默认为读写权限    | 无控制，均为读写权限             |\n| **移植性**                 | 移植性弱，与 **host path** 绑定 | 移植性强，无需指定 **host** 目录 |\n\n\n\n##### 什么是数据卷\n\n* Data Volume 数据卷 ：是可以存放在一个或多个容器内的 **特定的目录**，提供独立于容器之外的**持久化存储**；是经过**特殊设计的目录**，可以绕过联合文件系统（UFS），为一个或多个容器提供访问；\n\n  ```\n  Docker Contrainer\n  面向对象中的对象\n  \n  对象一旦被销毁，数据就不存在了\n  \n  容器一旦被销毁，则容器内的数据将一并被删除\n  \n  服务器中的图案也会一并销毁\n  \n  容器中的数据不是持久化状态的\n  ```\n\n  > 不使用 `volume`的时候，对容器进行的改动是不会被保存的，使用 `volume`可以实现持久化存储；比如运行一个数据的操作，数据库的一个容器，数据库的数据应该被持久化存储的，`volume`就可以实现这个，并且 `volume`可以提供容器与容器之间的共享数据；\n\n##### Docker 的理念之一：\n\n> 就是将其应用于其运行的环境打包，因此，通过`Docker` 容器的生存周期，都是与容器中运行的程序相一致的，而我们对数据的要求通常是持久化的；另一方面，`docker`容器之间也需要有一个 **共享数据的渠道** ，而这些需求就催生出了`docker`数据卷的产生；\n\n##### 数据卷的设计的目的：\n\n> 在于 **数据的永久化** ，它完全独立于容器的生存周期，因此，`Docekr`不会在容器删除时删除其挂载的数据卷，也不会存在类似垃圾收集机制，对容器引用的数据卷进行处理了；\n\n##### 数据卷特点：\n\n* 1. `Docker`数据卷是独立于`Docker`的存在，它存在于`Docker host`（宿主机）中，因此，它与容器的生存周期是分离的；\n  2. `Docker`数据卷本质上是存在于`Docker`宿主机的本地文件系统中；\n  3. `Docker` 数据卷可以是目录也可以是文件；（不是块设备）\n  4. `Docker` 容器可以利用数据卷的技术与容器宿主机进行数据共享；\n  5. 同一个目录或者文件，可以支持多个容器进行访问，这样其实实现了容器的数据共享和交换；\n  6. 数据卷是在容器启动是进行初始化的，那么如果容器使用的镜像包含了的数据也会在容器启动时拷贝到容器的数据卷中；\n  7. `数据卷可以在容器之间共享和重用`；\n  8. `数据卷的修改会立马生效`；容器可以对数据卷里的内容直接修改；容器对数据卷进行的修改是及时的，所有的修改都会直接体现在数据卷中；\n  9. `数据卷的更新不会影响镜像`；因为文件不会写到镜像中去，数据卷是独立于联合文件系统的，而镜像本身基于联合文件系统，so镜像与数据卷之间不会有相互影响的情况；\n  10. `数据卷会一直存在，即使挂载数据卷的容器已经删除`因为数据均本质上是宿主机上的一个目录，同时为了提供数据的永久化，它的生存周期与容器是完全隔离的；\n\n  ![在这里插入图片描述](https://img-blog.csdnimg.cn/20190617160156293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQ2ODkx,size_16,color_FFFFFF,t_70)\n\n> Docker 容器中的数据操作经过了UFS 的，UFS 会在宿主机中写一次文件，这个文件在宿主机上是临时的，这时候就出现了重复写的情况，会影响系统的性能；此外，删除容器的时候，就没有人能够通过UFS 在访问到宿主机中的文件了；\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190617160937555.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQ2ODkx,size_16,color_FFFFFF,t_70)\n\n> 容器卷可以绕过 UFS 直接操作主机上的文件，当容器删除的时候，宿主机上的文件还在，就在指定的目录下，在重新创建容器的时候们可以指定容器继续读取宿主机上的文件；\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20190617161045446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQ2ODkx,size_16,color_FFFFFF,t_70)\n\n##### 创建一个数据卷\n\n> 包含数据卷挂载的容器在容器关闭时，如果修改了宿主机下的数据卷会，容器里面会产生改变吗？ \n\n* **bind mount 数据卷**\n\n> 使用docker run --name nginx-test -p 8080:80 -d -v ~/myvolume:/usr/share/nginx/html nginx  创建一个bind mount 数据卷 是宿主机的存储位置必须是绝对路径。目录不存在则会生成\n\n~~~\n# 以下两种情况创建的数据卷如果浏览器访问宿主机的ip:8080 会出现报错，因为这是创建的时候清空了容器数据卷下index.html\n# 创建的宿主机和容器的数据卷都有读写的权限\n$ docker run --name nginx-test -p 8080:80 -d -v ~/myvolume:/usr/share/nginx/html nginx\n# 这样执行后的文件宿主机的~/myvolume 文件如果不存在直接创建，容器的文件路径不存在也会直接创建，如果/usr/share/nginx/html文件存在里面内容会清空\n\n# 给容器里面的数据卷加权限\n$ docker run --name nginx-test -p 8080:80 -d -v ~/myvolume:/usr/share/nginx/html:ro nginx\n# 如果执行这个 :/usr/share/nginx/html:ro这个地方加的是 :ro 是设置的只有读取权限\n~~~\n\n```\n# 运行dockers inspect 容器名称或容器（ID） 是将容器的配置文件已json字符串的形式返回\n\"Binds\": [\n                \"/root/myvolume:/usr/share/nginx/html\"   # 宿主机数据卷位置: 容器的目录位置\n            ],\n\n\"Mounts\": [\n            {\n                \"Type\": \"bind\",\n                \"Source\": \"/root/myvolume\",   # 是宿主机数据卷的存储位置\n                \"Destination\": \"/usr/share/nginx/html\",\n                \"Mode\": \"\",\n                \"RW\": true,   # 权限 true是可以读写 fales 是只读\n                \"Propagation\": \"rprivate\"\n            }\n        ],\n\n```\n\n```\n# 在宿主机的数据卷下执行:\nvim index.html \n# 在文件里写入hello ， 你在访问的时候就可以在页面上看到你写入得数据了\n```\n\n> 执行 docker exec -it 容器名称（容器ID） bahs进入到容器里面，每个容器都会包含一个迷你版的linux系统\n>\n> 执行 cd /usr/share/nginx/html  \n>\n> 执行 ls\n>\n> 你会看到容器目录里会有我们刚才创建好的文件\n>\n> index.html\n>\n> 执行 cat index.html  可以看到里面我们加入的数据\n>\n> 如果是挂载数据卷的时候加 `:ro` 容器内修改文件，发现会提示该文件是只读的  \n\n---\n\n* **docker managed volume 数据卷**\n  * 创建出来的两个都是有读写权限的\n\n> 使用docker run --name nginx-test2 -p 8080:80 -d -v /usr/share/nginx/html nginx 创建一个**docker managed volume 数据卷** \n>\n> 这种命令创建是不用指定宿主机数据卷存储位置的默认在 /var/lib/docker/volumes/ 下的文件名是经过`sha256` 摘要过的\n\n* 查看宿主机创建出来的数据卷\n\n```\n$ cd  /var/lib/docker/volumes/\n$ ls \n8d668720aaeccee44b5fb554571912a6a257eb3a28cecf334203805a0c9b6fd3  #这是自己创建出来的数据卷\n# 执行 cd _data 进入这这个文件夹里面\n$ ls\n50x.html  index.html   # 这两个文件是把容器里文件给拷贝了出来\n\n```\n\n> 可以在宿主机或者容器里面都可以对文件进行读写操作\n\n##### 挂载多个目录实现数据卷的\n\n* 就是执行多个 `-v` 就可以\n\n#####  容器间的数据共享\n\n* 数据卷容器挂载了一个本地文件系统的目录，其它容器通过挂载这个数据卷容器来实现容器间的数据的共享；\n\n![这里写图片描述](https://img-blog.csdn.net/20180524134945342?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM3NTQ2ODkx/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n##### 容器间挂载 \n\n> 创建数据卷，只要在`docker run`命令后面跟上`-v`参数即可创建一个数据卷，当然也可以跟多个`-v`参数来创建多个数据卷，当创建好带有数据卷的容器后，就可以在其他容器中通过`--volumes-from`参数来挂载该数据卷了，而不管该容器是否运行。\n\n```\ndocker run -tid --rm --volumes-from nginx-test --name nginx-test3 nginx\n```\n\n> -i  : 以交互模式运行容器，通常与 -t 同时使用；\n>\n> -t  : 为容器重新分配一个伪输入终端，通常与 -i 同时使用；\n>\n> -d : 后台运行容器，并返回容器ID；\n\n* 再创建一个nginx-test4，挂载nginx-test3中从nginx-test挂载的数据卷，当然也可以直接挂载初识的nginx-test容器的数据卷\n\n```\n* 即使删除了初始的数据卷容器 nginx-test，或者是删除了其他容器，但只要是有容器在使用该数据卷，那么它里面的数据就不会丢失\n* 命令中的rm表示当容器退出即停止的时候，会自动删除该容器\n```\n\n---\n\n##### 备份数据卷\n\n\n\n* 创建一个容器container1，包含两个数据卷/usr/share/nginx/html1和/usr/share/nginx/html2（这两个目录是在容器里的数据卷路径）\n\n```\n$ docker run -tid -v /usr/share/nginx/html1 -v /usr/share/nginx/html2 --name container1 -p 8080:80 nginx\n# 创建容器container1\n\n$ docker exec -it container1 bash   #进入创建好的容器里面\n\n$ cd html1/  # 进入到html1数据卷中\n$ echo html1 >> 1.text # 向 1.text 文件中追加数据，文件不存在则会创建文件\n\n$ cd html2/  # 进入到html2数据卷中\n$ echo html2 >> 2.text # 向 2.text 文件中追加数据，文件不存在则会创建文件\n```\n\n* 接下来进行数据卷的备份操作\n\n> 使用  - -volumes-from 来创建一个加载 container1 容器卷的容器，并从宿主机挂载当前所在目录到容器的 /backup 目录，容器内会 tar 压缩 /var/colume1 目录下的文件到 /backup/backup1.tar，因为宿主机当前目录已经映射到 /backup 目录了，因此会在宿主机当前目录也存在该压缩包。备份完毕后 -rm 自动删除该创建的容器。\n\n* 备份container1容器中的/usr/share/nginx/html1数据卷数据\n\n```\n# 备份container1容器中的/usr/share/nginx/html1数据卷数据\n# -tid 这个参数加不加都可以\n# --rm 加上，备份后就会自动删除这个容器，如果不加这个 --rm 参数，name备份后的容器就会保留，docker ps -a就会查看到）\n# $(pwd) \n[root@iz2zefaujekcdpmfw1qs4az ~]# pwd\n/root\n\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker run -tid --rm --volumes-from container1 -v $(pwd):/backup nginx tar cvf /backup/backup1.tar /usr/share/nginx/html1\nb3663a3bdd302a38036d6a156471cd448c8e5b9333a20f9480b3c61cbd9270df\n\n[root@iz2zefaujekcdpmfw1qs4az ~]# ls\nbackup1.tar\n\n```\n\n> * --volumes-from [containerName]：这个命令来指定需要备份的容器的名字；（数据卷容器的名字）\n> * -v $(pwd):/backup:权限：使用-v命令来指定希望备份文件存放的位置；本地存放目录：容器存放目录：读写权限；（默认权限是读写）\n> * tar cvf /backup/backup.tar [container data volume]：tar表示执行备份的操作是：压缩文件的命令；\n> *  /backup/backup.tar是文件存放的地址， [container data volume]指定需要备份的目录；\n> * tar cvf 压缩；tar xvf解压缩；\n\n*  备份container1容器中的/usr/share/nginx/html2数据卷数据\n\n```\n# 备份container1容器中的/usr/share/nginx/html2数据卷数据\n[root@iz2zefaujekcdpmfw1qs4az ~]# pwd\n/root\n\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker run -tid --rm --volumes-from container1 -v $(pwd):/backup nginx tar cvf /backup/backup2.tar /usr/share/nginx/html2\n001129bc393d5d0ed4665d053d4ca7972584cf2bd56980064be182ec758138cd\n\n[root@iz2zefaujekcdpmfw1qs4az ~]# ll\ntotal 22464\n-rw-r--r-- 1 root root    10240 Dec 16 18:52 backup1.tar  # 文件1\n-rw-r--r-- 1 root root    10240 Dec 16 19:05 backup2.tar  # 文件2\ndrwxr-xr-x 2 root root     4096 Dec 16 16:45 myvolume\n-rw-r--r-- 1 root root 22973527 Mar 26  2019 Python-3.7.3.tgz\n\n```\n\n*  备份container1 容器中的 /usr/share/nginx/html1 和 /usr/share/nginx/html2 数据卷数据\n\n```\n#  备份container1 容器中的 /usr/share/nginx/html2 和 /usr/share/nginx/html2 数据卷数据\n[root@iz2zefaujekcdpmfw1qs4az ~]# pwd\n/root\n\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker run -tid --rm --volumes-from container1 -v $(pwd):/backup nginx tar cvf /backup/backup.tar /usr/share/nginx/html1 /usr/share/nginx/html2\n441df929e123cbe51564ca3d6bf3f06a5ea415298a34bb9871f1ed2b68a60102\n\n[root@iz2zefaujekcdpmfw1qs4az ~]# ll\ntotal 22476\n-rw-r--r-- 1 root root    10240 Dec 16 18:52 backup1.tar\n-rw-r--r-- 1 root root    10240 Dec 16 19:05 backup2.tar\n-rw-r--r-- 1 root root    10240 Dec 16 19:09 backup.tar\ndrwxr-xr-x 2 root root     4096 Dec 16 16:45 myvolume\n-rw-r--r-- 1 root root 22973527 Mar 26  2019 Python-3.7.3.tgz\n\n\n```\n\n##### 恢复数据给同一个容器\n\n> 之前的数据卷是从 container1 中备份的，现在模拟 container1 数据卷丢失，然后直接用之前备份的 backup.tar 进行恢复\n\n```\n# 为了测试恢复，先删除容器里原先的数据（注意：数据卷目录不能删除，只能删除其中的数据）\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker exec -it container1 bash \n#进入到创建的容器里\nroot@6869560e6ff5:/# ls\nbin  boot  dev\tetc  home  lib\tlib64  media  mnt  opt\tproc  root  run  sbin  srv  sys  tmp  usr  var\nroot@6869560e6ff5:/# cd /usr/share/nginx  \n#进入到容器里面的数据卷所在的目录\nroot@6869560e6ff5:/usr/share/nginx# ls\nhtml  html1  html2  \n\nroot@6869560e6ff5:/usr/share/nginx# cd html1\n# 进入到 html1 数据卷目录\nroot@6869560e6ff5:/usr/share/nginx/html1# ls\n1.text\n\nroot@6869560e6ff5:/usr/share/nginx/html1# rm -rf 1.text \n# 删除 1.text 文件\nroot@6869560e6ff5:/usr/share/nginx/html1# ls\n\nroot@6869560e6ff5:/usr/share/nginx# cd html2\n# 进入到 html2 的数据卷目录\nroot@6869560e6ff5:/usr/share/nginx/html2# ls\n2.text\n\nroot@6869560e6ff5:/usr/share/nginx/html2# rm -rf 2.text \n# 删除 2.text 文件\nroot@6869560e6ff5:/usr/share/nginx/html2# ls\n\n# 进行数据卷恢复，恢复数据卷中的所有数据\n注意-C后面的路径，表示将数据恢复到容器里的路径直接使用压缩包中文件的各个路径。比如压缩包中的结果如下：\ntar -xvf backup.tar   #解压压缩文件\n\n# 数据1\nusr/share/nginx/html1/1.text\n--usr\n\t--share\n\t\t--nginx\n\t\t\t--html1\n\t\t\t\t--1.text\n# 数据2\t\t\nusr/share/nginx/html2/2.text\n--usr\n\t--share\n\t\t--nginx\n\t\t\t--html2\n\t\t\t\t--2.text\n# 直接将文件解压到 /usr/share/nginx/html1 和 /usr/share/nginx/html2 目录\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker run --rm --volumes-from container1 -v $(pwd):/backup nginx tar xvf /backup/backup.tar -C /\nusr/share/nginx/html1/\nusr/share/nginx/html1/1.text\nusr/share/nginx/html2/\nusr/share/nginx/html2/2.text\n\n# 直接进入容器查看\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker exec -it container1 bash\nroot@6869560e6ff5:/# cd /usr/share/nginx/ \nroot@6869560e6ff5:/usr/share/nginx# ls\nhtml  html1  html2\n# 查看数据是否存在\nroot@6869560e6ff5:/usr/share/nginx# ls html1\n1.text\nroot@6869560e6ff5:/usr/share/nginx# ls html2\n2.text\nroot@6869560e6ff5:/usr/share/nginx# cat html1/1.text \nhtml1\nroot@6869560e6ff5:/usr/share/nginx# cat html2/2.text \nhtml2\n\n```\n\n##### 恢复数据给新的容器\n\n~~~\n# 新建一个容器container2\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker run -tid -v /usr/share/nginx/html1 -v /usr/share/nginx/html2 --name container2 nginx\n89abb55858fb1e3dddc07c2066d05614349aaf78ba446a1ea12f1241b98e4896\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES\n89abb55858fb        nginx               \"/bin/bash\"         9 seconds ago       Up 8 seconds        80/tcp              container2\n6869560e6ff5        nginx               \"/bin/bash\"         2 hours ago         Up 2 hours          80/tcp              container1\n\n# 开始恢复数据\n[root@iz2zefaujekcdpmfw1qs4az ~]# pwd\n/root\n[root@iz2zefaujekcdpmfw1qs4az ~]# ll\ntotal 22476\n-rw-r--r-- 1 root root    10240 Dec 16 18:52 backup1.tar\n-rw-r--r-- 1 root root    10240 Dec 16 19:05 backup2.tar\n-rw-r--r-- 1 root root    10240 Dec 16 19:09 backup.tar\ndrwxr-xr-x 2 root root     4096 Dec 16 16:45 myvolume\n-rw-r--r-- 1 root root 22973527 Mar 26  2019 Python-3.7.3.tgz\n\n# 恢复数据\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker run --rm --volumes-from container2 -v $(pwd):/backup nginx tar xvf /backup/backup.tar -C /\nusr/share/nginx/html1/\nusr/share/nginx/html1/1.text\nusr/share/nginx/html2/\nusr/share/nginx/html2/2.text\n\n# 查看确实已经恢复了\n[root@iz2zefaujekcdpmfw1qs4az ~]# docker exec -it container2 bash\nroot@89abb55858fb:/# ls /usr/share/nginx/\nhtml  html1  html2\nroot@89abb55858fb:/# ls /usr/share/nginx/html1\n1.text\nroot@89abb55858fb:/# ls /usr/share/nginx/html2\n2.text\nroot@89abb55858fb:/# cat /usr/share/nginx/html1/1.text \nhtml1\nroot@89abb55858fb:/# cat /usr/share/nginx/html2/2.text \nhtml2\n\n~~~\n\n> 注意：\n>\n> * --volumes-from [containerName]：这个命令来指定需要备份的容器的名字；（数据卷容器的名字）\n> * -v $(pwd):/backup:权限：使用-v命令来指定希望备份文件存放的位置；本地存放目录：容器存放目录：读写权限；（默认权限是读写）\n> * tar cvf /backup/backup.tar [container data volume]：tar表示执行备份的操作是：压缩文件的命令；\n> *  /backup/backup.tar是文件存放的地址， [container data volume]指定需要备份的目录；\n> * tar cvf 压缩；tar xvf解压缩；\n>\n> * 新容器创建时挂载的数据卷路径最好和之前备份的数据卷路径一致\n> * 新容器创建时，如果挂载的数据卷只是备份卷的一部分，那么恢复的时候也只是恢复一部分数据。\n> * 比如新建容器挂载数据卷为 `-v /usr/share/nginx/html1` ,那么使用 `backup.tar` 恢复时，只会恢复 `/usr/share/nginx/html1` 的数据， `/usr/share/nginx/html2` 的数据是不会恢复的\n> * 比如新容器创建时挂载的数据卷目录和备份的数据卷目录不一致，那么数据恢复不了，除非修改 - C 后面的路径，比如新建容器时指定数据卷目录为 `/usr/share/nginx/html` ，恢复时也是用 ` -C /usr/share/nginx/html`，则是可以成功恢复的\n\n##### 删除数据卷\t\n\n```\ndocker volume ls     列出所有的数据卷\ndocker volume ls --filter dangling=true     过滤不在使用的数据卷\ndocker volume rm [volume name]     删除一个数据卷，容器正在使用的数据卷不能删除，绑定挂载的数据卷无法删除\n```\n\n```\ndocker volume rm my-volio  删除数据卷 my-volio\n```\n\n> 数据卷 是被设计用来持久化数据的，它的生命周期独立于容器，Docker 不会在容器被删除后自动删除 数据卷，并且也不存在垃圾回收这样的机制来处理没有任何容器引用的 数据卷。如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使用 docker rm -v 这个命令。\n\n* 无主的数据卷可能会占据很多空间，要清理请使用以下命令\n\n```\n$ docker volume prune\n```\n\n","tags":["Docker"]},{"title":"视频常见视频格式","url":"/2019/12/23/pout/视频常见格式/","content":"\n\n<!-- toc -->\n\n## 常见视频格式\n\n编辑\n\n### MPEG格式\n\n[MPEG](https://baike.baidu.com/item/MPEG/213546)的英文全称为Moving Picture Experts Group，即运动图像专家组格式，家里常看的VCD、SVCD、DVD就是这种格式。MPEG文件格式是运动图像压缩算法的国际标准，它采用了有损压缩方法，从而减少运动图像中的冗余信息。MPEG的压缩方法说的更加深入一点就是保留相邻两幅画面绝大多数相同的部分，而把后续图像中和前面图像有冗余的部分去除，从而达到压缩的目的。目前MPEG主要压缩标准有MPEG-1、MPEG-2、MPEG-4、MPEG-7与MPEG-21。\n\n[MPEG-1](https://baike.baidu.com/item/MPEG-1/214216)：制定于1992年，是针对1.5Mb/s以下数据传输率的数字存储媒体运动图像及其伴音编码而设计的国际标准。也就是通常所见到的VCD制作格式。MPEG-1视频采用YCbCr色彩空间，4：2：0采样，码流一般不超过1.8Mb/s，仅仅支持逐行图像。MPEG-1视频的典型分辨率：352×240@29.97fps（NTSC）或者352*288@25fps（PAL/SECAM）。这种视频格式的文件扩展名包括mpg、mlv、mpe、mpeg及VCD光盘中的.dat文件等。\n\n[MPEG-2](https://baike.baidu.com/item/MPEG-2/214322)：制定于1994年，是针对3~l0Mb/s的影音视频数据编码标准。MPEG-2视频采用YCbCr色彩空间，4：2：0或4：2：2或4：4：4采样，最高分辨率为1920×1080，支持5.1环绕立体声，支持隔行或者逐行扫描。这种格式主要应用在DVD/SVCD的制作（压缩）方面，同时在一些HDTV（高清晰电视广播）和一些高要求视频编辑、处理上面也有相当的应用。这种视频格式的文件扩展名包括.mpg、.mpe、.mpeg、.m2v及DVD光盘上的.vob文件等。\n\n[MPEG-4](https://baike.baidu.com/item/MPEG-4/214399)：制定于1998年，是面向低传输速率下的影音编码标准，它可利用很窄的带度，通过帧重建技术压缩和传输数据，以求使用最少的数据获得最佳的图像质量。MPEG-4最有吸引力的地方在于它能够保存接近于DVD画质的小体积视频文件。这种视频格式的文件扩展名包括.asf、.mov和Divx、AVI等。MPEG-4使用了基于对象的编码（Object Based Encoding）技术，即MPEG-4的视音频场景是由静止对象、运动对象和音频对象等多种媒体对象组合而成，只要记录动态图像的轨迹即可，因此在压缩量及品质上，较MPEG-1和MPEG-2更好。MPEG-4支持内容的交互性和流媒体特性。\n\n[MPEG-7](https://baike.baidu.com/item/MPEG-7/10661017)：MPEG-7并不是一种压缩编码方法，而是一个多媒体内容描述接口标准（Multimedia Content Description Interface）。继MPEG-4之后，要解决的矛盾就是对日渐庞大的图像、声音信息的管理和迅速搜索，MPEG-7就是针对这个矛盾的解决方案。MPEG-7力求能够快速且有效地搜索出用户所需的不同类型的多媒体材料。\n\n[**MPEG-21**](https://baike.baidu.com/item/MPEG-21/1493581)：MPEG-21标准称为多媒体框架（Multimedia Framework），其实就是一些关键技术的集成，通过这种集成环境对全球数字媒体资源进行透明和增强管理，实现内容描述、创建、发布、使用、识别、收费管理、产权保护、终端和网络资源抽取、事件报告等功能。MPEG-21的最终目标是要为多媒体信息的用户提供透明而有效的电子交易和使用环境，将在未来的电子商务活动中发挥重要的作用 [2]  。\n\n### AVI格式、nAVI格式\n\nAVI（Audio Video Interleaved）是音频视频交错的英文缩写，将视频和音频封装在一个文件里，且允许音频同步于视频播放。它于1992年被Microsoft公司推出，随Windows3.1\n\n一起被人们所认识和熟知。这种视频格式的优点是图像质量好，可以跨多个平台使用；其缺点是体积过大，而且更糟糕的是压缩标准不统一，最普遍的现象就是高版本Windows媒体播放器播放不了采用早期编码编辑的AVI格式视频，而低版本Windows媒体播放器又播放不了采用最新编码编辑的AVI格式视频，所以在进行一些AVI格式的视频播放时常会出现由于视频编码问题而造成的视频不能播放或即使能够播放，但存在不能调节播放进度和播放时只有声音没有图像等一些莫名其妙的问题，如果用户在进行AVI格式的视频播放时遇到了这些问题，可以通过下载相应的解码器来解决，与DVD视频格式类似，AVI文件支持多视频流和音频流。它对视频文件采用了一种有损压缩方式，但压缩比较高，因此尽管画面质量不是太好，但其应用范围仍然非常广泛。\n\n**nAVI**是NewAVl的缩写，是一个名为Shadow Realm的地下组织发展起来的一种新视频格式。它是由MicrosoftASF压缩算法修改而来的，视频格式追求的无非是压缩率和图像质量，所以nAVI为了追求这个目标，改善了原始的ASF格式的一些不足，让nAVI可以拥有更高的帧率。可以说，nAVI是一种去掉视频流特性的改良型ASF格式 [2]  。\n\n### ASF格式\n\n[ASF](https://baike.baidu.com/item/ASF/3918)（Advanced Streaming Format）高级流格式是Microsoft为了和现在的Real Player竞争而发展出来的一种可以直接在网上观看视频节目的文件压缩格式。用户可以直接使用Windows自带的Windows Media Player对其进行播放。它使用了MPEG-4的压缩算法，其压缩率和图像质量都很不错。因为ASF是以一种可以在网上即时观赏的视频流格式存在的，所以它的图像质量比VCD差一点，但比同是视频流格式的RAM格式要好 [2]  。\n\n### MOV格式\n\n[MOV](https://baike.baidu.com/item/MOV/213982)即[QuickTime](https://baike.baidu.com/item/QuickTime/3561948)影片格式，它是Apple公司开发的一种音频、视频文件格式，用于存储常用数字媒体类型。当选择QuickTime（w.mov）作为保存类型时，动画将保存为.mov文件。Quick Time原本是Apple公司用于Mac计算机上的一种图像视频处理软件。\n\nQuickTime提供了两种标准图像和数字视频格式，即可以支持静态的*.PIC和*.JPG图像格式，动态的基于Indeo压缩法的*.MOV和基于MPEG压缩法的*.MPG视频格式。\n\nQuickTime视频文件播放程序，除了可播放MP3外，还支持MIDI播放，并且可以收听/收视网络播放，支持HTTP、RTP和RTSP标准。该软件支持JPEG、BMP、PICT、PNG和GIF主要的图像格式。还支持数字视频文件，包括MiniDV、DVCPro、DVCam、AVI、AVR、MPEG-1、OpenDML.以及Macromedia Flash等。QuickTime文件格式支持25位彩色，支持领先的集成压缩技术，提供150多种视频效果，并配有提供了200多种MIDI兼容音响和设备的声音装置。它无论是在本地播放还是作为视频流格式在网上传播，都是一种优良的视频编码格式。QuickTime因具有跨平台（MacOS/Windows）、存储空间要求小等技术特点，而采用了有损压缩方式的MOV格式文件，画面效果较AVI格式要稍微好一些。现在这种格式有些非编软件也可以对它实行处理，其中包括[Adobe](https://baike.baidu.com/item/Adobe/211696)公司的专业级多媒体视频处理软件After Effect和Premiere等 [2]  。\n\n### WMV格式\n\n[WMV](https://baike.baidu.com/item/WMV/1195900)（Windows Media Video）是微软推出的一种流媒体格式，它是在ASF格式升级延伸来得。在同等视频质量下，WMV格式的体积非常小，因此很适合在网上播放和传输。\n\nWMV一种独立于编码方式的在Internet 上实时传播多媒体的技术标准，Microsoft公司希望用其取代QuickTime之类的技术标准以及WAV、AVI之类的文件扩展名。WMV的主要优点在于：可扩充的媒体类型、本地或网络回放、可伸缩的媒体类型、流的优先级化、多语言支持、扩展性等 [2]  。\n\n### 3GP格式\n\n[3GP](https://baike.baidu.com/item/3GP/203418)是“第三代合作伙伴项目”制定的一种多媒体标准，即一种3G流媒体的视频编码格式，主要是为了配合3G网络的高传输速度而开发的，也是目前手机中最为常见的一种视频格式。其核心由包括高级音频编码、自适应多速率和MPEG-4和H.263视频编码解码器等组成，目前大部分支持视频拍摄的手机都支持3GP格式的视频播放，Real VIDEO（RA、RAM）格式一开始就是在视频流应用方面的，也是视频流技术的始创者。它可以在用56K MODEM拨号上网的条件实现不间断的视频播放，当然，其图像质量不能和MPEG-2、DIVX等相比较，毕竟要实现在网上传输不间断的视频是需要很大的频宽的，在这方面它是ASF的有力竞争者 [2]  。\n\n### RM格式与RMVB格式\n\nRM格式是Real Networks公司所制定的音频视频压缩规范，全称为Real Media。用户可以使用RealPlayer 或RealOne Player对符合Real Media技术规范的网络音频/视频资源进行实况转播，并且Real Media可以根据不同的网络传输速率制定出不同的压缩比率，从而实现在低速率的网络上进行影像数据实时传送和播放。这种格式的另一个特点是用户使用Real Player或RealOne Player 播放器可以在不下载音频/视频内容的条件下实现在线播放。另外，RM作为目前主流网络视频格式，它还可以通过其Real Server服务器将其他格式的视频转换成RM视频并由Real Server服务器负责对外发布和播放。一般地，RM视频更柔和一些，而ASF视频则相对清晰一些。\n\nRMVB格式是由RM视频格式升级而来的视频格式，它的先进之处在于RMVB视频格式打破了原先RM格式那种平均压缩采样的方式，在保证平均压缩比的基础上合理利用比特率资源，就是说静止和动作场面少的画面场景采用较低的编码速率，这样可以留出更多的带宽空间，而这些带宽会在出现快速运动的画面场景时被利用。这样在保证了静止画面质量的前提下，大幅地提高了运动图像的画面质量，从而图像质量和文件大小之间就达到了微妙的平衡。另外，相对于[DVDrip](https://baike.baidu.com/item/DVDrip/3130063)格式，RMVB视频也有着较明显的优势，一部大小为700MB左右的DVD影片，如果将其转录成同样视听品质的RMVB格式，其个头最多也就400MB左右。不仅如此，这种视频格式还具有内置字幕和无须外挂插件支持等独特优点 [2]  。\n\n### FLV/F4V格式\n\n[FLV](https://baike.baidu.com/item/FLV/6623513)是Flash Video的简称，也是一种视频流媒体格式。由于它形成的文件较小、加载速度很快，使得网络观看视频文件成为可能，它的出现有效地解决了视频文件导入Flash后，使导出的SWF文件体积庞大，不能在网络上很好地使用等缺点，应用较为广泛。\n\n[F4V](https://baike.baidu.com/item/F4V/3112913)是继FLV格式后Adobe公司推出的支持H.264的高清流媒体格式，它和FLV的主要区别在于，FLV格式采用的是H.263编码，而F4V则支持H.264编码的高清晰视频，码率最高可达50Mbps。F4V更小更清晰，更利于网络传播，已逐渐取代FLV，且已被大多数主流播放器兼容播放，而不需要通过转换等复杂的方式。如目前主流的土豆、56、优酷等视频网站都开始用H.264编码的F4V文件，黑豆和酷6发布的视频大多数已为F4V，但下载后缀为FLV，这也是F4V特点之一。相同文件大小情况下，清晰度明显比MPEG-2和H.263编码的FLV要好。由于采用H.264高清编码，相比于传统的FLV，F4V在同等体积下，能够实现更高的分辨率，并支持更高比特率。但由于FAV是新兴的格式，目前各大视频网站采用的FAV标准非常之多，也决定了F4V相比于传统FLV，兼容能力相对还较弱。需要注意的是，F4V和MP4是兼容的格式，都属于ISMAMP4容器，但是F4V只用来封装H.264视频编码和音频AAC.FlV是Adobe私有格式，但是也可以用来封装H.264视频编码、[AAC](https://baike.baidu.com/item/AAC/382962)音频编码或H.263视频编码、MP3音频编码，\n\n此外，目前也有许多著名公司推出性能优异的视频格式，如SONY公司新近推出适合高清领域的[MTS](https://baike.baidu.com/item/MTS/1504859)格式，等等 [2]  。\n\n### H.264、H.265\n\n[**H.264**](https://baike.baidu.com/item/H.264/1022230)**标准**是ITU-T与ISO联合开发的新一代视频编码标准，这一新的图像信源编码压缩标准于2003年7月由ITU正式批准。与MPEG-2相比，在同样的图像质量条件下，H.264的数据速率只有其1/2左右，压缩比大大提高。通常也称H.264标准为高级视频编码标准（以AVC表示）。\n\nH.264的前身H.26L是由国际电信联盟ITU-T视频编码专家组（VCEG）于1998年首先提出的，2001年起，ITU-T的VCEG与国际标准化组织ISO/IEC的MPEG（动态图像专家组）共同组织了联合视频合作组（JVT），在H.26L的基础上开发出新一代视频压缩编码标准H.264，同时它将作为MPEG-4标准中的一个新的第10部分，它与MPEG-4中第2部分的视频压缩编码标准相比，有更优异的性能。因此，在谈及MPEG-4的视频编码方法和性能时，应特别区分是指其第2部分还是第10部分，二者不可混为一谈。H.264也是MPEG-4的一种，全称为MPEG-4Part 10或全称为MPEG-4AVC（高级视频编码）。它们都是活动图像编码方式的国际标准 [3]  。\n\nITU于2013年1月批准了[H.265](https://baike.baidu.com/item/H.265/7752521)标准，这个标准的正式名称是HEVC（High Efficiency Video Coding）。尽管H.265在编码架构上与H.264相似，但H.265引入可变量的尺寸转换以及更大尺寸的帧内预测块、更多的帧内预测模式减少空间冗余、更多空间域与时间域结合、更精准的运动补偿滤波器等手段，计算处理多核并行速度快，适应高清实时编码，其峰值计算量达500GOPS，H.264仅100GOPS，其在性能与功能上远超出H.264 [4]  。\n","tags":["视频"]},{"title":"使用xtrabackup备份","url":"/2019/12/23/pout/使用xtrabackup进行备份/","content":"\n\n<!-- toc -->\n\n# 使用xtrabackup备份mysql\n\n## 简介（[Percona XtraBackup](https://www.percona.com/software/mysql-database/percona-xtrabackup) ）简称PXB##\n\n> Xtrabackup是由percona开源的免费数据库热备份软件，它能对Innodb数据库和Xtradb存储引擎的数据库非阻塞地备份。（对于Myisam的备份同样需要加表锁），mysqldump备份方式是采用的逻辑备份，其最大的缺陷是备份和恢复速度较慢，如果数据库大于50G，mysqldump备份就不太适合。\n\n## 优点##\n\n+ 备份速度快，物理备份可靠\n+ 备份过程不会打断正在执行的事务（无需锁表）\n+ 能够基于压缩等功能节约磁盘空间和流量\n+ 自动备份校验\n+ 还原速度快\n+ 可以流传将备份传输到另一台机器上\n+ 在不增加服务器负载的情况下备份数据\n\n## 原理##\n\n> ​\t备份开始的时候，首先会开启一个后台检测进程，实时检测mysql redo到的变化，一旦发现有新的日志写入，立刻将日志记入后台日志文件xtrabackup_log中，之后赋值innodb的数据文件，系统表空间文件ibdatax，复制后，将上锁（读锁），flush tables with read lock，让后复制.frm MYI MYD等文件，最后执行 unlock tables（释放锁），最终停止xtrabackup_log。\n\n​\t\t![image ](http://mysql.taobao.org/monthly/pic/2016-03-07/PXB-backup-procedure.png)\n\n## 扩展##\n\n> ​\t在innodb内部会维护一个redo日志文件，我们也可以叫做事务日志文\t件，事务日志会存储每一个Innodb表数据的记录修改。当Innodb启动时，Innodb会检查数据文件和事务日志。并执行两个步骤：它应用（前滚）已经提交的事务日志到数据文件，并将修改过但没有提交的数据进行回滚操作。\n>\n> ​\txtrabackup在启动时会记住log\tsequence number（LSN），并且复制所有数据文件，复制过程需要一些时间，所以这期间如果数据文件有改动，那么将会使数据库处于一个不同的时间点。这时，Xtrabackup会运行一个后台进程，用于监测事务日志，并从事务日志复制最新的修改。xtrabackup必须持续的做这个操作，因为事务日志是会轮转重复的写入，并且事务日志可以被重用。所以xtrabackup自启动开始，就不停的将事务日志中每个数据文件的修改都记录下来。这就是xtrabackup的备份过程。\n>\n> ​\t接下来是准备（prepare）过程。在这个过程中，xtrabackup使用之前复制的事务日志。对各个数据文件执行灾难恢复（就像mysql刚启动时要做的一样）。当这个过程结束后，数据库就可以做恢复还原了。\n>\n> ​\t整个过程就是-备份-》准备。先将文件全部复制过来，在根据事务日志对部分操作进行回滚。\n>\n> ​\t程序innobbackupex可以允许我们备份Myisam表和文件从而增加了便捷和功能。\n>\n> ​\tinnobbackupex会启动xtrabackup，直到xtrabackup复制数据文件后，然后执行FLUSH TABLES WITH READ LOCK 来阻止新的写入刷新到磁盘上。之后复制Myisam数据文件。最后UNLOCK TABLES （释放锁）。\n>\n> ​\t备份Myisam和Innodb表最终会处于一致，在准备（prepare）过程结束后，Innodb表数据已经前滚到整个备份结束点，而不是回滚到xtrabackup感刚开始的点。这个时间点与执行FLUSH TABLES WITH READ LOCK的时间点相同，所以Myisam表数据与Innodb表数据是同步的。\n\n## xtrabackup增量备份\n\n+ 原理\n\n  > ​\t首先是建立在完全备份的基础上，记录下此时的检查点LSN\n  >\n  > ​\t在进行增量备份时，比较表空间中每个页的LSN是否大于上次备份的LSN，若是则备份该页并记录当前检查点的LSN。\n\n+ 优点：\n\n  + 数据库太大没有足够的空间全量备份，增量备份能有效节省空间，并且效率高\n  + 支持热备份，备份过程不锁表（针对Innodb而言），不阻塞数据库读写。\n  + 每日备份只产生少量数据，也可采用远程备份，节省本地空间\n  + 备份恢复基于文件操作，降低直接对数据库操作风险\n  + 备份效率更高，恢复效率更高。\n\n## 工具集\n\n软件包安装完后一共有4个可执行文件，如下：\n\n```shell\nusr\n├── bin\n│   ├── innobackupex\n│   ├── xbcrypt  #用来加密或解密备份的数据\n│   ├── xbstream  #用来解压或压缩xbstream格式的文件\n│   └── xtrabackup\n```\n\n其中最主要的是 `innobackupex` 和 `xtrabackup`，前者是一个 perl 脚本，后者是 C/C++ 编译的二进制。\n\n`xtrabackup` 是用来备份 InnoDB 表的，不能备份非 InnoDB 表，和 mysqld server 没有交互；`innobackupex` 脚本用来备份非 InnoDB 表，同时会调用 `xtrabackup` 命令来备份 InnoDB 表，还会和 mysqld server 发送命令进行交互，如加读锁（FTWRL）、获取位点（SHOW SLAVE STATUS）等。简单来说，`innobackupex` 在 `xtrabackup` 之上做了一层封装。\n\n一般情况下，我们是希望能备份 MyISAM 表的，虽然我们可能自己不用 MyISAM 表，但是 mysql 库下的系统表是 MyISAM 的，因此备份基本都通过 `innobackupex` 命令进行；另外一个原因是我们可能需要保存位点信息。\n\n另外2个工具相对小众些，`xbcrypt` 是加解密用的；`xbstream` 类似于tar，是 Percona 自己实现的一种支持并发写的流文件格式。两都在备份和解压时都会用到（如果备份用了加密和并发）。\n\n本文的介绍的主角是 `innobackupex` 和 `xtrabackup`。\n\n## 下载##\n\n+ 安装percona仓库\n\n  ```shell\n  yum -y install http://www.percona.com/downloads/percona-release/redhat/0.1-4/percona-release-0.1-4.noarch.rpm \n  ```\n\n+ 安装xtrabackup\n\n  ```shell\n  yum install percona-xtrabackup -y\n  ```\n\n+ 创建备份用户及设置权限（也可以直接使用root用户）\n\n  ```shell\n  CREATE USER ‘用户名’@'localhost' IDENTIFIED BY '密码';#创建\n  GRANT RELOAD,LOCK TABLES,PROCESS,REPLICATION CLIENT ON *.* TO '用户名'@'localhost';#设置权限\n  FLUSH PRIVILEGES;#刷新权限\n  ```\n\n+ 配置xtrabackup（可配置也可以已参数的形式写入）\n\n  ```shell\n  vim /root/.my.cnf\n  \n  [xtrabackup]\n  \n  user=创建的用户名\n  \n  password=密码\n  ```\n\n+ 创建备份使用的文件夹\n\n  ```shell\n  mkdir /data/backup/mysql\n  ```\n\n## 基于xtrabackup的备份和恢复##\n\n+ _xtrabackup_ 只支持innodb引擎和xtradb引擎\n\n+ 语法：\n\n  + --backup 表示该操作代表备份操作\n  + --target-dir 指定备份文件的路径\n  + --user 备份的用户 （设定配置文件后，无需指定）\n  + --password  用户密码（同上）\n  + --socket 指定socket启动文件路径（不添加使用默认路径）\n  + --incremental-basedir  表示在某个全量备份的基础上进行增备\n\n+ 全量备份\n\n  ```shell\n  [root@ax mysql]# xtrabackup --backup --target-dir=/data/backup/mysql\n  #以下是返回的结果\n  191221 13:49:02  version_check Connecting to MySQL server with DSN 'dbi:mysql:;mysql_read_default_group=xtrabackup' as 'backuper'  (using password: YES).\n  191221 13:49:02  version_check Connected to MySQL server\n  191221 13:49:02  version_check Executing a version check against the server...\n  191221 13:49:02  version_check Done.\n  191221 13:49:02 Connecting to MySQL server host: localhost, user: backuper, password: set, port: not set, socket: not set\n  Using server version 5.5.64-MariaDB\n  xtrabackup version 2.3.10 based on MySQL server 5.6.24 Linux (x86_64) (revision id: bd0d4403f36)\n  xtrabackup: uses posix_fadvise().\n  xtrabackup: cd to /var/lib/mysql\n  xtrabackup: open files limit requested 0, set to 65535\n  xtrabackup: using the following InnoDB configuration:\n  #省略.....\n  MySQL binlog position: filename 'mysql-bin.000001', position '245'\n  191221 13:49:03 [00] Writing backup-my.cnf\n  191221 13:49:03 [00]        ...done\n  191221 13:49:03 [00] Writing xtrabackup_info\n  191221 13:49:03 [00]        ...done\n  xtrabackup: Transaction log of lsn (8622624) to (8622624) was copied.\n  191221 13:49:04 completed OK!#代表成功全量备份\n  ```\n\n+ 在全量备份的基础上进行增量备份\n\n  ```shell\n  #xtrabackup --backup --target-dir=/data/mysql/增量备份文件夹的名字（自定义）--incremental-basedir=/全量备份文件路径\n  [root@ax mysql]# xtrabackup --backup --target-dir=/data/backup/mysql/mysql_increment1 --incremental-basedir=/data/backup/mysql\n  191221 14:00:43  version_check Connecting to MySQL server with DSN 'dbi:mysql:;mysql_read_default_group=xtrabackup' as 'backuper'  (using password: YES).\n  191221 14:00:43  version_check Connected to MySQL server\n  191221 14:00:43  version_check Executing a version check against the server...\n  191221 14:00:43  version_check Done.\n  191221 14:00:43 Connecting to MySQL server host: localhost, user: backuper, password: set, port: not set, socket: not set\n  Using server version 5.5.64-MariaDB\n  xtrabackup version 2.3.10 based on MySQL server 5.6.24 Linux (x86_64) (revision id: bd0d4403f36)\n  incremental backup from 8622624 is enabled.\n  xtrabackup: uses posix_fadvise().\n  xtrabackup: cd to /var/lib/mysql\n  xtrabackup: open files limit requested 0, set to 65535\n  #省略....\n  xtrabackup: Stopping log copying thread.\n  .191221 14:00:45 >> log scanned up to (8622624)\n  \n  191221 14:00:45 Executing UNLOCK TABLES\n  191221 14:00:45 All tables unlocked\n  191221 14:00:45 Backup created in directory '/data/backup/mysql/mysql_increment1/'\n  MySQL binlog position: filename 'mysql-bin.000001', position '245'\n  191221 14:00:45 [00] Writing backup-my.cnf\n  191221 14:00:45 [00]        ...done\n  191221 14:00:45 [00] Writing xtrabackup_info\n  191221 14:00:45 [00]        ...done\n  xtrabackup: Transaction log of lsn (8622624) to (8622624) was copied.\n  191221 14:00:45 completed OK!#代表增量备份成功\n  ```\n\n+ 在增量备份的基础上继续增量备份\n\n  ```shell\n  #在第一次增量备份后，以后的每一次增量备份都是以上一次增量备份为基准\n  #xtrabackup --backup --target-dir=增量备份文件路径 --incremental-basedir=上次增量备份的文件位置\n  [root@ax ~]# xtrabackup --backup --target-dir=/data/backup/mysql_increment2 --incremental-basedir=/data/backup/mysql/mysql_increment1\n  191221 18:39:28  version_check Connecting to MySQL server with DSN 'dbi:mysql:;mysql_read_default_group=xtrabackup' as 'backuper'  (using password: YES).\n  191221 18:39:28  version_check Connected to MySQL server\n  191221 18:39:28  version_check Executing a version check against the server...\n  191221 18:39:28  version_check Done.\n  191221 18:39:28 Connecting to MySQL server host: localhost, user: backuper, password: set, port: not set, socket: not set\n  Using server version 5.5.64-MariaDB\n  xtrabackup version 2.3.10 based on MySQL server 5.6.24 \n  #省略...\n  191221 18:39:32 Executing UNLOCK TABLES\n  191221 18:39:32 All tables unlocked\n  191221 18:39:32 Backup created in directory '/data/backup/mysql_increment2/'\n  MySQL binlog position: filename 'mysql-bin.000001', position '245'\n  191221 18:39:32 [00] Writing backup-my.cnf\n  191221 18:39:32 [00]        ...done\n  191221 18:39:32 [00] Writing xtrabackup_info\n  191221 18:39:32 [00]        ...done\n  xtrabackup: Transaction log of lsn (8622624) to (8622624) was copied.\n  191221 18:39:32 completed OK!#代表成功\n  ```\n\n## 使用xtrabackup恢复##\n\n+ 语法：\n\n  + xtrabackup --prepare --apply-log-only --target-dir=全量备份文件路径\n\n    + --prepare  表示还原\n    + --apply-log-only 表示不回滚事务，因为后面有基于全备的增量备份，所以不需要回滚，如果没有增量备份则可以不添加。\n\n  + 将第一次增量备份加载至全备中（增量备份多每次都要以上一次加载的备份文件为基准，命令相同，只需修改增量备份文件的路径即可。）\n\n  + 在加载最后一次的增量备份文件时，不需要添加--apply-log-only，因为增量备份都加载完成了，所以需要事务回滚。\n\n    ```shell\n    #xtrabackup --prepare --apply-log-only --target-dir=全量备份文件路径 --incremental-dir=增量备份文件路径\n    [root@ax ~]# xtrabackup --prepare --apply-log-only --target-dir=/data/backup/mysql --incremental-dir=/data/backup/mysql_increment1\n    xtrabackup version 2.3.10 based on MySQL server 5.6.24 Linux (x86_64) (revision id: bd0d4403f36)\n    incremental backup from 8622624 is enabled.\n    xtrabackup: cd to /data/backup/mysql/\n    xtrabackup: This target seems to be already \n    #省略...\n    191221 18:56:43 [01]        ...done\n    191221 18:56:43 [01] Copying /data/backup/mysql_increment1/mysql/db.MYD to ./mysql/db.MYD\n    191221 18:56:43 [01]        ...done\n    191221 18:56:43 [00] Copying /data/backup/mysql_increment1//xtrabackup_binlog_info to ./xtrabackup_binlog_info\n    191221 18:56:43 [00]        ...done\n    191221 18:56:43 [00] Copying /data/backup/mysql_increment1//xtrabackup_info to ./xtrabackup_info\n    191221 18:56:43 [00]        ...done\n    191221 18:56:43 completed OK!#代表成功\n    #最后一加载增量备份到全量备份\n    [root@ax ~]# xtrabackup --prepare --target-dir=/data/backup/mysql --incremental-dir=/data/backup/mysql_increment2\n    #返回也上面最后一行的结果代表成功\n    ```\n\n+ 恢复\n\n  + 停止mysql服务\n\n  + 清空mysql的数据目录\n\n    ```she\n    MariaDB [(none)]> show variables like 'datadir'; #查询数据目录\n    +---------------+-----------------+\n    | Variable_name | Value           |\n    +---------------+-----------------+\n    | datadir       | /var/lib/mysql/ |\n    +---------------+-----------------+\n    1 row in set (0.00 sec)\n    #cd到指定目录 rm -rf ./*  这是模拟数据库损坏\n    ```\n\n  + 恢复 `xtrabackup --copy-back --target-dir=/data/backup/mysql`  --copy-back 将备份的数据目录下\n\n    ```shel\n    [root@ax ~]# xtrabackup --copy-back --target-dir=/data/backup/mysql\n    xtrabackup version 2.3.10 based on MySQL server 5.6.24 Linux (x86_64) (revision id: bd0d4403f36)\n    191221 19:23:55 [01] Copying ib_logfile0 to /var/lib/mysql/ib_logfile0\n    191221 19:23:55 [01]        ...done\n    191221 19:23:55 [01] Copying ib_logfile1 to /var/lib/mysql/ib_logfile1\n    191221 19:23:55 [01]        ...done\n    191221 19:23:55 [01] Copying ibdata1 to /var/\n    ```\n\n  + 恢复后的数据目录下的文件及文件夹，用户数属于root的,mysql用户是没有权限使用的，所以需要重新赋予权限\n\n    ```shell\n    [root@ax mysql]# ll\n    total 36896\n    drwx------ 2 root root     4096 Dec 21 19:23 exam\n    -rw-r----- 1 root root 27262976 Dec 21 19:23 ibdata1\n    -rw-r----- 1 root root  5242880 Dec 21 19:23 ib_logfile0\n    -rw-r----- 1 root root  5242880 Dec 21 19:23 ib_logfile1\n    drwx------ 2 root root     4096 Dec 21 19:23 mysql\n    drwx------ 2 root root     4096 Dec 21 19:23 nextcloud\n    drwx------ 2 root root     4096 Dec 21 19:23 performance_schema\n    drwx------ 2 root root     4096 Dec 21 19:23 siyouyun\n    drwx------ 2 root root     4096 Dec 21 19:23 text\n    -rw-r----- 1 root root       23 Dec 21 19:23 xtrabackup_binlog_pos_innodb\n    -rw-r----- 1 root root      548 Dec 21 19:23 xtrabackup_info\n    #重新赋予权限，所属者和所属组改为mysql\n    [root@ax mysql]# chown -R mysql:mysql ../mysql\n    [root@ax mysql]# ll\n    total 36896\n    drwx------ 2 mysql mysql     4096 Dec 21 19:23 exam\n    -rw-r----- 1 mysql mysql 27262976 Dec 21 19:23 ibdata1\n    -rw-r----- 1 mysql mysql  5242880 Dec 21 19:23 ib_logfile0\n    -rw-r----- 1 mysql mysql  5242880 Dec 21 19:23 ib_logfile1\n    drwx------ 2 mysql mysql     4096 Dec 21 19:23 mysql\n    drwx------ 2 mysql mysql     4096 Dec 21 19:23 nextcloud\n    drwx------ 2 mysql mysql     4096 Dec 21 19:23 performance_schema\n    drwx------ 2 mysql mysql     4096 Dec 21 19:23 siyouyun\n    drwx------ 2 mysql mysql     4096 Dec 21 19:23 text\n    -rw-r----- 1 mysql mysql       23 Dec 21 19:23 xtrabackup_binlog_pos_innodb\n    -rw-r----- 1 mysql mysql      548 Dec 21 19:23 xtrabackup_info\n    \n    ```\n\n  + 重新启动mysql服务查看\n\n    ```shell\n    [root@ax mysql]# systemctl restart mariadb\n    [root@ax mysql]# mysql -uroot -p\n    Enter password: \n    Welcome to the MariaDB monitor.  Commands end with ; or \\g.\n    Your MariaDB connection id is 2\n    Server version: 5.5.64-MariaDB MariaDB Server\n    \n    Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.\n    \n    Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n    \n    MariaDB [(none)]> show databases;\n    +--------------------+\n    | Database           |\n    +--------------------+\n    | information_schema |\n    | exam               |\n    | mysql              |\n    | nextcloud          |\n    | performance_schema |\n    | siyouyun           |\n    | text               |\n    +--------------------+\n    7 rows in set (0.00 sec)\n    \n    MariaDB [(none)]> \n    #恢复成功\n    ```\n\n+ 单表/单库备份\n\n  ```shell\n  单表：xtrabackup --backup --datadir=数据目录路径 --tables='库名.表名' --target-dir=备份文件路径\n  \n  --tables:单引号中填写databases.tables\n  \n  单库：trabackup --backup  --databases=数据库名 --target-dir=备份文件路径 \n  --databases:库名（database）\n  ```\n\n  ```shell\n  [root@ax ~]# xtrabackup --backup --datadir=/var/lib/mysql --target-dir=/data/backup/mysql --tables='exam.a1';\n  #还原也需要进行事务回滚\n  [root@ax exam]# xtrabackup --prepare --target-dir=单表或单库备份文件\n  #停止数据库\n  #数据目录中的原表删除，复制备份文件中单表或单库文件到数据目录\n  #重置所属用户和组到mysql\n  #重启数据库\n  #成功\n  ```\n\n##使用innobackupex的备份与恢复\n\n+ innobackupex封装了xtrabackup，支持Myisam的数据表\n\n+ innobackupex完整备份后生成的几个重要文件：\n\n  + 记录当前最新的Log position\n  + xtrabackup_binlog_pos_innodb:innodb log position\n  + xtrabackup_checkpoints:存放备份的起始LSN（beginlsn），和结束的位置LSN（endlsn）\n  + 增量备份需要上次备份的endlsn\n\n+ innobackupex命令相当于冷备份，复制数据目录的索引，数据结构文件，为保证数据一致，需要短暂的锁表（时间的长短依赖于Myisam表的大小。）\n\n+ 参数解释（同样适用于xtrabackup）\n\n  ```python\n  #--defaults-file：指定my.cnf参数文件的位置[此配置文件里必须指定datadir]\n  #--apply-log：同xtrabackup的--prepare参数,一般情况下,在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务。因此，此时数据 文件仍处理不一致状态。--apply-log的作用是通过回滚未提交的事务及同步已经提交的事务至数据文件使数据文件处于一致性状态。\n  #--copy-back：做数据恢复时将备份数据文件拷贝到MySQL服务器的datadir\n  #--remote-host=HOSTNAME： 通过ssh将备份数据存储到进程服务器上\n  #--stream=[tar]：备份文件输出格式, 该文件可在XtarBackup binary文件中获得. 在使用参数stream=tar备份的时候,你的xtrabackup_logfile可能会临时放在/tmp目录下,如果你备份的时候并发写入较大的话,xtrabackup_logfile可能会很大(5G+),很可能会撑满你的/tmp目录,可以通过参数--tmpdir指定目录来解决这个问题.\n  #--tmpdir=DIRECTORY：当有指定--remote-host or --stream时, 事务日志临时存储的目录, 默认采用MySQL配置文件中所指定的临时目录tmpdir\n  #--redo-only --apply-log：强制备份日志时只redo,跳过rollback,这在做增量备份时非常必要\n  #--use-memory=*：该参数在prepare的时候使用,控制prepare时innodb实例使用的内存\n  #--databases=LIST：列出需要备份的databases,如果没有指定该参数,所有包含MyISAM和InnoDB表的database都会被备份\n  #--slave-info：备份从库, 加上--slave-info备份目录下会多生成一个xtrabackup_slave_info 文件, 这里会保存主日志文件以及偏移, 文件内容类似于:CHANGE MASTER TO MASTER_LOG_FILE='', MASTER_LOG_POS=0\n  #--socket=SOCKET：指定mysql.sock所在位置，以便备份进程登录mysql.\n  ```\n\n+ 全量备份\n\n  ```shell\n  [root@ax backup]# innobackupex --user=备份账户 --password=密码 备份文件存储路径\n  191223 10:08:18 innobackupex: Starting the backup operation\n  \n  IMPORTANT: Please check that the backup run completes successfully.\n             At the end of a successful backup run innobackupex\n             prints \"completed OK!\".\n  #省略...\n  191223 10:08:20 completed OK!#代表成功\n  ```\n\n+ 全量恢复\n\n  ```shell\n  #执行innobackupex --apply-log --use-memory=size（可加可不加） 备份文件路径    这一步是准备操作。apply-only在上面介绍过。\n  [root@ax backup]# innobackupex --apply-only --use-memory=4G /data/backup/2019-12-23_10-08-18\n  191223 10:16:02 innobackupex: Starting the backup operation\n  \n  IMPORTANT: Please check that the backup run completes successfully.\n             At the end of a successful backup run innobackupex\n             prints \"completed OK!\".\n  #省略...\n  191223 10:16:04 completed OK!#代表成功\n  \n  #接下来是恢复\n  #关闭mysql服务,模拟数据库损坏，清空数据目录（不清空会报错）\n  #执行 innobackupex --copy-back 备份文件路径\n  #给予mysql数据目录下所有文件的操作权限\n  #启动mysql\n  ```\n\n+ 增量备份（所有的增量备份都是在全量备份的基础上进行的）\n\n  ```shell\n  #执行 innobackupex --incremental 指定增量文件存储路径 --incremental-basedir=第一次是全量备份的文件路径/在增量的备份的基础上继续增量这里就需要填写上次增量备份文件的路径\n  [root@ax backup]# innobackupex --incremental /data/backup/ --incremental-basedir=/data/backup/2019-12-23_11-25-13/\n  xtrabackup: Transaction log of lsn (8626208) to (8626208) was copied.\n  191223 11:34:20 completed OK!#代表成功\n  ```\n\n+ 恢复：（将各个增量备份的数据文件合并到全量备份的目录下，最终是从全量备份的这个目录上进行恢复的）\n\n  ```shell\n  #准备工作\n  #innobackupex --apply-log --redo-only 全被文件路径\n  #innobackupex --apply-log --redo-only 全备路径 --incremental-dir=第一次增备路径\n  #innobackupex --apply-log --redo-only 全备文件路径 --incremental-dir=第二次增备路径 #多次增备执行多次\n  #innobackupex --apply-log 全备路径 --incremental-dir=最后一次增备路径 最后一次不需要填加--redo-only参数\n  #恢复\n  #关闭mysql服务,模拟数据库损坏，清空数据目录（不清空会报错）\n  #innobackupex --copy-back 全量备份路径 \n  #给予mysql数据目录下所有文件的操作权限\n  #启动mysql\n  ```\n\n","tags":["使用xtrabackup备份"]},{"title":"如何自我介绍","url":"/2019/12/23/pout/自我介绍/","content":"\n\n\n\n## 如何自我介绍(语气要抑扬顿挫，神态要声情并茂，不能机械的没有感情的背)\n\n一般人的自我介绍过于平常，只说姓名、年龄、爱好、工作经验，这些在简历上都有，其实，企业最希望知道的是求职者能否胜任工作，包括：最强的技能、最深入研究的知识领域、个性中最积极的部分、做过的最成功的事，主要的成就等\n\n您好（下午好/上午好），我15年毕业，大学所学的专业是计算机，毕业之后来到北京，一开始在一家创业型团队起步，当时主力开发语言是python，版本号我记得还很清楚是python2.7，在公司主要和业务打交道（这里可以简单谈一下主营业务，也可以不谈），大概沉淀了两年左右吧，我跳槽到了，薪酬实现了double，在新的技术团队里，我接触到了微服务架构思想，使用thrift框架写后台接口，同时技术栈也越来越丰富，前端技术也有所涉猎，比如vue框架，先进的数据双向绑定理念，同时也学习了在业务解耦和服务封装层面比较流行的docker容器技术，这项技术使我平时开发和测试工作都提高了效率，最近一年左右吧，我经常使用的web框架是tornado,这个框架我个人非常喜欢，它的异步非阻塞特性让我对异步编程思想的认识更深入了。其实三四年下来，做过的东西解决过的问题也挺多的，待过大公司也经历过小团队，给我的感觉就是互联网企业随着发展，技术和行业边界其实是越来越模糊的，也就是说技术都是具有相通性的，我个人来讲，优势就是技术涉猎比较广，前后端都接触过，踩得坑也比较多，在特定领域有一定的深入，比如异步编程这块。另外我觉得搞开发的，学习能力，总结能力很重要，所以我一直保持着写技术博客的习惯，这样经过沉淀，可以提高一个人的分析能力，也就是解决问题的能力，我的介绍完了，谢谢。\n\n# 三大卖点（如何推销）\n\n**微服务**：什么是微服务，什么是rpc，thrift框架的优势和缺点，与传统http接口的差异，能带来什么收益，二级制协议的原理，如何解耦，为什么使用thrift\n\nthrift怎么用，.thrift文件的好处，thrift的8种数据类型，3种容器\n\n<https://v3u.cn/a_id_104>\n\n**docker**:什么是linux容器技术，简化配置，应用隔离，快速部署，服务合并，快速部署，底层原理，隔离性怎样做到？命名空间(namespace),cgroups,docker常用命令\n\n举例子：Docker 提供轻量的虚拟化，你能够从Docker获得一个额外抽象层，你能够在单台机器上运行多个Docker微容器，而每个微容器里都有一个微服务或独立应用，例如你可以将Tornado运行在一个Docker，而MySQL运行在另外一个Docker，两者可以运行在同一个服务器，或多个服务器上。未来可能每个应用都要Docker化，引申出低耦合高内聚的概念\n\n实际业务：docker结合fastdfs docker结合redisearch docker结合redis-sentinel docker-componse来部署编排容器集群，使用dockerfile来搭建自己的镜像环境\n\n个人线下调试：以往别人的代码需要git clone然后搭建环境，现在直接下载docker镜像利用容器调试即可，通过-v命令可以挂在代码并且实时调试\n\n**tornado**异步非阻塞:什么是异步非阻塞，什么是同步阻塞，select poll epoll网络模型，举例子：育婴室。强调tornado单线程，由此引出，线程 进程 协程，tornado的异步写法，引出开发人员综合素质问题，引出原生协程：async和await，怎么使用关键字来实现真正的异步非阻塞特性，再由协程的io多路复用和状态保持与切换引出迭代器和生成器，点出生成器是python协程的底层实现，再由生成器引出性能问题，生成器对象和list对象的区别（range），由线程可以引申出线程安全问题，io密集型任务，由tornado+supervisor配合cpu核心起进程来引出cpu密集型任务。\n\nTornado的异步原理： 单线程的torndo打开一个IO事件循环， 当碰到IO请求（新链接进来 或者 调用api获取数据），由于这些IO请求都是非阻塞的IO，都会把这些非阻塞的IO socket 扔到一个socket管理器，所以，这里单线程的CPU只要发起一个网络IO请求，就不用挂起线程等待IO结果，这个单线程的事件继续循环，接受其他请求或者IO操作，如此循环。\n\n![img](img/tornado.gif)\n\n## IOLoop模块\n\nIOLoop是Tornado的核心，负责服务器的异步非阻塞机制。IOLoop是一个基于level-triggered的I/O事件循环，它使用I/O多路复用模型(select,poll,epoll)监视每个I/O的事件，当指定的事件发生时调用对用的handler处理。\n\n## IOStream模块\n\nIOStream模块封装了file-like(file or socket)的一系列非阻塞读写操作。IOStream对file-like的非阻塞读写进行了缓存，提供了读&写Buffer。当读写操作结束时通过callback通知上层调用者从缓存中读写数据。\n\n## HttpServer模块\n\n服务器模块\n\n## Application模块\n\n实现 URI 转发，将 Application 的实例传递给 httpserver ，当监听到请求时，把服务器传回来的请求进行转发，通过调用 **call** ，处理请求。\n\n## RequestHandeler模块\n\n实现控制器业务的模块\n\n# autoreload模块\n\n实时监测代码修改，也就是debug模式的开关\n\n# 内存管理（重点）\n\n为了探索对象在内存的存储，我们可以求助于Python的内置函数id()。它用于返回对象的身份(identity)。其实，这里所谓的身份，就是该对象的内存地址\n\n在Python中，整数和短小的字符，Python都会缓存这些对象，以便重复使用。当我们创建多个等于1的引用时，实际上是让所有这些引用指向同一个对象\n\n```\na = 1\nb = 1\n\nprint(id(a))\n```\n\n为了检验两个引用指向同一个对象，我们可以用is关键字。is用于判断两个引用所指的对象是否相同。由此引出 is 和 == 的区别\n\n同时，由内存地址和链接指向引出 fastdfs的去重功能和文件指纹概念，还能引出linux中的软链接体系\n\n还能扩展出设计模式中的单例模式\n\n同时需要记忆的是，获取一个元素的内存占用量：sys.getsizeof() 单位是字节\n\n## 引用计数\n\n在Python中，每个对象都有存有指向该对象的引用总数，即引用计数(reference count)。\n\n我们可以使用sys包中的getrefcount()，来查看某个对象的引用计数。需要注意的是，当使用某个引用作为参数，传递给getrefcount()时，参数实际上创建了一个临时的引用。因此，getrefcount()所得到的结果，会比期望的多1。\n\n某个对象的引用计数可能减少。比如，可以使用del关键字删除某个引用\n\n```\nfrom sys import getrefcount\n\na = [1, 2, 3]\nb = a\nprint(getrefcount(b))\n\ndel a\nprint(getrefcount(b))\n```\n\n## 循环引用\n\n两个对象可能相互引用，从而构成所谓的引用环（循环引用）\n\n```\na = []\nb = [a]\na.append(b)\n```\n\n## 垃圾回收\n\n从基本原理上，当Python的某个对象的引用计数降为0时，说明没有任何引用指向该对象，该对象就成为要被回收的垃圾了。比如某个新建对象，它被分配给某个引用，对象的引用计数变为1。如果引用被删除，对象的引用计数为0，那么该对象就可以被垃圾回收\n\n然而，减肥是个昂贵而费力的事情。垃圾回收时，Python不能进行其它的任务。频繁的垃圾回收将大大降低Python的工作效率。如果内存中的对象不多，就没有必要总启动垃圾回收。所以，Python只会在特定条件下，自动启动垃圾回收。当Python运行时，会记录其中分配对象(object allocation)和取消分配对象(object deallocation)的次数。当两者的差值高于某个阈值时，垃圾回收才会启动\n\n## 分代回收\n\nPython同时采用了分代(generation)回收的策略。这一策略的基本假设是，存活时间越久的对象，越不可能在后面的程序中变成垃圾。我们的程序往往会产生大量的对象，许多对象很快产生和消失，但也有一些对象长期被使用。出于信任和效率，对于这样一些“长寿”对象，我们相信它们的用处，所以减少在垃圾回收中扫描它们的频率。\n\nPython将所有的对象分为0，1，2三代。所有的新建对象都是0代对象。当某一代对象经历过垃圾回收，依然存活，那么它就被归入下一代对象。垃圾回收启动时，一定会扫描所有的0代对象。如果0代经过一定次数垃圾回收，那么就启动对0代和1代的扫描清理。当1代也经历了一定次数的垃圾回收后，那么会启动对0，1，2，即对所有对象进行扫描。\n\n# 数据类型和基本数据结构（重点）\n\nlist tuple dict set 区别，常用方法，使用场景，底层实现\n\n扩展点：有序无序，可变不可变（由此引出深拷贝浅拷贝，python传参机制，由传参又引申出不定长参数，不定长参数又可以反着引出tuple和dict，也就是实际应用场景），同构异构，列表推导式，字典推导式，集合推导式，生成器推导式\n\n数据结构：数组 栈（子弹夹，后进先出） 队列（先进先出，由此扩展任务队列，消息队列，rabbitmq和redis，以及进程间通信） 树（层级结构，btree,b+tree，引申出索引，递归算法） 链表（单双向，闭环） 散列表（引申出dict，hash,hash一致性算法）\n\n# python基础（重点）\n\n解释器，其他的解释器，全局解释器锁（存在原因，如果避免，如何更换解释器），python自省，切片，负索引，装饰器，手写装饰器，手写单例模式（好处和坏处，使用背景，又单例模式引申出数据库连接池），手写工厂模式，上下文管理，lambda表达式(优点和缺点)，命名空间，三元运算，magic方法(init和new的区别)，继承，类方法静态方法，magic方法，四大高阶函数（引申python2和3区别），线程 进程 协程（引申出原生协程async await和greenlet以及gevent的区别，引申出大文件操作<https://v3u.cn/a_id_97）> 迭代器和生成器（yield和return区别，yield和send的区别）\n\n# 算法\n\n递归（引申出实际业务：无限分类,celery遇到的关键字问题，使用递归脚本进行修改），八大查找，八大排序，完全手写，时间复杂度，空间复杂度\n\n# 数据库（重中之重）\n\n**mysql**:sql优化，事务，并发事务带来的脏读，幻读，事务隔离级别（四个），主从热备读写分离(<https://v3u.cn/a_id_85>)\n\nbinlog日志相关，主从同步延迟问题（架构层面和硬件层面），悲观锁和乐观锁，引擎（myisam和innnodb），表锁和行锁，索引（优缺点和使用场景），索引存储结构(b+tree和btree (<https://v3u.cn/a_id_91>)\n\n联合索引，最左前缀原则，梯度漏斗，执行计划(explain),慢查询日志（showprocesslist），分表（水平和垂直），44道场景题（必须闭着眼睛也能写出来），备份\n\n**redis**:特点，背景，memcached和redis区别，为什么快（引申io多路复用，避免上下文切换，引申和协程的相同点），数据类型：string，list（引申队列，消息中间件），set（去重功能引申商品标签，在线人数统计），sorted set（有序集合引申出排行榜），hash 分别深入研究，各个数据类型的实现原理，使用背景，持久化方案（三种，怎么用，在什么场景下用什么方案），缓存集群（哨兵模式，同步，选举），并发竞争问题和分布式锁（setnx），incr(自动计数)，与mysql数据同步问题怎么解决（终极方案由mysql的binlog日志入手，撰写同步脚本），redisearch全文检索（为什么用，背景，怎么用）\n\n**mongodb**:底层，实现原理，背景，bson和json,聚合，备份，和mysql的区别，基本语法，分组查询，异步读写（Motor和pymongo的区别，配合tornado如何实现异步操作），命名空间\n\n# 网络编程（重点）\n\n网络七层协议，tcp协议（三次握手四次挥手）,udp协议，http和websocket(websocket心跳以及重连问题),http和https,ipv4和ipv6,4g和5g,32位和64位，跨域问题（浏览器同源策略问题，如何解决），http所有状态码,get和post区别，delete和put以及options的区别\n\n# celery和rabbitmq/redis(重点)\n\n工作原理，使用背景(永远不要说发短信和邮件，直接说用celery任务队列执行爬虫任务)，生产者消费者模型，消息中间件，rabbbitmq和redis区别，Broker，Worker,Backend ，重复消费问题，消息丢失问题，消息稳定性问题\n\n# jwt和token\n\n背景，为什么用，怎么用，token(三部分，各有什么用)，引申前后端分离，引申单点登录(sso)，csrf跨域攻击，自验证流程，签名，session和cookie和webstorage区别和用法：<https://v3u.cn/a_id_94>\n\n# 高负载和高可用\n\nqps,pv,uv,留存用户，活跃用户，流量，带宽，负载均衡结构图，nginx负载均衡策略，几台机器，数据层，服务层，缓存层\n\n话术：<https://v3u.cn/a_id_95>\n\n具体操作：<https://v3u.cn/a_id_87>\n\n# Supervisor和Fastdfs\n\n守护进程，如何配置，supervisor+tornado,文件指纹，分布式文件存储方案（引申阿里云oss），文件hash（hash一致性算法）\n\n<https://v3u.cn/a_id_102>\n\n<https://v3u.cn/Index_a_id_76>\n\n# 行业背景\n\n5g 新能源 短视频 p2p 在线教育 少儿编程 区块链 微服务 低耦合高内聚 车联网\n\n# 你在大厂也是前后端都做，而且还会部署？\n\n我在公司主要负载后端的微服务架构(thrift)和web接口(tornado)，但是同时我对前端也很感兴趣，自己喜欢私下研究新技术，也经常和公司的前端有沟通和联调，我本人自己也有开源的vue项目和基于mpvue的小程序也上线过，没有人逼我，完全是我对自己的严格要求，我始终认为只有拥抱新技术才能进步，并且架构师的职业规划也促使我关注前端用来反哺后端，同时我自己的个人网站也是自己开发的，用于练手的同时也会实验新技术，所以前端也是我的强项之一，没有任何问题。\n\n# 你对加班的看法？\n\n回答提示：实际上好多公司问这个问题，并不证明一定要加班。 只是想测试你是否愿意为公司奉献。\n\n回答样本：如果是工作需要我会义不容辞加班。我现在单身，没有任何家庭负担，可以全身心的投入工作。但同时，我也会提高工作效率，减少不必要的加班，也就是不会为了加班而加班\n\n# 你对薪资的要求？\n\n回答提示：如果你对薪酬的要求太低，那显然贬低自己的能力；如果你对薪酬的要求太高，那又会显得你分量过重，公司受用不起。一些雇主通常都事先对求聘的职位定下开支预算，因而他们第一次提出的价钱往往是他们所能给予的最高价钱。他们问你只不过想证实一下这笔钱是否足以引起你对该工作的兴趣。\n\n回答样本一：“我对工资没有硬性要求。我相信贵公司在处理我的问题上会友善合理。我注重的是找对工作机会，所以只要条件公平，我则不会计较太多\n\n回答样本二：我受过系统的软件编程的训练，不需要进行大量的培训。而且我本人也对编程特别感兴趣。因此，我希望公司能根据我的情况和市场标准的水平，给我合理的薪水。\n\n回答样本三：如果你必须自己说出具体数目，请不要说一个宽泛的范围，那样你将只能得到最低限度的数字。最好给出一个具体的数字，这样表明你已经对当今的人才市场作了调查，知道像自己这样学历的雇员有什么样的价值。\n\n# 你的职业规划?\n\n最普通的回答应该是“我准备在技术领域有所作为”或“我希望能按照公司的管理思路发展”,不过也可以参照首页的30年规划\n\n# 你能为公司带来什么？\n\n我目前可以帮助公司在业务层解耦，在技术层优化，可以提供多个解决方案的同时，寻找最优方案\n\n最拿手的技术目前是脚本语言python，也就是后端技术一直在深研，同时我也关注前端技术，反哺后端\n\n独立支撑过公司的主力项目，从需求调研和分析，到项目原型图的构建，功能设计，具体接口实现，有高负载高并发系统的架构经验，对系统性能优化有自己的见解和解决方案\n\n# 你还有什么问题要问吗？\n\n企业不喜欢说“没有问题”的人，因为其很注重员工的个性和创新能力。企业不喜欢求职者问个人福利之类的问题\n\n贵公司对新入公司的员工有没有什么培训项目，我可以参加吗？贵公司的晋升机制是什么样的？贵公司有定期的技术分享吗？\n\n# 谈谈你对跳槽的看法？\n\n正常的\"跳槽\"能促进人才合理流动，应该支持；\n\n频繁的跳槽对单位和个人双方都不利，应该反对。\n\n# 你对于我们公司了解多少？\n\n在去公司面试前上网查一下该公司主营业务，近期新闻，或者职位描述深入挖掘一下\n\n# 你之前的待遇是？\n\n根据面试效果随时调整(15k)，13薪（年底一个月的年终奖），全勤奖，项目奖金，大厂有限制出售的股票（通常国外上市是美股，国内上市是a股，你就说给了100股，但是限制出售，需要待够5年才能全部售出）\n","tags":["自我介绍"]},{"title":"Docker简介","url":"/2019/12/23/pout/Docker/Docker简介/","content":"\n\n<!-- toc -->\n\n\n# 什么是Docker\n\nDocker 是一个开源的应用容器引擎，可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在本地编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。\n\n简单的理解，Docker类似于集装箱，各式各样的货物，经过集装箱的标准化进行托管，而集装箱和集装箱之间没有影响。也就是说，Docker平台就是一个软件集装箱化平台，这就意味着我们自己可以构建应用程序，将其依赖关系一起打包到一个容器中，然后这容器就很容易运送到其他的机  器上进行运行，而且非常易于装载、复制、移除，非常适合软件弹性架构。 \n\n因此，就像船只、火车或卡车运输集装箱而不论其内部的货物一样，软件容器充当软件部署的标准单元，其中可以包含不同的代码和依赖项。 按照这种方式容器化软件，开发人员和 IT 专业人员只需进行极少修改或不修改，即可将其部署到不同的环境。\n\n总而 言之，Docker 是一个开放平台，使开发人员和管理员可以在称为容器的松散隔离的环境中构建镜像、交付和运行分布式应用程序。以便在开发、QA 和生产环境之间进行高效的应用程序生命周期管理。\n\n# Docker能解决什么问题  \n\n高效有序利用资源\n\n- 机器资源有限；\n- 单台机器得部署多个应用；\n- 应用之间互相隔离；\n- 应用之间不能发生资源抢占，每个应用只能使用事先注册申请的资源。\n\n一次编译，到处运行\n\n- 类似于java代码，应用及依赖的环境构建一次，可以到处运行\n\n\n\n# Docker架构\n\nDocker使用C/S架构，Client 通过接口与Server进程通信实现容器的构建，运行和发布。client和server可以运行在同一台集群，也可以通过跨主机实现远程通信。\n\n![在这里插入图片描述](https://img-blog.csdnimg.cn/20181108181808777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NsZXZlckNvZGU=,size_16,color_FFFFFF,t_70)\n\n\n\n# Docker核心原理\n\n### Namespaces\n\n命名空间（namespaces）是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。\n\n[![3.png](http://dockone.io/uploads/article/20190625/6498a2be3f3ddcb2a032df838ac24069.png)](http://dockone.io/uploads/article/20190625/6498a2be3f3ddcb2a032df838ac24069.png)\n\n在这种情况下，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这也是我们不想看到的，而 Docker 其实就通过 Linux 的 Namespaces 对不同的容器实现了隔离。\n\nLinux 的命名空间机制提供了以下七种不同的命名空间，包括 CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET、CLONE_NEWNS、CLONE_NEWPID、CLONE_NEWUSER 和 CLONE_NEWUTS，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。\n\n# Docker镜像原理\n\n1. ## 镜像是什么\n\n  镜像是一种轻量级、可执行的独立软件包，用来打包软件运行环境和基于运行环境开发的软件, 它包含运行某个软件所需的所有内容，包括代码、运行时、库、环境变量和配置文件。\n\n### 1.1 UnionFS(联合文件系统)\n\nUnionFS(联合文件系统): Union文件系统(UnionFS)是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union文件系统是Docker镜像的基础。镜像可以通过分层来进行继承, 基于基础镜像(没有父镜像)， 可以制作各种具体的应用镜像。\n特性: 一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。\n\n### 1.2 Docker镜像加载原理\n\ndocker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。\n\n**bootfs(boot file system)**主要包含bootloader和kernel，bootloader主要是引导加载kernel，Linux刚启动时会加载bootfs文件系统，在Docker镜像的最底层是bootfs。这一层与我们典型的Linux/Unix系统是一样的, 包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。\n\nrootfs(root file system), 在bootfs之上。包含的就是典型Linux系统中的/dev, /proc, /bin, /etc等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。\n\n\n平时我们安装虚拟机的CentOS都是好几个G，为什么docker这里才200M？linux mini 200G 2G \n\n对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供rootfs就行了。由此可见对于不同的linux发行版，bootfs基本是一致的，rootfs会有差别，因此不同的发行版可以共用bootfs。\n\n### 1.3 分层的镜像\n\n以我们的pull为例，在下载的过程中我们可以看到docker的镜像好像是在一层一层的在下载\n\n### 1.4 为什么docker镜像要采用这种分层结构呢\n\n最大的一个好吃就是共享资源\n比如：有多个镜像都从相同的base镜像构建而来，那么宿主机只需在磁盘上保存一份base镜像,同时内存中也只需加载一份base镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。\n\n\n2. #### 镜像的特点\n\n  Docker镜像都是只读的\n  当容器启动时，一个新的可写层被加载到镜像的顶部。\n  这一层通常被称作为\"容器层\"，“容器层”之下的都叫\"镜像层\"。\n\n# Docker容器原理\n\n**Docker 容器通过 Docker 镜像来创建，容器与镜像的关系类似于面向对象编程中的对象与类。**\n\n如图所示基本架构：\n\n\n\n![阿里P8架构师谈:Docker容器的原理、特征、基本架构、与应用场景](http://p3.pstatp.com/large/pgc-image/1536290868344c25780cff9)\n\n\n\n**Docker 镜像(Images)**\n\nDocker 镜像是用于创建 Docker 容器的模板。\n\n**Docker 容器(Container)**\n\n容器是独立运行的一个或一组应用。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Docker"]},{"title":"MYSQL慢查询概念及配置","url":"/2019/12/23/pout/数据库/mysql/慢查询/","content":"\n<!-- toc -->\n\n### 1 概念\n\nMySQL的慢查询，全名是慢查询日志，是MySQL提供的一种日志记录，用来记录在MySQL中响应时间超过阀值的语句。\n\n具体环境中，运行时间超过long_query_time值的SQL语句，则会被记录到慢查询日志中。\n\nlong_query_time的默认值为10，意思是记录运行10秒以上的语句。\n\n默认情况下，MySQL数据库并不启动慢查询日志，需要手动来设置这个参数。\n\n当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。\n\n慢查询日志支持将日志记录写入文件和数据库表。\n\n官方文档，关于慢查询的日志介绍如下（部分资料，具体参考官方相关链接）：\n\n### 2 参数\n\nMySQL 慢查询的相关参数解释：\n\nslow_query_log：是否开启慢查询日志，1表示开启，0表示关闭。\n\nlog-slow-queries ：旧版（5.6以下版本）MySQL数据库慢查询日志存储路径。可以不设置该参数，系统则会默认给一个缺省的文件host_name-slow.log\n\nslow-query-log-file：新版（5.6及以上版本）MySQL数据库慢查询日志存储路径。可以不设置该参数，系统则会默认给一个缺省的文件host_name-slow.log\n\nlong_query_time：慢查询阈值，当查询时间多于设定的阈值时，记录日志。\n\nlog_queries_not_using_indexes：未使用索引的查询也被记录到慢查询日志中（可选项）。\n\nlog_output：日志存储方式。log_output='FILE'表示将日志存入文件，默认值是'FILE'。log_output='TABLE'表示将日志存入数据库。\n\n### 3 配置\n\n3.1 slow_query_log\n默认情况下slow_query_log的值为OFF，表示慢查询日志是禁用的，可以通过设置slow_query_log的值来开启，如下所示：\n\nmysql> show variables  like '%slow_query_log%';\n +---------------------+-----------------------------------------------+\n | Variable_name       | Value                                         |\n +---------------------+-----------------------------------------------+\n | slow_query_log      | OFF                                           |\n | slow_query_log_file | /home/WDPM/MysqlData/mysql/DB-Server-slow.log |\n +---------------------+-----------------------------------------------+\n 2 rows in set (0.00 sec)\n\nmysql> set global slow_query_log=1;\n Query OK, 0 rows affected (0.09 sec)\n使用set global slow_query_log=1开启了慢查询日志只对当前数据库生效，MySQL重启后则会失效。\n\n如果要永久生效，就必须修改配置文件my.cnf（其它系统变量也是如此）。\n\nmy.cnf要增加或修改参数slow_query_log 和slow_query_log_file，如下所示\n\nslow_query_log = 1\nslow_query_log_file = /tmp/mysql_slow.log\n然后重启MySQL服务器。\n\n3.2 slow_query_log_file\n这个参数用于指定慢查询日志的存放路径，缺省情况是host_name-slow.log文件，\n\nmysql> show variables like 'slow_query_log_file';\n +---------------------+-----------------------------------------------+\n | Variable_name       | Value                                         |\n +---------------------+-----------------------------------------------+\n | slow_query_log_file | /home/WDPM/MysqlData/mysql/DB-Server-slow.log |\n +---------------------+-----------------------------------------------+\n 1 row in set (0.00 sec)\n3.3 long_query_time\n开启了慢查询日志后，什么样的SQL才会记录到慢查询日志里面呢？\n\n这个是由参数long_query_time控制，默认情况下long_query_time的值为10秒，可以使用命令修改，也可以在my.cnf参数里面修改。\n\n关于运行时间正好等于long_query_time的情况，并不会被记录下来。\n\n也就是说，在mysql源码里是判断大于long_query_time，而非大于等于。\n\n从MySQL 5.1开始，long_query_time开始以微秒记录SQL语句运行时间，之前仅用秒为单位记录。\n\n如果记录到表里面，只会记录整数部分，不会记录微秒部分。\n\nmysql> show variables like 'long_query_time%';\n +-----------------+-----------+\n | Variable_name   | Value     |\n +-----------------+-----------+\n | long_query_time | 10.000000 |\n +-----------------+-----------+\n 1 row in set (0.00 sec)\n\nmysql> set global long_query_time=4;\n Query OK, 0 rows affected (0.00 sec)\n\nmysql> show variables like 'long_query_time';\n +-----------------+-----------+\n | Variable_name   | Value     |\n +-----------------+-----------+\n | long_query_time | 10.000000 |\n +-----------------+-----------+\n 1 row in set (0.00 sec)\n如上所示，我修改了变量long_query_time，但是查询变量long_query_time的值还是10，难道没有修改到呢？\n\n注意：使用命令 set global long_query_time=4修改后，需要重新连接或新开一个会话才能看到修改值。\n\n用show variables like 'long_query_time'查看是当前会话的变量值。\n\n也可以不用重新连接会话，而是用show global variables like 'long_query_time';。\n\n3.4 log_output\nlog_output参数指定日志的存储方式。\n\nlog_output='FILE'表示将日志存入文件，默认值也是'FILE'。\n\nlog_output='TABLE'表示将日志存入数据库，这样日志信息就会被写入到mysql.slow_log表中。\n\n同时也支持两种日志存储方式，配置的时候以逗号隔开即可，如：log_output='FILE,TABLE'。\n\n日志记录到系统的专用日志表中，要比记录到文件耗费更多的系统资源。\n\n因此对于需要启用慢查询日志，又需要能够获得更高的系统性能，那么建议优先记录到文件。\n\nmysql> show variables like '%log_output%';\n +---------------+-------+\n | Variable_name | Value |\n +---------------+-------+\n | log_output    | FILE  |\n +---------------+-------+\n 1 row in set (0.00 sec)\n\nmysql> set global log_output='TABLE';\n Query OK, 0 rows affected (0.00 sec)\n\nmysql> show variables like '%log_output%';\n +---------------+-------+\n | Variable_name | Value |\n +---------------+-------+\n | log_output    | TABLE |\n +---------------+-------+\n 1 row in set (0.00 sec)\n\nmysql> select sleep(5) ;\n +----------+\n | sleep(5) |\n +----------+\n |        0 |\n +----------+\n 1 row in set (5.00 sec)\n\nmysql>\n\nmysql> select * from mysql.slow_log;\n +---------------------+---------------------------+------------+-----------+-----------+---------------+----+----------------+-----------+-----------+-----------------+-----------+\n | start_time          | user_host                 | query_time | lock_time | rows_sent | rows_examined | db | last_insert_id | insert_id | server_id | sql_text        | thread_id |\n +---------------------+---------------------------+------------+-----------+-----------+---------------+----+----------------+-----------+-----------+-----------------+-----------+\n | 2016-06-16 17:37:53 | root[root] @ localhost [] | 00:00:03   | 00:00:00  |         1 |             0 |    |              0 |         0 |         1 | select sleep(3) |         5 |\n | 2016-06-16 21:45:23 | root[root] @ localhost [] | 00:00:05   | 00:00:00  |         1 |             0 |    |              0 |         0 |         1 | select sleep(5) |         2 |\n +---------------------+---------------------------+------------+-----------+-----------+---------------+----+----------------+-----------+-----------+-----------------+-----------+\n 2 rows in set (0.00 sec)\n3.5 log-queries-not-using-indexes\n该系统变量指定未使用索引的查询也被记录到慢查询日志中（可选项）。\n\n如果调优的话，建议开启这个选项。\n\n另外，开启了这个参数，其实使用full index scan的SQL也会被记录到慢查询日志。\n\nmysql> show variables like 'log_queries_not_using_indexes';\n +-------------------------------+-------+\n | Variable_name                 | Value |\n +-------------------------------+-------+\n | log_queries_not_using_indexes | OFF   |\n +-------------------------------+-------+\n 1 row in set (0.00 sec)\n\nmysql> set global log_queries_not_using_indexes=1;\n Query OK, 0 rows affected (0.00 sec)\n\nmysql> show variables like 'log_queries_not_using_indexes';\n +-------------------------------+-------+\n | Variable_name                 | Value |\n +-------------------------------+-------+\n | log_queries_not_using_indexes | ON    |\n +-------------------------------+-------+\n 1 row in set (0.00 sec)\n3.6 log_slow_admin_statements\n这个系统变量表示，是否将慢管理语句例如ANALYZE TABLE和ALTER TABLE等记入慢查询日志。\n\nmysql> show variables like 'log_slow_admin_statements';\n +---------------------------+-------+\n | Variable_name             | Value |\n +---------------------------+-------+\n | log_slow_admin_statements | OFF   |\n +---------------------------+-------+\n 1 row in set (0.00 sec)\n3.7 Slow_queries\n如果你想查询有多少条慢查询记录，可以使用Slow_queries系统变量。\n\nmysql> show global status like '%Slow_queries%';\n +---------------+-------+\n | Variable_name | Value |\n +---------------+-------+\n | Slow_queries  | 2104  |\n +---------------+-------+\n 1 row in set (0.00 sec)\n另外，还有log_slow_slave_statements 和 --log-short-format 参数，可到MySQL网站了解。\n\n4 mysqldumpslow工具\n在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活。\n\nMySQL提供了日志分析工具mysqldumpslow\n\n查看mysqldumpslow的帮助信息：\n\n[root@DB-Server ~]# mysqldumpslow --help\n Usage: mysqldumpslow [ OPTS... ] [ LOGS... ]\n\nParse and summarize the MySQL slow query log. Options are\n\n  --verbose    verbose\n  --debug      debug\n  --help       write this text to standard output\n\n  -v           verbose\n  -d           debug\n  -s ORDER     what to sort by (al, at, ar, c, l, r, t), 'at' is default（排序方式）\n                 al: average lock time（平均锁定时间）\n                 ar: average rows sent（平均返回记录数）\n                 at: average query time（平均查询时间）\n                  c: count（访问计数）\n                  l: lock time（锁定时间）\n                  r: rows sent（返回记录）\n                  t: query time（查询时间）\n   -r           reverse the sort order (largest last instead of first)\n   -t NUM       just show the top n queries（返回前面n条数据）\n   -a           don't abstract all numbers to N and strings to 'S'\n   -n NUM       abstract numbers with at least n digits within names\n   -g PATTERN   grep: only consider stmts that include this string（正则匹配模式，大小写不敏感）\n   -h HOSTNAME  hostname of db server for *-slow.log filename (can be wildcard),\n                default is '*', i.e. match all\n   -i NAME      name of server instance (if using mysql.server startup script)\n   -l           don't subtract lock time from total time\n\n\n比如，得到返回记录集最多的10个SQL。\n\nmysqldumpslow -s r -t 10 /database/mysql/mysql06_slow.log\n得到访问次数最多的10个SQL\n\nmysqldumpslow -s c -t 10 /database/mysql/mysql06_slow.log\n得到按照时间排序的前10条里面含有左连接的查询语句。\n\nmysqldumpslow -s t -t 10 -g “left join” /database/mysql/mysql06_slow.log\n另外建议在使用这些命令时结合 | 和more 使用 ，否则有可能出现刷屏的情况。\n\nmysqldumpslow -s r -t 20 /mysqldata/mysql/mysql06-slow.log | more\n","tags":["数据库"]},{"title":"Centos上配置nginx+uwsgi+负载均衡配置","url":"/2019/12/23/pout/Linux/Centos上配置nginx+uwsgi+负载均衡配置/","content":"\n\n<!-- toc -->\n\n\n# 在阿里云Centos上配置nginx+uwsgi+负载均衡配置\n\n\n\n​    负载均衡在服务端开发中算是一个比较重要的特性。因为Nginx除了作为常规的Web服务器外，还会被大规模的用于反向代理后端，Nginx的异步框架可以处理很大的并发请求，把这些并发请求hold住之后就可以分发给后台服务端(backend servers, 后面简称backend)来做复杂的计算、处理和响应，并且在业务量增加的时候可以方便地扩容后台服务器。\n\n​    说白了就是，随着业务和用户规模的增长，仅仅一台服务器无法肩负起高并发的响应，所以需要两台以上的服务器共同分担压力，而分担压力的媒介就是万能的Nginx。\n\n​    ![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190517/20190517162157_34533.jpg)\n\n​    首先，利用wsgi在不同的端口上起两个Django服务，比如8002和8003\n\n​    然后修改nginx网站配置 vim /etc/nginx/conf.d/default.conf，将原uwsgi_pass注释，改成变量绑定\n\n​    \n\n```\nserver {\n    listen       80;\n    server_name  localhost;\n\n    access_log      /root/myweb_access.log;\n    error_log       /root/myweb_error.log;\n\n\n    client_max_body_size 75M;\n\n\n    location / {\n        include uwsgi_params;\n        #uwsgi_pass 127.0.0.1:8000;\n        uwsgi_pass mytest;\n        uwsgi_param UWSGI_SCRIPT mypro.wsgi;\n        uwsgi_param UWSGI_CHDIR  /root/mypro;\n\n    }\n\n    location /static {\n        alias /root/mypro/static;\n    }\n}\n```\n\n```\n\n```\n\n然后修改主配置文件 vim /etc/nginx/nginx.conf，在http配置内添加负载均衡配置\n\n```\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n\n\n    upstream mytest {\n    server 127.0.0.1:8002;  #负载均衡服务器群\n    server 127.0.0.1:8003;\n\t}\n}\n```\n\n然后重启服务即可：\n\n```\nsystemctl restart nginx.service\n```\n\n```\n \n```\n\n```\n值得注意的是常用的负载均衡策略有以下几种：\n```\n\n```\n1、轮询（默认）\n每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。\n\nupstream backserver {\n    server 192.168.0.14;\n    server 192.168.0.15;\n}\n\n\n2、权重 weight\n指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。\n\nupstream backserver {\n    server 192.168.0.14 weight=3;\n    server 192.168.0.15 weight=7;\n}\n\n\n3、ip_hash（ IP绑定）\n上述方式存在一个问题就是说，在负载均衡系统中，假如用户在某台服务器上登录了，那么该用户第二次请求的时候，因为我们是负载均衡系统，每次请求都会重新定位到服务器集群中的某一个，那么已经登录某一个服务器的用户再重新定位到另一个服务器，其登录信息将会丢失，这样显然是不妥的。\n\n我们可以采用ip_hash指令解决这个问题，如果客户已经访问了某个服务器，当用户再次访问时，会将该请求通过哈希算法，自动定位到该服务器。\n\n每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。\n\nupstream backserver {\n    ip_hash;\n    server 192.168.0.14:88;\n    server 192.168.0.15:80;\n}\n\n\n4、fair（第三方插件）\n按后端服务器的响应时间来分配请求，响应时间短的优先分配。\n\nupstream backserver {\n    server server1;\n    server server2;\n    fair;\n}\n\n\n5、url_hash（第三方插件）\n按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。\n\nupstream backserver {\n    server squid1:3128;\n    server squid2:3128;\n    hash $request_uri;\n    hash_method crc32;\n}\n\n\n\t\n```\n\n \n\n","tags":["Linux"]},{"title":"运维","url":"/2019/12/23/pout/运维/运维/","content":"\n\n<!-- toc -->\n\n## 服务环境背景话术\n\n使用 Siege 和 ab 作为压测工具\n\n强调我们的日均pv在千万级，也就是每天的pv在1000万左右，日活(uv)在80万左右，QPS(每秒请求数)1000左右 峰值1200，时间段在晚上，因为晚上看视频的人多，并发数最高\n\n而经过测试，我们单台服务器的极限值是300左右（uwsgi+django）\n\n单台mysql连接数维持在1200个左右\n\n所以nginx负载均衡部署了8台机器，其中两台作为备用机\n\nmysql主从分离集群部署了4台从机，其中一台作为备用机\n\n前端做了cdn缓存和做了gizp压缩 webpack优化，精简http请求数，视频缩略图延迟加载\n\n基本在打出富裕(超出极限负载30%左右)的前提下，在高并发高负载环境下可以正常运营(但是不排除特殊情况下出现的宕机，比如ddos攻击)\n\n## Linux查看系统负载常用命令\n\n```\ntop   查看实时负载，3秒一刷新\n\ncat /proc/cpuinfo ：即可查看CPU信息，几个processor即为几个CPU；\n\nsar：可以监控系统所有资源状态，sar -n DEV查网卡流量历史、sar -q 查看历史负载，最有用的就是查网卡流量，流量过大：rxpck/s大于4000,或者rxKB/s大于5000，则很有可能被攻击了，需要抓包分析；\n\nfree：查看当前系统的总内存大小以及使用内存的情况；\n\nps：查看进程，ps aux 或者 ps -elf，常和管道符一起使用，查看某个进程或者它的数量；\n\nnetstat：查看端口，netstat -lnp用于打印当前系统启动了哪些端口，netstat -an用于打印网络连接状况；\n\ntcpdump：抓包工具分析数据包，知道有哪些IP在攻击；可以将内容写入指定文件1.cap中，显示包的内容，不加-w屏幕上显示数据流向；\n\nwireshark：抓包工具，可以临时用该命令查看当前服务器上的web请\n```\n\n## 自动化运维系统\n\n### 背景\n\n在软件开发生命周期中，遇到了两次瓶颈。第一次瓶颈是在需求阶段和开发阶段之间，针对不断变化的需求，对软件开发者提出了高要求，后来出现了敏捷方法论，强调适应需求、快速迭代、持续交付。第二个瓶颈是在开发阶段和构建部署阶段之间，大量完成的开发任务可能阻塞在部署阶段，影响交付，于是有了自动化运维系统(DevOps)。\n\nDevOps的三大原则：\n\n1、基础设施即代码（Infrastructure as Code） DeveOps的基础是将重复的事情使用自动化脚本或软件来实现，例如Docker（容器化）、Jenkins（持续集成）、Puppet（基础架构构建）、Vagrant（虚拟化平台）等\n\n2、持续交付（Continuous Delivery） 持续交付是在生产环境发布可靠的软件并交付给用户使用。而持续部署则不一定交付给用户使用。涉及到2个时间，TTR（Time to Repair）修复时间，TTM（Time To Marketing）产品上线时间。要做到高效交付可靠的软件，需要尽可能的减少这2个时间。部署可以有多种方式，比如蓝绿部署、金丝雀部署等。\n\n3、协同工作（Culture of Collaboration） 开发者和运维人员必须定期进行密切的合作。开发应该把运维角色理解成软件的另一个用户群体。协作有几个的建议：1、自动化（减少不必要的协作）；2、小范围（每次修改的内容不宜过多，减少发布的风险）；3、统一信息集散地（如wiki，让双方能够共享信息）；4、标准化协作工具（比如jenkins）\n\n在很多初创公司和中小型企业里，运维还停留在“刀耕火种”的原始状态，这里所说的“刀”和“火”就是运维人员的远程客户端，例如SecureCRT和Windows远程桌面。\n\n在这种工作方式下，服务器的安装、初始化，软件部署、服务发布和监控都是通过手动方式来完成的，需要运维人员登录到服务器上，一台一台去管理和维护。这种非并发的线性工作方式是制约效率的最大障碍。\n\n同时，因为手动的操作方式过于依赖运维人员的执行顺序和操作步骤，稍有不慎即可能导致服务器配置不一致，也就是同一组服务器的配置上出现差异。有时候，这种差异是很难直接检查出来的，例如在一个负载均衡组里面个别服务器的异常就很难发现。\n\n随着业务的发展，服务器数量越来越多，运维人员开始转向使用脚本和批量管理工具。脚本和批量管理工具与“刀耕火种”的工作方式相比，确实提升了效率和工程质量。\n\n### 项目地址\n\n版本库地址： <https://gitee.com/QiHanXiBei/spug/tree/master>\n\n演示地址： <https://spug.qbangmang.com> 用户名:admin 密码:spug\n\n具体功能话术：\n\nUser 用户管理\n\nRbac 权限管理\n\nCMDB 资产管理\n\n也就是服务器管理 通过后台对服务器集群进行管理\n\n其中可以对服务器进行配置，远程命令行连接，连接服务器挂载的docker服务器，对docker映射端口号进行修改和配置\n\nTask 任务计划管理\n\n可以对计划任务进行配置和管理，例如发送邮件，发送短信，定时重启服务等等\n\nCI/CD 部署、发布管理\n\n集成jenkins，将冗长的代码拉取，克隆完全编程在网页端一键式的部署流程\n\nConfig File 配置文件管理\n\n对项目的config文件统一管理，对重要文件做到一键式迁移\n\nMonitor 监控\n\n对服务器监控，监控服务器的cpu和内存占用\n\n集成supervisor对后台服务做监控，是否宕机，服务进程监控\n\n对站点监控，对集群网站的带宽以及http请求进行监控\n\nAlarm 报警\n\n如果出现数据异常或者内存占用过高，cpu占用过高的情况，磁盘空间不够等紧急情况\n\n进行发邮件或者短信预警\n\n## 工单系统\n\n1、解决的问题\n\n　　　　　　1. 工作流程不统一，大量复杂工作流程，让人眼花缭乱（工单的边界比较模糊）。\n\n　　　　　　2. 邮寄审批严重耗时、效率低下（特别是很多大部门leader被淹没在邮件中）。\n\n　　　　　　3. 多部门协同工作，进度无法把控（到处拉群讨论）。\n\n　　　　　　4. 大量人员每天做重复性工作，严重浪费人力成本。\n\n2、工单系统模块\n\n　　　　1） 用户管理\n\n　　　　　　　　用户表、部门表、角色表 （设置代理人）\n\n　　　　2）工单模板配置\n\n　　　　　　　　工单模板配置 ==》 审批流程配置 ==》 自动化工单配置\n\n　　　　3）工单实例化\n\n　　　　　　　　申请人工单 (审批中、被退回、完成)\n\n　　　　　　　　审批人子工单 （待处理、通过、退回、否决、确认）\n\n　　　　　　　　自动化工单 （待执行、完成、执行异常）\n\n　　　　4）工单通知\n\n　　　　　　　　邮件、企业微信、超时报警\n\n　　　　5）移动审批\n\n　　　　　　　　企业微信审批\n\n　　　　6）工单报表\n\n　　　　　　　　工单类型、工单处理量、满意度统计\n\n## 什么是Docker\n\nDocker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。\n\n## Docker与虚拟机比较\n\n作为一种轻量级的虚拟化方式，Docker在运行应用上跟传统的虚拟机方式相比具有显著优势：\n\nDocker容器很快，启动和停止可以在秒级实现，这相比传统的虚拟机方式要快得多。 Docker容器对系统资源需求很少，一台主机上可以同时运行数千个Docker容器。 Docker通过类似Git的操作来方便用户获取、分发和更新应用镜像，指令简明，学习成本较低。 Docker通过Dockerfile配置文件来支持灵活的自动化创建和部署机制，提高工作效率。\n\n![img](img/docker1.jpg)\n\n![img](img/docker2.png)\n\n## Docker构架\n\nDocker使用C/S架构，Client 通过接口与Server进程通信实现容器的构建，运行和发布。client和server可以运行在同一台集群，也可以通过跨主机实现远程通信。\n\n## Docker优势\n\n更高效的利用系统资源 由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker 对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。\n\n更快速的启动时间 传统的虚拟机技术启动应用服务往往需要数分钟，而 Docker 容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。\n\n一致的运行环境 开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。\n\n持续交付和部署 对开发和运维（DevOps）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。\n\n使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并结合 持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合 持续部署(Continuous Delivery/Deployment) 系统进行自动部署。\n\n而且使用 Dockerfile 使镜像构建透明化，不仅仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。\n\n更轻松的迁移 由于 Docker 确保了执行环境的一致性，使得应用的迁移更加容易。Docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。因此用户可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。\n\n更轻松的维护和扩展 Docker 使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，Docker 团队同各个开源项目团队一起维护了一大批高质量的 官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。\n\n## Docker核心概念\n\n镜像(image) Docker 镜像（Image）就是一个只读的模板。例如：一个镜像可以包含一个完整的操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。Docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。\n\n仓库(repository) 仓库（Repository）是集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。\n\n仓库分为公开仓库（Public）和私有仓库（Private）两种形式。最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。国内的公开仓库包括 时速云 、网易云 等，可以提供大陆用户更稳定快速的访问。当然，用户也可以在本地网络内创建一个私有仓库。\n\n当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 pull 下来就可以了。\n\nDocker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。\n\n容器(container) Docker 利用容器（Container）来运行应用。容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。\n\n容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。\n\n## Docker隔离性\n\nDocker容器本质上是宿主机上的进程。Docker 通过namespace实现了资源隔离，通过cgroups实现资源限制，通过写时复制机制(Copy-on-write)实现了高效的文件操作。\n\nLinux内核实现namespace的主要目的之一就是实现轻量级虚拟化(容器)服务。在同一个namespace下的进程可以感知彼此的变化，而对外界的进程一无所知。这样就可以让容器中的进程产生错觉，仿佛自己置身于一个独立的系统环境中，以达到独立和隔离的目的。\n\n一般,Docker容器需要并且Linux内核也提供了这6种资源的namespace的隔离： UTS : 主机与域名 IPC : 信号量、消息队列和共享内存 PID : 进程编号 NETWORK : 网络设备、网络栈、端口等 Mount : 挂载点(文件系统) User : 用户和用户组 Pid namespace\n\n用户进程是lxc-start进程的子进程，不同用户的进程就是通过pid namespace隔离开的，且不同namespace中可以有相同PID,具有以下特征： 1、每个namespace中的pid是有自己的pid=1的进程(类似/sbin/init进程） 2、每个namespace中的进程只能影响自己的同一个namespace或子namespace中的进程 3、因为/proc包含正在运行的进程，因此而container中的pseudo-filesystem的/proc目录只能看到自己namespace中的进程 4、因为namespace允许嵌套，父进程可以影响子namespace进程，所以子namespace的进程可以在父namespace中看到，但是具有不同的pid Net namespace\n\n有了pid namespace ,每个namespace中的pid能够相互隔离，但是网络端口还是共享host的端口。网络隔离是通过netnamespace实现的， 每个net namespace有独立的network devices, IP address, IP routing tables, /proc/net目录，这样每个container的网络就能隔离开来， LXC在此基础上有5种网络类型，docker默认采用veth的方式将container中的虚拟网卡同host上的一个docker bridge连接在一起\n\nIPC namespace Container中进程交互还是采用linux常见的进程间交互方法(interprocess communication-IPC)包括常见的信号量、消息队列、内存共享。然而同VM不同， container的进程交互实际是host具有相同pid namespace中的进程间交互，因此需要在IPC资源申请时加入namespace信息，每个IPC资源有一个唯一的32bit ID Mnt namespace 类似chroot ,将一个进程放到一个特定的目录执行，mnt namespace允许不同namespace的进程看到的文件结构不同，这样每个namespace中的进程所看到的文件目录被隔离开了，同chroot不同，每个namespace中的container在/proc/mounts的信息只包含所在namespace 的mount point\n\nUts namespace UTS(UNIX Time sharing System）namespace允许每个container拥有独立地hostname和domain name,使其在网络上被视作一个独立的节点而非Host上的一个进程\n\nUser namespace 每个container可以有不同的user和group id ,也就是说可以container内部的用户在container内部执行程序而非 Host上的用户\n\n## Nginx负载均衡\n\n正向代理，是在用户端的。比如需要访问某些国外网站，我们可能需要购买vpn。 并且vpn是在我们的用户浏览器端设置的(并不是在远端的服务器设置)。\n\n反向代理是作用在服务器端的，是一个虚拟ip(VIP)。对于用户的一个请求，会转发到多个后端处理器中的一台来处理该具体请求。\n\nNginx一般作为反向代理服务器来实现反向代理来转发处理请求，同时也可以作为静态资源服务器来加快静态资源的获取和处理。\n\n反向代理是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。客户端是无感知代理的存在的，反向代理对外都是透明的，访问者者并不知道自己访问的是一个代理。因为客户端不需要任何配置就可以访问。\n\n作用：保证内网的安全，可以使用反向代理提供WAF功能，阻止web攻击；负载均衡，通过反向代理服务器来优化网站的负载\n\n具体搭建：<https://v3u.cn/a_id_77>\n\n![img](img/fandai.png)\n\n## Nginx架构\n\n![img](img/nginx.png)\n\nNginx采用多进程方式，一个Master与多个Worker进程。客户端访问请求通过负载均衡配置来转发到不同的后端服务器依次来实现负载均衡。\n\n## Nginx负载均衡策略\n\nNginx负载均衡策略主要有 轮询，加权轮询，最少连接数以及IP Hash。\n\n![img](img/fuzai.jpg)\n\n轮询策略： 实现请求的按顺序转发，即从服务srv1--srv2--srv3依次来处理请求\n\n![img](img/n1.png)\n\n加权轮询策略： 请求将按照服务器的设置权重来实现请求转发和处理，如下所示，最终请求处理数将为3：1：1，一般情况下加权的依据是根据服务器配置来决定的，即配置好的机器分配的权重高 ![img](img/n2.png)\n\n最少连接数策略： 请求将转发到连接数较少的服务器上 ![img](img/n3.png)\n\nIp Hash策略： web服务需要共享session,使用该策略可以实现某一客户端的请求固定转发至某一服务器 ![img](img/n4.png)\n\nip_hash算法的原理很简单，根据请求所属的客户端IP计算得到一个数值，然后把请求发往该数值对应的后端。\n\n所以同一个客户端的请求，都会发往同一台后端，除非该后端不可用了。ip_hash能够达到保持会话的效果。\n\nip_hash是基于round robin的，判断后端是否可用的方法是一样的。\n\n```\nfor (i = 0; i < 3; i++) {  \n    hash = (hash * 113 + iphp->addr[i]) % 6271; //iphp->addr[i]为ip的点分十进制法的第i段\n}\n```\n\nhash3就是计算所得的数值，它只和初始数值hash0以及客户端的IP有关。\n\nfor循环 i取 012三个值，而ip的点分十进制表示方法将ip分成四段（如：192.168.1.1），但是这里循环时只是将ip的前三个端作为参数加入hash函数。这样做的目的是保证ip地址前三位相同的用户经过hash计算将分配到相同的后端server。\n\n作者的这个考虑是极为可取的，因此ip地址前三位相同通常意味着来着同一个局域网或者相邻区域，使用相同的后端服务让nginx在一定程度上更具有一致性。\n\n第二步，根据计算所得数值，找到对应的后端。\n\n```\nw = hash3 % total_weight;\n\nwhile (w >= peer->weight) {\n\n    w -= peer->weight;\n\n    peer = peer->next;\n\n    p++;\n\n}\n```\n\ntotal_weight为所有后端权重之和。遍历后端链表时，依次减去每个后端的权重，直到w小于某个后端的权重。\n\n选定的后端在链表中的序号为p。因为total_weight和每个后端的weight都是固定的，所以如果hash3值相同，\n\n则找到的后端相同。\n\n## Supervisor\n\nSupervisor\n\n是用Python开发的一个client/server服务，是Linux/Unix系统下的一个进程管理工具，不支持Windows系统。它可以很方便的监听、启动、停止、重启一个或多个进程。用Supervisor管理的进程，当一个进程意外被杀死，supervisort监听到进程死后，会自动将它重新拉起，很方便的做到进程自动恢复的功能，不再需要自己写shell脚本来控制。\n\n说白了，它真正有用的功能是俩个将非daemon(守护进程)程序变成deamon方式运行对程序进行监控，当程序退出时，可以自动拉起程序。\n\n但是它无法控制本身就是daemon的服务。\n\n具体安装和配置流程参照\n\n## Fastdfs\n\nFastDFS是一个开源的分布式文件系统，它对文件进行管理，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等。可以说它就是为互联网而生，为大数据而生的。\n\nFastDFS服务端有两个角色：跟踪器（tracker）和存储节点（storage）。跟踪器主要做调度工作，在访问上起负载均衡的作用。 存储节点存储文件，完成文件管理的所有功能：存储、同步和提供存取接口，FastDFS同时对文件的meta data进行管理。跟踪器和存储节点都可以由多台服务器构成。跟踪器和存储节点中的服务器均可以随时增加或下线而不会影响线上服务。其中跟踪器中的所有服务器都是对等的，可以根据服务器的压力情况随时增加或减少。\n\n![img](img/fastdfs.png)\n\n1 解决海量存储，同时存储容量扩展方便。\n\n2 解决文件内容重复,如果用户上传的文件重复(文件指纹一样)，那么系统只有存储一份数据，值得一提的是，这项技术目前被广泛应用在网盘中。\n\n3 结合Nginx提高网站读取图片的效率。\n\n具体搭建：<https://v3u.cn/a_id_78> 通过python交互：<https://v3u.cn/Index_a_id_79>\n\n其排重原理为：\n\nFastDFS的storage server每次上传均计算文件的hash值，然后从FastDHT服务器上进行查找比对，如果没有返回，则写入hash，并将文件保存；如果有返回，则建立一个新的文件链接（软链），不保存文件。\n\n## 自动化部署 Jenkins\n\nJenkins是目前非常流行的一款持续集成工具，可以帮助大家把更新后的代码自动部署到服务器上运行，整个流程非常自动化，你可以理解为部署命令操作的可视化界面。\n\n部署流程:\n","tags":["运维"]},{"title":"前端技术及相关","url":"/2019/12/23/pout/前端技术/前端技术及相关/","content":"\n\n<!-- toc -->\n\n## 前后端分离\n\n前后端只通过 JSON 来交流，组件化、工程化不需要依赖后端去实现。 可以通过Vue.js来实现组件化工程化；有哪些好处或弊端？现在的发展趋势是否往这个方面发展\n\n## 前端优化技术\n\n具体操作：<https://v3u.cn/Index_a_id_86>\n\n1. cdn加速\n\n网站上静态资源即css、js全都使用cdn分发，图片亦然。具体来说，CDN就是采用更多的缓存服务器（CDN边缘节点），布放在用户访问相对集中的地区或网络中。当用户访问网站时，利用全局负载技术，将用户的访问指向距离最近的缓存服务器上，由缓存服务器响应用户请求\n\n![img](img/cdn.gif)\n\n2.使用Gzip压缩网页\n\n3 减少 HTTP请求数，如果可以的话，尽可能的将外部的脚本、样式进行合并，多个合为一个。另外， CSS、 Javascript、Image 都可以用相应的工具进行压缩，压缩后往往能省下不少空间，如何压缩以及合并外部脚本和样式请参照这篇文章 利用grunt插件来压缩js和css文件用来减少http请求，提高页面效率\n\n4 避免空的src和href 当link标签的href属性为空、script标签的src属性为空的时候，浏览器渲染的时候会把当前页面的URL作为它们的属性值，从而把页面的内容加载进来作为它们的值。所以要避免犯这样的疏忽。\n\n5 把CSS放到顶部 网页上的资源加载时从上网下顺序加载的，所以css放在页面的顶部能够优先渲染页面，让用户感觉页面加载很快。\n\n6 把JS放到底部 加载js时会对后续的资源造成阻塞，必须得等js加载完才去加载后续的文件 ，所以就把js放在页面底部最后加载。\n\n7 可缓存的AJAX 异步请求同样的造成用户等待，所以使用ajax请求时，要主动告诉浏览器如果该请求有缓存就去请求缓存内容。如下代码片段, cache:true就是显式的要求如果当前请求有缓存的话，直接使用缓存\n\n8 减少作用域链查找，这一点在循环中是尤其需要注意的问题。如果在循环中需要访问非本作用域下的变量时请在遍历之前用局部变量缓存该变量，并在遍历结束后再重写那个变量，这一点对全局变量尤其重要，因为全局变量处于作用域链的最顶端，访问时的查找次数是最多的。\n\n9 生成纯静态页，也就是把动态内容事先生成好，这样在前端就避免请求后端数据，加快了页面访问速度\n\n### 使用场景 利弊切记一定要举例子,如下：\n\n前后端要不要分，怎么分，是由具体业务决定的。\n\n需要搜索引擎带流量的，必须由服务器端渲染。\n\n需要用户登录且不能由搜索引擎抓取，前后端分离是鼓励的。\n\n需要App和后端交互，必须分离。\n\n但是分了就表示架构合理？不一定。设计一套合理／可升级／客户端友好的API也不容易。\n\n要想做好前后端分离，前端开发要了解后端架构，后端开发要虚心学习前端技术，双方如果互相鄙视，分了也白搭\n\n## 为什么选择Vue.js\n\nMVVM 是Model-View-ViewModel 的缩写，它是一种基于前端开发的架构模式，其核心是提供对View 和 ViewModel 的双向数据绑定，这使得ViewModel 的状态改变可以自动传递给 View，即所谓的数据双向绑定。 Vue.js 是一个提供了 MVVM 风格的双向数据绑定的 Javascript 库，专注于View 层。它的核心是 MVVM 中的 VM，也就是 ViewModel。 ViewModel负责连接 View 和 Model，保证视图和数据的一致性，这种轻量级的架构让前端开发更加高效、便捷。\n\n### 强调前端为什么要用vue.js 为什么要用工程化\n\n相对 HTML4 , HTML5 最大的亮点是它为移动设备提供了一些非常有用的功能，使得 HTML5 具备了开发App的能力, HTML5开发App 最大的好处就是跨平台、快速迭代和上线，节省人力成本和提高效率，因此很多企业开始对传统的App进行改造，逐渐用H5代替Native，到2015年的时候，市面上大多数App 或多或少嵌入都了H5 的页面。\n\n### Vue.js 和 jquery的区别 强调没有最好的，只有最适合的\n\njQuery是使用选择器（$）选取DOM对象，对其进行赋值、取值、事件绑定等操作，其实和原生的HTML的区别只在于可以更方便的选取和操作DOM对象，而数据和界面是在一起的。比如需要获取label标签的内容：$(\"lable\").val();,它还是依赖DOM元素的值。 Vue则是通过Vue对象将数据和View完全分离开来了。对数据进行操作不再需要引用相应的DOM对象，可以说数据和View是分离的，他们通过Vue对象这个vm实现相互的绑定。这就是传说中的MVVM。\n\n## 混合式开发\n\n近几年，混合模式移动应用的概念甚嚣尘上，受到了一些中小型企业的青睐，究其原因，混合模式开发可以比传统移动开发节约大量的开发成本和人力成本。\n\nHybrid App（混合模式移动应用）是指介于web-app、native-app这两者之间的app，兼具“Native App良好用户交互体验的优势”和“Web App跨平台开发的优势”。\n\n说白了，如果走传统移动开发路线，公司业务覆盖多端，那么每个平台势必要请一个专属开发人员，安卓要请一个前端开发，ios同理，那么人力成本则进行了翻倍，同时，如果多端使用不同的代码，当有功能上的修改或者维护时，成本也是不可想象的。试想如果开发者编写一套代码，可编译到iOS、Android、H5、小程序等多个平台，这绝对是省时省力的良好方案。\n\n具体操作：<https://v3u.cn/a_id_82>\n\n## mvvm 和 mvc 区别？\n\nmvc 和 mvvm 其实区别并不大。都是一种设计思想。主要就是 mvc 中 Controller 演变成 mvvm 中的 viewModel。mvvm 主要解决了 mvc 中大量的 DOM 操作使页面渲染性能降低，加载速度变慢，影响用户体验。和当 Model 频繁发生变化，开发者需要主动更新到 View 。\n\n## vue 的优点是什么？\n\n低耦合。视图（View）可以独立于 Model 变化和修改，一个 ViewModel 可以绑定到不同的\"View\"上，当 View 变化的时候 Model 可以不变，当 Model 变化的时候 View 也可以不变。 可重用性。你可以把一些视图逻辑放在一个 ViewModel 里面，让很多 view 重用这段视图逻辑。 独立开发。开发人员可以专注于业务逻辑和数据的开发（ViewModel），设计人员可以专注于页面设计，使用 Expression Blend 可以很容易设计界面并生成 xml 代码。 可测试。界面素来是比较难于测试的，而现在测试可以针对 ViewModel 来写。\n\n## vue生命周期的理解？\n\n总共分为 8 个阶段创建前/后，载入前/后，更新前/后，销毁前/后。\n\n创建前/后： 在 beforeCreate 阶段，vue 实例的挂载元素 el 还没有。 载入前/后：在 beforeMount 阶段，vue 实例的$el 和 data 都初始化了，但还是挂载之前为虚拟的 dom 节点，data.message 还未替换。在 mounted 阶段，vue 实例挂载完成，data.message 成功渲染。 更新前/后：当 data 变化时，会触发 beforeUpdate 和 updated 方法。 销毁前/后：在执行 destroy 方法后，对 data 的改变不会再触发周期函数，说明此时 vue 实例已经解除了事件监听以及和 dom 的绑定，但是 dom 结构依然存在\n\n## 组件之间的传值？\n\n```\n//父组件通过标签上面定义传值\n<template>\n    <Main :obj=\"data\"></Main>\n</template>\n<script>\n    //引入子组件\n    import Main form \"./main\"\n\n    exprot default{\n        name:\"parent\",\n        data(){\n            return {\n                data:\"我要向子组件传递数据\"\n            }\n        },\n        //初始化组件\n        components:{\n            Main\n        }\n    }\n</script>\n\n\n//子组件通过props方法接受数据\n<template>\n    <div>{{data}}</div>\n</template>\n<script>\n    exprot default{\n        name:\"son\",\n        //接受父组件传值\n        props:[\"obj\"]\n    }\n</script>\n```\n\n## 路由之间跳转？\n\n声明式（标签跳转）router-link 编程式（ js 跳转） router.push('index')\n\n## 懒加载（按需加载路由）（常考）\n\nwebpack 中提供了 require.ensure()来实现按需加载。以前引入路由是通过 import 这样的方式引入，改为 const 定义的方式进行引入。\n\n不进行页面按需加载引入方式： import home from '../../common/home.vue'\n\n进行页面按需加载的引入方式： const home = r => require.ensure( [], () => r (require('../../common/home.vue')))\n\n## vuex 是什么？怎么使用？哪种功能场景使用它？\n\nvue 框架中状态管理。在 main.js 引入 store，注入。新建了一个目录 store，….. export 。场景有：单页应用中，组件之间的状态。音乐播放、登录状态、加入购物车\n\n```\n// 新建 store.js\nimport vue from 'vue'\nimport vuex form 'vuex'\nvue.use(vuex)\nexport default new vuex.store({\n    //...code\n})\n\n//main.js\nimport store from './store'\n...\n```\n\n## vue 的双向绑定的原理是什么(常考)\n\nvue.js 是采用数据劫持结合发布者-订阅者模式的方式，通过 Object.defineProperty()来劫持各个属性的 setter，getter，在数据变动时发布消息给订阅者，触发相应的监听回调。\n\n具体步骤： 第一步：需要 observe 的数据对象进行递归遍历，包括子属性对象的属性，都加上 setter 和 getter 这样的话，给这个对象的某个值赋值，就会触发 setter，那么就能监听到了数据变化\n\n第二步：compile 解析模板指令，将模板中的变量替换成数据，然后初始化渲染页面视图，并将每个指令对应的节点绑定更新函数，添加监听数据的订阅者，一旦数据有变动，收到通知，更新视图\n\n第三步：Watcher 订阅者是 Observer 和 Compile 之间通信的桥梁，主要做的事情是:\n\n在自身实例化时往属性订阅器(dep)里面添加自己 自身必须有一个 update()方法 待属性变动 dep.notice()通知时，能调用自身的 update() 方法，并触发 Compile 中绑定的回调，则功成身退。 第四步：MVVM 作为数据绑定的入口，整合 Observer、Compile 和 Watcher 三者，通过 Observer 来监听自己的 model 数据变化，通过 Compile 来解析编译模板指令，最终利用 Watcher 搭起 Observer 和 Compile 之间的通信桥梁，达到数据变化 -> 视图更新；视图交互变化(input) -> 数据 model 变更的双向绑定效果。\n","tags":["前端技术"]},{"title":"python2和python3的区别","url":"/2019/12/23/pout/python中高级面试题/python2与3的区别/","content":"\n\n<!-- toc -->\n\n\n# python2和python3的区别\n\nprint语句被python3废弃，只能使用print函数\n\nPython3中字符串是Unicode (utf-8)编码，支持中文做标识符。\n\npython2中是ASCII编码，需要更改字符集才能正常支持中文，所以在.py文件中会看到#-- coding: UTF-8 --\n\n异常处理 Python2中try:...except Exception, e:...，在Python3中改为了try:...except Exception as e:...\n\nPython3中不再使用xrange方法，只有range方法。\n\nrange在Python2中返回列表，而在Python3中返回range可迭代对象。\n\n在Python2中有两个不等运算符!=和<>，在Python3中去掉了<>，只有!=符号表示不等\n\n在Python2中双反引号`可以替代repr函数，在Python3中去掉了双反引号的表是方法，只能用repr`方法。\n\nStringIO模块现在被合并到新的io模组内。new, md5, gopherlib等模块被删除。\n\nhttplib, BaseHTTPServer, CGIHTTPServer, SimpleHTTPServer, Cookie, cookielib被合并到http包内。\n\n取消了exec语句，只剩下exec()函数。\n\n在Python2中long是比int取值范围更大的整数，Python3中取消了long类型，int的取值范围扩大到之前的long类型范围。\n\n列表推导 不再支持[n for n in a,b]语法，改为[n for n in (a,b)]或[n for n in [a,b]]\n\npython 2 中通过input输入的类型是int，只有通过raw_input()输入的类型才是str。\n\npython 3中通过input输入的类型都是str，去掉了row_input()方法。\n\n## python3.6中dict有序\n\n有序是指遍历时的输出顺序与输入顺序相同\n\n## 关于哈希表\n\n1. 散列表概念 散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。\n2. 哈希函数 给定表M，存在函数f(key)，对任意给定的关键字值key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表M为哈希(Hash）表，函数f(key)为哈希(Hash) 函数。 （相关：是不是可以这样理解，数组可以通过下标进行访问，时间复杂度是O(1)，对于不连续存储的数据结构，如果知道下标也可以直接进行访问，所以可以通过哈希函数将key映射成数组下标，进行访问）\n3. 冲突 不同的key经过hash函数运行后得到相同的值，产生冲突；\n4. 冲突解决方式\n5. 开放寻址：线性探测、二次探测、伪随机数序列（python的dict解决冲突用的这个，具体的策略没有看太明白）\n6. 再哈希法：将哈希值再哈希，然后存储；\n7. 链地址法：hash过后值相同的存储在链表里；\n8. 公共溢出区\n\npython实现dict无序到有序：\n\n1. 原先的内存布局entries为哈希表，表中直接存储PyDictKeyEntry（hash、key、value），也就是说当当前位置为空的时候存的是（0， null， null）浪费了大量内存；\n\n1. python3.6: indices充当哈希表，存储的entries的index，使用index去访问存有PyDictKeyEntry的数组\n\n## python3 dict性能优化\n\n节省存储空间：将存储PyDictKeyEntry的稀疏数组更改为存储int的稀疏数组； 之前的dict_entry是稀疏表，经压缩后在密集表上循环，使用更少的内存； 调整大小更快，并且触及更少的内存。 目前，每一个散列/键/值条目在一个过程中被移动或复制调整。 在新的布局中，只有索引是更新。 大多数情况下，散列/键/值条目从不移动（除了偶尔交换填充删除留下的空洞）。\n\n## range和xrange区别\n\nrange返回的是一个包含所有元素的列表，xrange返回的是一个生成器，生成器是一个可迭代对象，在对生成器进行迭代时，元素是逐个被创建的。而列表需要根据列表长度而开辟出相应的内存空间用来遍历，一般来看，在对大序列进行迭代的时候，因为xrange的特性，所以它会比较节约内存。\n","tags":["python中高级面试题"]},{"title":"MongoDB简介","url":"/2019/12/23/pout/数据库/Mongodb/mongodb/","content":"\n\n<!-- toc -->\n\n## 什么是 MongoDB ？\n\nMongoDB 是一个介于关系数据库和非关系数据库之间的开源产品，是最接近于关系型数据库的 NoSQL 数据库。它在轻量级JSON 交换基础之上进行了扩展，即称为 BSON 的方式来描述其无结构化的数据类型。尽管如此它同样可以存储较为复杂的数据类型。它和上一篇文章讲到的Redis有异曲同工之妙。虽然两者均为 NoSQL ，但是 MongoDB 相对于 Redis 而言，MongoDB 更像是传统的数据库。早些年我们是先有了 Relation Database (关系型数据库)，然后出现了很多很复杂的query ，里面用到了很多嵌套，很多 join 操作。所以在设计数据库的时候，我们也考虑到了如何应用他们的关系，使得写 query 可以使 database 效率达到最高。后来人们发现，不是每个系统，都需要如此复杂的关系型数据库。有些简单的网站，比如博客，比如社交网站，完全可以斩断数据库之间的一切关系。这样做带来的好处是，设计数据库变得更加简单，写 query 也变得更加简单。然后，query 消耗的时间可能也会变少。因为 query 简单了，少了许多消耗资源的 join 操作，速度自然会上去。正如所说的， query 简单了，很有以前 MySQL 可以找到的东西，现在关系没了，通过 Mongo 找不到了。我们只能将几组数据都抓到本地，然后在本地做 join ，所以在这点上可能会消耗很多资源。这里我们可以发现。如何选择数据库，完全取决于你所需要处理的数据的模型，即 Data Model 。如果它们之间，关系错综复杂，千丝万缕，这个时候 MySQL 一定是首选。如果他们的关系并不是那么密切，那么， NoSQL 将会是利器。\n\nMongoDB 和 Redis 一样均为 key-value 存储系统，它具有以下特点：\n\n面向集合存储，易存储对象类型的数据。 模式自由。 支持动态查询。 支持完全索引，包含内部对象。 支持查询。 支持复制和故障恢复。 使用高效的二进制数据存储，包括大型对象(如视频等)。 自动处理碎片，以支持云计算层次的扩展性 支持 Python ， PHP ， Ruby ， Java ， C ， C# ， Javascript ，Perl 及 C++ 语言的驱动程序，社区中也提供了对 Erlang 及 .NET 等平台的驱动程序。 文件存储格式为 BSON (一种 JSON 的扩展)。 可通过网络访问。\n\n## MongoDB 与 MySQL 性能比较\n\n像 MySQL 一样， MongoDB 提供了丰富的远远超出了简单的键值存储中提供的功能和功能。 MongoDB 具有查询语言，功能强大的辅助索引(包括文本搜索和地理空间)，数据分析功能强大的聚合框架等。相比使用关系数据库而言，使用MongoDB ，您还可以使用如下表所示的这些功能，跨越更多样化的数据类型和数据规模。\n\nMySQLMongoDB丰富的数据模型否是动态 Schema否是数据类型是是数据本地化否是字段更新是是易于编程否是复杂事务是否审计是是自动分片否是\n\nMySQL 中的许多概念在 MongoDB 中具有相近的类比。本表概述了每个系统中的一些常见概念。\n\nMySQLMongoDB表集合行文档列字段joins嵌入文档或者链接\n\n## MongoDB应用范围和限制\n\nMongoDB 的主要目标是在 key-value (键/值)存储方式(提供了高性能和高度伸缩性)以及传统的 RDBMS 系统(丰富的功能)架起一座桥梁，集两者的优势于一身。 MongoDB 适用范围如下：\n\n网站数据： Mongo 非常适合实时的插入，更新与查询，并具备网站实时数据存储所需的复制及高度伸缩性。 缓存：由于性能很高， Mongo 也适合作为信息基础设施的缓存层。在系统重启之后，由 Mongo 搭建的持久化缓存层可以避免下层的数据源过载。 大尺寸，低价值的数据：使用传统的关系型数据库存储一些数据时可能会比较昂贵，在此之前，很多时候程序员往往会选择传统的文件进行存储。 高伸缩性的场景： Mongo 非常适合由数十或数百台服务器组成的数据库。 Mongo 的路线图中已经包含对 MapReduce 引擎的内置支持。 用于对象及 JSON 数据的存储： Mongo 的 BSON 数据格式非常适合文档化格式的存储及查询。 MongoDB 当然也会有以下场景的限制：\n\n高度事物性的系统：例如银行或会计系统。传统的关系型数据库目前还是更适用于需要大量原子性复杂事务的应用程序。 传统的商业智能应用：针对特定问题的 BI 数据库会对产生高度优化的查询方式。对于此类应用，数据仓库可能是更合适的选择。 需要 SQL 的问题。\n","tags":["数据库"]},{"title":"Linux常见命令","url":"/2019/12/23/pout/Linux/Linux常见命令/","content":"\n\n<!-- toc -->\n\n\n**系统信息** \narch 显示机器的处理器架构\nuname -m 显示机器的处理器架构\nuname -r 显示正在使用的内核版本 \ndmidecode -q 显示硬件系统部件 - (SMBIOS / DMI) \nhdparm -i /dev/hda 罗列一个磁盘的架构特性 \nhdparm -tT /dev/sda 在磁盘上执行测试性读取操作 \ncat /proc/cpuinfo 显示CPU info的信息 \ncat /proc/interrupts 显示中断 \ncat /proc/meminfo 校验内存使用 \ncat /proc/swaps 显示哪些swap被使用 \ncat /proc/version 显示内核的版本 \ncat /proc/net/dev 显示网络适配器及统计 \ncat /proc/mounts 显示已加载的文件系统 \nlspci -tv 罗列 PCI 设备 \nlsusb -tv 显示 USB 设备 \ndate 显示系统日期 \ncal 2007 显示2007年的日历表 \ndate 041217002007.00 设置日期和时间 - 月日时分年.秒 \nclock -w 将时间修改保存到 BIOS \n**关机 (系统的关机、重启以及登出 )** \nshutdown -h now 关闭系统\ninit 0 关闭系统\ntelinit 0 关闭系统\nshutdown -h hours:minutes & 按预定时间关闭系统 \nshutdown -c 取消按预定时间关闭系统 \nshutdown -r now 重启\nreboot 重启\nlogout 注销 \n**文件和目录** \ncd /home 进入 '/ home' 目录' \ncd .. 返回上一级目录 \ncd ../.. 返回上两级目录 \ncd 进入个人的主目录 \ncd ~user1 进入个人的主目录 \ncd - 返回上次所在的目录 \npwd 显示工作路径 \nls 查看目录中的文件 \nls -F 查看目录中的文件 \nls -l 显示文件和目录的详细资料 \nls -a 显示隐藏文件 \nls *[0-9]* 显示包含数字的文件名和目录名 \ntree 显示文件和目录由根目录开始的树形结构\nlstree 显示文件和目录由根目录开始的树形结构\nmkdir dir1 创建一个叫做 'dir1' 的目录' \nmkdir dir1 dir2 同时创建两个目录 \nmkdir -p /tmp/dir1/dir2 创建一个目录树 \nrm -f file1 删除一个叫做 'file1' 的文件' \nrmdir dir1 删除一个叫做 'dir1' 的目录' \nrm -rf dir1 删除一个叫做 'dir1' 的目录并同时删除其内容 \nrm -rf dir1 dir2 同时删除两个目录及它们的内容 \nmv dir1 new_dir 重命名/移动 一个目录 \ncp file1 file2 复制一个文件 \ncp dir/* . 复制一个目录下的所有文件到当前工作目录 \ncp -a /tmp/dir1 . 复制一个目录到当前工作目录 \ncp -a dir1 dir2 复制一个目录 \n\ncp -r dir1 dir2 复制一个目录及子目录\nln -s file1 lnk1 创建一个指向文件或目录的软链接 \nln file1 lnk1 创建一个指向文件或目录的物理链接 \ntouch -t 0712250000 file1 修改一个文件或目录的时间戳 - (YYMMDDhhmm) \nfile file1 outputs the mime type of the file as text \niconv -l 列出已知的编码 \niconv -f fromEncoding -t toEncoding inputFile > outputFile creates a new from the given input file by assuming it is encoded in fromEncoding and converting it to toEncoding. \nfind . -maxdepth 1 -name *.jpg -print -exec convert \"{}\" -resize 80x60 \"thumbs/{}\" \\; batch resize files in the current directory and send them to a thumbnails directory (requires convert from Imagemagick) \n**文件搜索** \nfind / -name file1 从 '/' 开始进入根文件系统搜索文件和目录 \nfind / -user user1 搜索属于用户 'user1' 的文件和目录 \nfind /home/user1 -name \\*.bin 在目录 '/ home/user1' 中搜索带有'.bin' 结尾的文件 \nfind /usr/bin -type f -atime +100 搜索在过去100天内未被使用过的执行文件 \nfind /usr/bin -type f -mtime -10 搜索在10天内被创建或者修改过的文件 \nfind / -name \\*.rpm -exec chmod 755 '{}' \\; 搜索以 '.rpm' 结尾的文件并定义其权限 \nfind / -xdev -name \\*.rpm 搜索以 '.rpm' 结尾的文件，忽略光驱、捷盘等可移动设备 \nlocate \\*.ps 寻找以 '.ps' 结尾的文件 - 先运行 'updatedb' 命令 \nwhereis halt 显示一个二进制文件、源码或man的位置 \nwhich halt 显示一个二进制文件或可执行文件的完整路径 \n**挂载一个文件系统** \nmount /dev/hda2 /mnt/hda2 挂载一个叫做hda2的盘 - 确定目录 '/ mnt/hda2' 已经存在 \numount /dev/hda2 卸载一个叫做hda2的盘 - 先从挂载点 '/ mnt/hda2' 退出 \nfuser -km /mnt/hda2 当设备繁忙时强制卸载 \numount -n /mnt/hda2 运行卸载操作而不写入 /etc/mtab 文件- 当文件为只读或当磁盘写满时非常有用 \nmount /dev/fd0 /mnt/floppy 挂载一个软盘 \nmount /dev/cdrom /mnt/cdrom 挂载一个cdrom或dvdrom \nmount /dev/hdc /mnt/cdrecorder 挂载一个cdrw或dvdrom \nmount /dev/hdb /mnt/cdrecorder 挂载一个cdrw或dvdrom \nmount -o loop file.iso /mnt/cdrom 挂载一个文件或ISO镜像文件 \nmount -t vfat /dev/hda5 /mnt/hda5 挂载一个Windows FAT32文件系统 \nmount /dev/sda1 /mnt/usbdisk 挂载一个usb 捷盘或闪存设备 \nmount -t smbfs -o username=user,password=pass //WinClient/share /mnt/share 挂载一个windows网络共享 \n**磁盘空间** \ndf -h 显示已经挂载的分区列表 \nls -lSr |more 以尺寸大小排列文件和目录 \ndu -sh dir1 估算目录 'dir1' 已经使用的磁盘空间' \ndu -sk * | sort -rn 以容量大小为依据依次显示文件和目录的大小 \nrpm -q -a --qf '%10{SIZE}t%{NAME}n' | sort -k1,1n 以大小为依据依次显示已安装的rpm包所使用的空间 (fedora, redhat类系统) \ndpkg-query -W -f='${Installed-Size;10}t${Package}n' | sort -k1,1n 以大小为依据显示已安装的deb包所使用的空间 (ubuntu, debian类系统) \n**用户和群组** \ngroupadd group_name 创建一个新用户组 \ngroupdel group_name 删除一个用户组 \ngroupmod -n new_group_name old_group_name 重命名一个用户组 \nuseradd -c \"Name Surname \" -g admin -d /home/user1 -s /bin/bash user1 创建一个属于 \"admin\" 用户组的用户 \nuseradd user1 创建一个新用户 \nuserdel -r user1 删除一个用户 ( '-r' 排除主目录) \nusermod -c \"User FTP\" -g system -d /ftp/user1 -s /bin/nologin user1 修改用户属性 \npasswd 修改口令 \npasswd user1 修改一个用户的口令 (只允许root执行) \nchage -E 2005-12-31 user1 设置用户口令的失效期限 \npwck 检查 '/etc/passwd' 的文件格式和语法修正以及存在的用户 \ngrpck 检查 '/etc/passwd' 的文件格式和语法修正以及存在的群组 \nnewgrp group_name 登陆进一个新的群组以改变新创建文件的预设群组 \n**文件的权限 - 使用 \"+\" 设置权限，使用 \"-\" 用于取消** \nls -lh 显示权限 \nls /tmp | pr -T5 -W$COLUMNS 将终端划分成5栏显示 \nchmod ugo+rwx directory1 设置目录的所有人(u)、群组(g)以及其他人(o)以读（r ）、写(w)和执行(x)的权限 \nchmod go-rwx directory1 删除群组(g)与其他人(o)对目录的读写执行权限 \nchown user1 file1 改变一个文件的所有人属性 \nchown -R user1 directory1 改变一个目录的所有人属性并同时改变改目录下所有文件的属性 \nchgrp group1 file1 改变文件的群组 \nchown user1:group1 file1 改变一个文件的所有人和群组属性 \nfind / -perm -u+s 罗列一个系统中所有使用了SUID控制的文件 \nchmod u+s /bin/file1 设置一个二进制文件的 SUID 位 - 运行该文件的用户也被赋予和所有者同样的权限 \nchmod u-s /bin/file1 禁用一个二进制文件的 SUID位 \nchmod g+s /home/public 设置一个目录的SGID 位 - 类似SUID ，不过这是针对目录的 \nchmod g-s /home/public 禁用一个目录的 SGID 位 \nchmod o+t /home/public 设置一个文件的 STIKY 位 - 只允许合法所有人删除文件 \nchmod o-t /home/public 禁用一个目录的 STIKY 位 \n**文件的特殊属性 - 使用 \"+\" 设置权限，使用 \"-\" 用于取消** \nchattr +a file1 只允许以追加方式读写文件 \nchattr +c file1 允许这个文件能被内核自动压缩/解压 \nchattr +d file1 在进行文件系统备份时，dump程序将忽略这个文件 \nchattr +i file1 设置成不可变的文件，不能被删除、修改、重命名或者链接 \nchattr +s file1 允许一个文件被安全地删除 \nchattr +S file1 一旦应用程序对这个文件执行了写操作，使系统立刻把修改的结果写到磁盘 \nchattr +u file1 若文件被删除，系统会允许你在以后恢复这个被删除的文件 \nlsattr 显示特殊的属性 \n**打包和压缩文件** \nbunzip2 file1.bz2 解压一个叫做 'file1.bz2'的文件 \nbzip2 file1 压缩一个叫做 'file1' 的文件 \ngunzip file1.gz 解压一个叫做 'file1.gz'的文件 \ngzip file1 压缩一个叫做 'file1'的文件 \ngzip -9 file1 最大程度压缩 \nrar a file1.rar test_file 创建一个叫做 'file1.rar' 的包 \nrar a file1.rar file1 file2 dir1 同时压缩 'file1', 'file2' 以及目录 'dir1' \nrar x file1.rar 解压rar包 \nunrar x file1.rar 解压rar包 \ntar -cvf archive.tar file1 创建一个非压缩的 tarball \ntar -cvf archive.tar file1 file2 dir1 创建一个包含了 'file1', 'file2' 以及 'dir1'的档案文件 \ntar -tf archive.tar 显示一个包中的内容 \ntar -xvf archive.tar 释放一个包 \ntar -xvf archive.tar -C /tmp 将压缩包释放到 /tmp目录下 \ntar -cvfj archive.tar.bz2 dir1 创建一个bzip2格式的压缩包 \ntar -jxvf archive.tar.bz2 解压一个bzip2格式的压缩包 \ntar -cvfz archive.tar.gz dir1 创建一个gzip格式的压缩包 \ntar -zxvf archive.tar.gz 解压一个gzip格式的压缩包 \nzip file1.zip file1 创建一个zip格式的压缩包 \nzip -r file1.zip file1 file2 dir1 将几个文件和目录同时压缩成一个zip格式的压缩包 \nunzip file1.zip 解压一个zip格式压缩包 \n**RPM 包 - （Fedora, Redhat及类似系统）** \nrpm -ivh package.rpm 安装一个rpm包 \nrpm -ivh --nodeeps package.rpm 安装一个rpm包而忽略依赖关系警告 \nrpm -U package.rpm 更新一个rpm包但不改变其配置文件 \nrpm -F package.rpm 更新一个确定已经安装的rpm包 \nrpm -e package_name.rpm 删除一个rpm包 \nrpm -qa 显示系统中所有已经安装的rpm包 \nrpm -qa | grep httpd 显示所有名称中包含 \"httpd\" 字样的rpm包 \nrpm -qi package_name 获取一个已安装包的特殊信息 \nrpm -qg \"System Environment/Daemons\" 显示一个组件的rpm包 \nrpm -ql package_name 显示一个已经安装的rpm包提供的文件列表 \nrpm -qc package_name 显示一个已经安装的rpm包提供的配置文件列表 \nrpm -q package_name --whatrequires 显示与一个rpm包存在依赖关系的列表 \nrpm -q package_name --whatprovides 显示一个rpm包所占的体积 \nrpm -q package_name --scripts 显示在安装/删除期间所执行的脚本l \nrpm -q package_name --changelog 显示一个rpm包的修改历史 \nrpm -qf /etc/httpd/conf/httpd.conf 确认所给的文件由哪个rpm包所提供 \nrpm -qp package.rpm -l 显示由一个尚未安装的rpm包提供的文件列表 \nrpm --import /media/cdrom/RPM-GPG-KEY 导入公钥数字证书 \nrpm --checksig package.rpm 确认一个rpm包的完整性 \nrpm -qa gpg-pubkey 确认已安装的所有rpm包的完整性 \nrpm -V package_name 检查文件尺寸、 许可、类型、所有者、群组、MD5检查以及最后修改时间 \nrpm -Va 检查系统中所有已安装的rpm包- 小心使用 \nrpm -Vp package.rpm 确认一个rpm包还未安装 \nrpm2cpio package.rpm | cpio --extract --make-directories *bin* 从一个rpm包运行可执行文件 \nrpm -ivh /usr/src/redhat/RPMS/`arch`/package.rpm 从一个rpm源码安装一个构建好的包 \nrpmbuild --rebuild package_name.src.rpm 从一个rpm源码构建一个 rpm 包 \n**YUM 软件包升级器 - （Fedora, RedHat及类似系统）** \nyum install package_name 下载并安装一个rpm包 \nyum localinstall package_name.rpm 将安装一个rpm包，使用你自己的软件仓库为你解决所有依赖关系 \nyum update package_name.rpm 更新当前系统中所有安装的rpm包 \nyum update package_name 更新一个rpm包 \nyum remove package_name 删除一个rpm包 \nyum list 列出当前系统中安装的所有包 \nyum search package_name 在rpm仓库中搜寻软件包 \nyum clean packages 清理rpm缓存删除下载的包 \nyum clean headers 删除所有头文件 \nyum clean all 删除所有缓存的包和头文件 \n**DEB 包 (Debian, Ubuntu 以及类似系统)** \ndpkg -i package.deb 安装/更新一个 deb 包 \ndpkg -r package_name 从系统删除一个 deb 包 \ndpkg -l 显示系统中所有已经安装的 deb 包 \ndpkg -l | grep httpd 显示所有名称中包含 \"httpd\" 字样的deb包 \ndpkg -s package_name 获得已经安装在系统中一个特殊包的信息 \ndpkg -L package_name 显示系统中已经安装的一个deb包所提供的文件列表 \ndpkg --contents package.deb 显示尚未安装的一个包所提供的文件列表 \ndpkg -S /bin/ping 确认所给的文件由哪个deb包提供 \n**APT 软件工具 (Debian, Ubuntu 以及类似系统)** \napt-get install package_name 安装/更新一个 deb 包 \napt-cdrom install package_name 从光盘安装/更新一个 deb 包 \napt-get update 升级列表中的软件包 \napt-get upgrade 升级所有已安装的软件 \napt-get remove package_name 从系统删除一个deb包 \napt-get check 确认依赖的软件仓库正确 \napt-get clean 从下载的软件包中清理缓存 \napt-cache search searched-package 返回包含所要搜索字符串的软件包名称 \n**查看文件内容** \ncat file1 从第一个字节开始正向查看文件的内容 \ntac file1 从最后一行开始反向查看一个文件的内容 \nmore file1 查看一个长文件的内容 \nless file1 类似于 'more' 命令，但是它允许在文件中和正向操作一样的反向操作 \nhead -2 file1 查看一个文件的前两行 \ntail -2 file1 查看一个文件的最后两行 \ntail -f /var/log/messages 实时查看被添加到一个文件中的内容 \n**文本处理** \ncat file1 file2 ... | command <> file1_in.txt_or_file1_out.txt general syntax for text manipulation using PIPE, STDIN and STDOUT \ncat file1 | command( sed, grep, awk, grep, etc...) > result.txt 合并一个文件的详细说明文本，并将简介写入一个新文件中 \ncat file1 | command( sed, grep, awk, grep, etc...) >> result.txt 合并一个文件的详细说明文本，并将简介写入一个已有的文件中 \ngrep Aug /var/log/messages 在文件 '/var/log/messages'中查找关键词\"Aug\" \ngrep ^Aug /var/log/messages 在文件 '/var/log/messages'中查找以\"Aug\"开始的词汇 \ngrep [0-9] /var/log/messages 选择 '/var/log/messages' 文件中所有包含数字的行 \ngrep Aug -R /var/log/* 在目录 '/var/log' 及随后的目录中搜索字符串\"Aug\" \nsed 's/stringa1/stringa2/g' example.txt 将example.txt文件中的 \"string1\" 替换成 \"string2\" \nsed '/^$/d' example.txt 从example.txt文件中删除所有空白行 \nsed '/ *#/d; /^$/d' example.txt 从example.txt文件中删除所有注释和空白行 \necho 'esempio' | tr '[:lower:]' '[:upper:]' 合并上下单元格内容 \nsed -e '1d' result.txt 从文件example.txt 中排除第一行 \nsed -n '/stringa1/p' 查看只包含词汇 \"string1\"的行 \nsed -e 's/ *$//' example.txt 删除每一行最后的空白字符 \nsed -e 's/stringa1//g' example.txt 从文档中只删除词汇 \"string1\" 并保留剩余全部 \nsed -n '1,5p;5q' example.txt 查看从第一行到第5行内容 \nsed -n '5p;5q' example.txt 查看第5行 \nsed -e 's/00*/0/g' example.txt 用单个零替换多个零 \ncat -n file1 标示文件的行数 \ncat example.txt | awk 'NR%2==1' 删除example.txt文件中的所有偶数行 \necho a b c | awk '{print $1}' 查看一行第一栏 \necho a b c | awk '{print $1,$3}' 查看一行的第一和第三栏 \npaste file1 file2 合并两个文件或两栏的内容 \npaste -d '+' file1 file2 合并两个文件或两栏的内容，中间用\"+\"区分 \nsort file1 file2 排序两个文件的内容 \nsort file1 file2 | uniq 取出两个文件的并集(重复的行只保留一份) \nsort file1 file2 | uniq -u 删除交集，留下其他的行 \nsort file1 file2 | uniq -d 取出两个文件的交集(只留下同时存在于两个文件中的文件) \ncomm -1 file1 file2 比较两个文件的内容只删除 'file1' 所包含的内容 \ncomm -2 file1 file2 比较两个文件的内容只删除 'file2' 所包含的内容 \ncomm -3 file1 file2 比较两个文件的内容只删除两个文件共有的部分 \n**字符设置和文件格式转换** \ndos2unix filedos.txt fileunix.txt 将一个文本文件的格式从MSDOS转换成UNIX \nunix2dos fileunix.txt filedos.txt 将一个文本文件的格式从UNIX转换成MSDOS \nrecode ..HTML < page.txt > page.html 将一个文本文件转换成html \nrecode -l | more 显示所有允许的转换格式 \n**文件系统分析** \nbadblocks -v /dev/hda1 检查磁盘hda1上的坏磁块 \nfsck /dev/hda1 修复/检查hda1磁盘上linux文件系统的完整性 \nfsck.ext2 /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性 \ne2fsck /dev/hda1 修复/检查hda1磁盘上ext2文件系统的完整性 \ne2fsck -j /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性 \nfsck.ext3 /dev/hda1 修复/检查hda1磁盘上ext3文件系统的完整性 \nfsck.vfat /dev/hda1 修复/检查hda1磁盘上fat文件系统的完整性 \nfsck.msdos /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性 \ndosfsck /dev/hda1 修复/检查hda1磁盘上dos文件系统的完整性 \n**初始化一个文件系统** \nmkfs /dev/hda1 在hda1分区创建一个文件系统 \nmke2fs /dev/hda1 在hda1分区创建一个linux ext2的文件系统 \nmke2fs -j /dev/hda1 在hda1分区创建一个linux ext3(日志型)的文件系统 \nmkfs -t vfat 32 -F /dev/hda1 创建一个 FAT32 文件系统 \nfdformat -n /dev/fd0 格式化一个软盘 \nmkswap /dev/hda3 创建一个swap文件系统 \n**SWAP文件系统** \nmkswap /dev/hda3 创建一个swap文件系统 \nswapon /dev/hda3 启用一个新的swap文件系统 \nswapon /dev/hda2 /dev/hdb3 启用两个swap分区 \n**备份** \ndump -0aj -f /tmp/home0.bak /home 制作一个 '/home' 目录的完整备份 \ndump -1aj -f /tmp/home0.bak /home 制作一个 '/home' 目录的交互式备份 \nrestore -if /tmp/home0.bak 还原一个交互式备份 \nrsync -rogpav --delete /home /tmp 同步两边的目录 \nrsync -rogpav -e ssh --delete /home ip_address:/tmp 通过SSH通道rsync \nrsync -az -e ssh --delete ip_addr:/home/public /home/local 通过ssh和压缩将一个远程目录同步到本地目录 \nrsync -az -e ssh --delete /home/local ip_addr:/home/public 通过ssh和压缩将本地目录同步到远程目录 \ndd bs=1M if=/dev/hda | gzip | ssh user@ip_addr 'dd of=hda.gz' 通过ssh在远程主机上执行一次备份本地磁盘的操作 \ndd if=/dev/sda of=/tmp/file1 备份磁盘内容到一个文件 \ntar -Puf backup.tar /home/user 执行一次对 '/home/user' 目录的交互式备份操作 \n( cd /tmp/local/ && tar c . ) | ssh -C user@ip_addr 'cd /home/share/ && tar x -p' 通过ssh在远程目录中复制一个目录内容 \n( tar c /home ) | ssh -C user@ip_addr 'cd /home/backup-home && tar x -p' 通过ssh在远程目录中复制一个本地目录 \ntar cf - . | (cd /tmp/backup ; tar xf - ) 本地将一个目录复制到另一个地方，保留原有权限及链接 \nfind /home/user1 -name '*.txt' | xargs cp -av --target-directory=/home/backup/ --parents 从一个目录查找并复制所有以 '.txt' 结尾的文件到另一个目录 \nfind /var/log -name '*.log' | tar cv --files-from=- | bzip2 > log.tar.bz2 查找所有以 '.log' 结尾的文件并做成一个bzip包 \ndd if=/dev/hda of=/dev/fd0 bs=512 count=1 做一个将 MBR (Master Boot Record)内容复制到软盘的动作 \ndd if=/dev/fd0 of=/dev/hda bs=512 count=1 从已经保存到软盘的备份中恢复MBR内容 \n**光盘** \ncdrecord -v gracetime=2 dev=/dev/cdrom -eject blank=fast -force 清空一个可复写的光盘内容 \nmkisofs /dev/cdrom > cd.iso 在磁盘上创建一个光盘的iso镜像文件 \nmkisofs /dev/cdrom | gzip > cd_iso.gz 在磁盘上创建一个压缩了的光盘iso镜像文件 \nmkisofs -J -allow-leading-dots -R -V \"Label CD\" -iso-level 4 -o ./cd.iso data_cd 创建一个目录的iso镜像文件 \ncdrecord -v dev=/dev/cdrom cd.iso 刻录一个ISO镜像文件 \ngzip -dc cd_iso.gz | cdrecord dev=/dev/cdrom - 刻录一个压缩了的ISO镜像文件 \nmount -o loop cd.iso /mnt/iso 挂载一个ISO镜像文件 \ncd-paranoia -B 从一个CD光盘转录音轨到 wav 文件中 \ncd-paranoia -- \"-3\" 从一个CD光盘转录音轨到 wav 文件中（参数-3） \ncdrecord --scanbus 扫描总线以识别scsi通道 \ndd if=/dev/hdc | md5sum 校验一个设备的md5sum编码，例如一张 CD \n**网络 - （以太网和WIFI无线**） \nifconfig eth0 显示一个以太网卡的配置 \nifup eth0 启用一个 'eth0' 网络设备 \nifdown eth0 禁用一个 'eth0' 网络设备 \nifconfig eth0 192.168.1.1 netmask 255.255.255.0 控制IP地址 \nifconfig eth0 promisc 设置 'eth0' 成混杂模式以嗅探数据包 (sniffing) \ndhclient eth0 以dhcp模式启用 'eth0' \nroute -n show routing table \nroute add -net 0/0 gw IP_Gateway configura default gateway \nroute add -net 192.168.0.0 netmask 255.255.0.0 gw 192.168.1.1 configure static route to reach network '192.168.0.0/16' \nroute del 0/0 gw IP_gateway remove static route \necho \"1\" > /proc/sys/net/ipv4/ip_forward activate ip routing \nhostname show hostname of system \nhost www.example.com lookup hostname to resolve name to ip address and viceversa\nnslookup www.example.com lookup hostname to resolve name to ip address and viceversa\nip link show show link status of all interfaces \nmii-tool eth0 show link status of 'eth0' \nethtool eth0 show statistics of network card 'eth0' \nnetstat -tup show all active network connections and their PID \nnetstat -tupl show all network services listening on the system and their PID \ntcpdump tcp port 80 show all HTTP traffic \niwlist scan show wireless networks \niwconfig eth1 show configuration of a wireless network card \nhostname show hostname \nhost www.example.com lookup hostname to resolve name to ip address and viceversa \nnslookup www.example.com lookup hostname to resolve name to ip address and viceversa \nwhois www.example.com lookup on Whois database \n\n \n\n**JPS工具**\n\njps(Java Virtual Machine Process Status Tool)是JDK 1.5提供的一个显示当前所有java进程pid的命令，简单实用，非常适合在linux/unix平台上简单察看当前java进程的一些简单情况。\n\n​    我想很多人都是用过unix系统里的ps命令，这个命令主要是用来显示当前系统的进程情况，有哪些进程，及其 id。 jps 也是一样，它的作用是显示当前系统的java进程情况，及其id号。我们可以通过它来查看我们到底启动了几个java进程（因为每一个java程序都会独占一个java虚拟机实例），和他们的进程号（为下面几个程序做准备），并可通过opt来查看这些进程的详细启动参数。\n\n​     **使用方法：在当前命令行下打 jps(需要JAVA_HOME，没有的话，到改程序的目录下打) 。**\n\n**jps存放在JAVA_HOME/bin/jps，使用时为了方便请将JAVA_HOME/bin/加入到Path.**\n\n$> **jps**\n23991 Jps\n23789 BossMain\n23651 Resin\n\n \n\n比较常用的参数：\n\n**-q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数**\n$>  **jps -q**\n28680\n23789\n23651\n\n**-m 输出传递给main 方法的参数，在嵌入式jvm上可能是null**\n\n$> **jps -m**\n28715 Jps -m\n23789 BossMain\n23651 Resin -socketwait 32768 -stdout /data/aoxj/resin/log/stdout.log -stderr /data/aoxj/resin/log/stderr.log\n\n**-l 输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名**\n\n$> **jps -l**\n28729 sun.tools.jps.Jps\n23789 com.asiainfo.aimc.bossbi.BossMain\n23651 com.caucho.server.resin.Resin\n\n**-v 输出传递给JVM的参数**\n\n$> **jps -v**\n23789 BossMain\n28802 Jps -Denv.class.path=/data/aoxj/bossbi/twsecurity/java/trustwork140.jar:/data/aoxj/bossbi/twsecurity/java/:/data/aoxj/bossbi/twsecurity/java/twcmcc.jar:/data/aoxj/jdk15/lib/rt.jar:/data/aoxj/jd\n\nk15/lib/tools.jar -Dapplication.home=/data/aoxj/jdk15 -Xms8m\n23651 Resin -Xss1m -Dresin.home=/data/aoxj/resin -Dserver.root=/data/aoxj/resin -Djava.util.logging.manager=com.caucho.log.LogManagerImpl -\n\nDjavax.management.builder.initial=com.caucho.jmx.MBeanServerBuilderImpl\n","tags":["Linux"],"categories":["Linux"]},{"title":"数据结构","url":"/2019/12/23/pout/python中高级面试题/数据结构/","content":"\n\n<!-- toc -->\n\n# 关于数据结构\n\n什么是数据结构？\n\n简单地说，数据结构是以某种特定的布局方式存储数据的容器。这种“布局方式”决定了数据结构对于某些操作是高效的，而对于其他操作则是低效的。首先我们需要理解各种数据结构，才能在处理实际问题时选取最合适的数据结构。\n\n为什么我们需要数据结构？\n\n数据是计算机科学当中最关键的实体，而数据结构则可以将数据以某种组织形式存储，因此，数据结构的价值不言而喻。\n\n无论你以何种方式解决何种问题，你都需要处理数据——无论是涉及员工薪水、股票价格、购物清单，还是只是简单的电话簿问题。\n\n数据需要根据不同的场景，按照特定的格式进行存储。有很多数据结构能够满足以不同格式存储数据的需求。\n\n常见的数据结构\n\n首先列出一些最常见的数据结构，我们将逐一说明：\n\n数组 栈 队列 链表 树 字典树（这是一种高效的树形结构，但值得单独说明） 散列表（哈希表）\n\n### 数组\n\n数组是最简单、也是使用最广泛的数据结构。栈、队列等其他数据结构均由数组演变而来。下图是一个包含元素（1，2，3和4）的简单数组，数组长度为4。\n\n每个数据元素都关联一个正数值，我们称之为索引，它表明数组中每个元素所在的位置。大部分语言将初始索引定义为零。\n\n以下是数组的两种类型：\n\n一维数组（如上所示） 多维数组（数组的数组）\n\n数组的基本操作\n\nInsert——在指定索引位置插入一个元素 Get——返回指定索引位置的元素 Delete——删除指定索引位置的元素 Size——得到数组所有元素的数量\n\n面试中关于数组的常见问题\n\n寻找数组中第二小的元素 找到数组中第一个不重复出现的整数 合并两个有序数组 重新排列数组中的正值和负值\n\n### 栈\n\n著名的撤销操作几乎遍布任意一个应用。但你有没有思考过它是如何工作的呢？这个问题的解决思路是按照将最后的状态排列在先的顺序，在内存中存储历史工作状态（当然，它会受限于一定的数量）。这没办法用数组实现。但有了栈，这就变得非常方便了。\n\n可以把栈想象成一列垂直堆放的书。为了拿到中间的书，你需要移除放置在这上面的所有书。这就是LIFO（后进先出）的工作原理。\n\n下图是包含三个数据元素（1，2和3）的栈，其中顶部的3将被最先移除：\n\n栈的基本操作\n\nPush——在顶部插入一个元素 Pop——返回并移除栈顶元素 isEmpty——如果栈为空，则返回true Top——返回顶部元素，但并不移除它\n\n面试中关于栈的常见问题\n\n使用栈计算后缀表达式 对栈的元素进行排序 判断表达式是否括号平衡\n\n### 队列\n\n与栈相似，队列是另一种顺序存储元素的线性数据结构。栈与队列的最大差别在于栈是LIFO（后进先出），而队列是FIFO，即先进先出。\n\n一个完美的队列现实例子：售票亭排队队伍。如果有新人加入，他需要到队尾去排队，而非队首——排在前面的人会先拿到票，然后离开队伍。\n\n下图是包含四个元素（1，2，3和4）的队列，其中在顶部的1将被最先移除：\n\n移除先入队的元素、插入新元素\n\n队列的基本操作\n\nEnqueue()——在队列尾部插入元素 Dequeue()——移除队列头部的元素 isEmpty()——如果队列为空，则返回true Top()——返回队列的第一个元素\n\n面试中关于队列的常见问题\n\n使用队列表示栈 对队列的前k个元素倒序 使用队列生成从1到n的二进制数\n\n### 链表\n\n链表是另一个重要的线性数据结构，乍一看可能有点像数组，但在内存分配、内部结构以及数据插入和删除的基本操作方面均有所不同。\n\n链表就像一个节点链，其中每个节点包含着数据和指向后续节点的指针。 链表还包含一个头指针，它指向链表的第一个元素，但当列表为空时，它指向null或无具体内容。\n\n链表一般用于实现文件系统、哈希表和邻接表。\n\n这是链表内部结构的展示：\n\n链表包括以下类型：\n\n单链表（单向） 双向链表（双向）\n\n链表的基本操作：\n\nInsertAtEnd - 在链表的末尾插入指定元素 InsertAtHead - 在链接列表的开头/头部插入指定元素 Delete - 从链接列表中删除指定元素 DeleteAtHead - 删除链接列表的第一个元素 Search - 从链表中返回指定元素 isEmpty - 如果链表为空，则返回true\n\n面试中关于链表的常见问题\n\n反转链表 检测链表中的循环 返回链表倒数第N个节点 删除链表中的重复项\n\n### 树\n\n树形结构是一种层级式的数据结构，由顶点（节点）和连接它们的边组成。 树类似于图，但区分树和图的重要特征是树中不存在环路。\n\n树形结构被广泛应用于人工智能和复杂算法，它可以提供解决问题的有效存储机制。\n\n这是一个简单树的示意图，以及树数据结构中使用的基本术语：\n\nRoot - 根节点\n\nParent - 父节点\n\nChild - 子节点\n\nLeaf - 叶子节点\n\nSibling - 兄弟节点\n\n以下是树形结构的主要类型：\n\nN元树 平衡树 二叉树 二叉搜索树 AVL树 红黑树 2-3树\n\n其中，二叉树和二叉搜索树是最常用的树。\n\n面试中关于树结构的常见问题：\n\n求二叉树的高度 在二叉搜索树中查找第k个最大值 查找与根节点距离k的节点 在二叉树中查找给定节点的祖先节点\n\n字典树（Trie）\n\n字典树，也称为“前缀树”，是一种特殊的树状数据结构，对于解决字符串相关问题非常有效。它能够提供快速检索，主要用于搜索字典中的单词，在搜索引擎中自动提供建议，甚至被用于IP的路由。\n\n这些单词以顶部到底部的方式存储，其中绿色节点“p”，“s”和“r”分别表示“top”，“thus”和“theirs”的底部。\n\n面试中关于字典树的常见问题\n\n计算字典树中的总单词数 打印存储在字典树中的所有单词 使用字典树对数组的元素进行排序 使用字典树从字典中形成单词 构建T9字典（字典树+ DFS ）\n\n### 哈希表\n\n哈希法（Hashing）是一个用于唯一标识对象并将每个对象存储在一些预先计算的唯一索引（称为“键（key）”）中的过程。因此，对象以键值对的形式存储，这些键值对的集合被称为“字典”。可以使用键搜索每个对象。基于哈希法有很多不同的数据结构，但最常用的数据结构是哈希表。\n\n哈希表通常使用数组实现。\n","tags":["python中高级面试题"]},{"title":"Linux用户组及权限管理","url":"/2019/12/23/pout/Linux/Linux用户组及权限管理/","content":"\n\n<!-- toc -->\n\n# Linux用户组及权限管理\n\n\n\n## 用户和组\n\n> `Linux`是一个多用户的操作系统，引入用户，可以更加方便管理`Linux`服务器\n>\n> 系统默认需要以一个用户的身份登入，而且在系统上启动进程也需要以一个用户身份器运行，用户可以限制某些进程对特定资源的权限控制\n\n### Linux用户及组\n\n> `Linux`操作系统对多用户的管理，是非常繁琐的，所以用组的概念来管理用户就变得简单，每个用户可以在一个独立的组，每个组也可以有零个用户或者多个用户。\n>\n> `Linux`系统用户是根据用户`ID`来识别的，默认`ID`长度为`32`位，从默认`ID`编号从`0`开始，但是为了和老式系统兼容，用户`ID`限制在`60000`以下，`Linux`用户分总共分为三种，分别如下\n\n- 超级用户：`root`，`ID`为0\n- 系统用户：`ID`从1到499\n- 普通用户：`ID`为500以上\n\n> `Linux`系统中的每个文件或者文件夹，都有一个所属用户及所属组\n>\n> 使用`id`命令可以显示当前用户的信息，使用`passwd`命令可以修改当前用户密码。`Linux`操作系统用户的特点如下\n\n- 每个用户拥有一个`UserID`，操作系统实际读取的是`UID`，而非用户名；\n- 每个用户属于一个主组，属于一个或多个附属组，一个用户最多有`31`个附属组；\n- 每个组拥有一个`GroupID`；\n- 每个进程以一个用户身份运行，该用户可对进程拥有资源控制权限；\n- 每个可登陆用户拥有一个指定的`Shell`环境\n\n### Linux用户管理\n\n> `Linux`用户在操作系统可以进行日常管理和维护，涉及到的相关配置文件如下\n\n- `/etc/passwd`：保存用户信息\n- `/etc/shadow`：保存用户密码（以加密形式保存）\n- `/etc/group`：保存组信息\n- `/etc/login.defs`：用户属性限制，密码过期时间，密码最大长度等限制\n- `/etc/default/useradd`：显示或更改默认的`useradd`配置文件\n\n#### 创建新用户\n\n```\nuseradd usertest # 创建用户usertest\n```\n\n> 创建新用户，可以使用命令`useradd`，执行命令即可创建新用户\n>\n> 同时会创建一个同名的组，默认该用户属于该用户组\n\n> 创建用户，会根据如下步骤进行操作\n\n- 在`/etc/passwd`文件中添加用户信息\n- 如使用`passwd`命令创建密码，密码会被加密保存在`/etc/shdaow`中\n- 为用户创建家目录：`/home/usertest`，创建目录操作应操作系统而异\n- 将`/etc/skel`中的`.bash`开头的文件复制至用户家目录\n- 创建与用户名相同的组，该用户默认属于这个同名组，组信息保存在`/etc/group`配置文件中\n\n> 其他命令可选参数如下所示\n\n```\n-d # 指定新用户的主目录\n-G # 指定新用户的组列表\n-s # 新用户所使用的shell环境\n```\n\n```\nuseradd usertest -s /bin/bash -d /home/usertest\n# 创建新用户usertest，指定shell环境为bash，主目录在/home/usertest\n```\n\n#### 删除用户\n\n```\nuserdel # 保留用户的家目录\nuserdel –r usertest # 删除用户及用户家目录，用户login系统无法删除\nuserdel –rf usertest # 强制删除用户及该用户家目录，不论是否login系统\n```\n\n> 当一个用户创建之后，我们可以通过`usermod`命令来修改用户及组的属性\n\n- `linux`下命令选项\n\n```\n选项：\n  -c, --comment 注释            GECOS 字段的新值\n  -d, --home HOME_DIR           用户的新主目录\n  -e, --expiredate EXPIRE_DATE  设定帐户过期的日期为 EXPIRE_DATE\n  -f, --inactive INACTIVE       过期 INACTIVE 天数后，设定密码为失效状态\n  -g, --gid GROUP               强制使用 GROUP 为新主组\n  -G, --groups GROUPS           新的附加组列表 GROUPS\n  -a, --append GROUP            将用户追加至上边 -G 中提到的附加组中，\n                                并不从其它组中删除此用户\n  -h, --help                    显示此帮助信息并推出\n  -l, --login LOGIN             新的登录名称\n  -L, --lock                    锁定用户帐号\n  -m, --move-home               将家目录内容移至新位置 (仅于 -d 一起使用)\n  -o, --non-unique              允许使用重复的(非唯一的) UID\n  -p, --password PASSWORD       将加密过的密码 (PASSWORD) 设为新密码\n  -R, --root CHROOT_DIR         chroot 到的目录\n  -s, --shell SHELL             该用户帐号的新登录 shell\n  -u, --uid UID                 用户帐号的新 UID\n  -U, --unlock                  解锁用户帐号\n  -Z, --selinux-user  SEUSER       用户账户的新 SELinux 用户映射\n```\n\n```\ngroups username\n# 查看用户所属组\n```\n\n#### 修改用户所属组\n\n```\nusermod usertest -G old_normal\n# 将用户usertest修改加入old_normal组中\n```\n\n```\nusermod usertest -a -G other_normal\n# 将用户追加至other_normal组中，且不影响原有组状态\n```\n\n```\ncat /etc/group | grep usertest \n# 可以查看到usertest用户当前所属组的情况\n```\n\n#### 修改用户家目录及启动shell\n\n```\nusermod usertest -d /home/user -s /bin/sh\n```\n\n#### 修改用户名\n\n```\nusermod -l new old\n# 将old用户名变为new\n```\n\n#### 锁定/解锁用户\n\n```\nusermod -L usertest;\n# 锁定usertest用户\nusermod -U usertest;\n# 解锁usertest用户\n```\n\n### Linux组管理\n\n> 所有的`Linux`或者`Windows`系统都有组的概念，通过组可以更加方便的管理用户\n>\n> 组的概念应用于各行行业，例如企业会使用部门、职能或地理区域的分类方式来管理成员，映射在`Linux`系统，同样可以创建用户，并用组的概念对其管理\n>\n> Linux组有如下特点\n\n- 每个组有一个组`ID`\n- 组信息保存在`/etc/group`中\n- 每个用户至少拥有一个主组，同时还可以拥有`31`个附属组\n\n#### 创建新组\n\n```\ngroupadd normal # 创建normal组\n```\n\n```\ngroupadd -g 1000 normal # 创建ID为1000的分组\n```\n\n#### 其他组属性\n\n> 常见参数\n\n```\n-r # 系统账户\n-f # 如果指定的组已经存在，则退出\n-g # 指定当前组id\n-n --new --old # 修改组名\n```\n\n```\ngroupmod -n old_normal normal\n# 修改normal组名为old_normal\n```\n\n```\ngroupmod -g 1001 old_normal\n# 修改old_normal组id为1001\n```\n\n## 权限\n\n> 设置好了用户和组，那么接下来就可以对其进行权限控制\n>\n> 由于linux下处处皆文件，所谓权限也就是对文件的**读**、**写**、**执行**，至少这三种\n>\n> 当操作系统下某个进程在运行时，进程的权限，也相当于这个进程的运行用户身份权限\n\n| 权限 | 文件     | 目录           |\n| ---- | -------- | -------------- |\n| r    | 读取文件 | 列出目录       |\n| w    | 修改文件 | 修改目录内文件 |\n| x    | 执行文件 | 进入目录       |\n\n- 权限分组\n\n> 默认的linux的权限分为三种角色\n>\n> `user`、`group`、`other`\n>\n> 每个文件将基于**UGO**三种权限进行设置\n>\n> 一般一个文件创建之后，谁创建该文件，默认成为该文件的所有者\n\n### 用户及组权限\n\n```\nls -ahl\n# 查看文件所有者\n```\n\n```\nchmod g+rwx file\n# 给file文件增加rwx权限\nchmod g-x file\n# 给file文件减少x权限\n```\n\n### 用户及组修改\n\n> 修改某个文件或目录所属**用户**或**组**\n\n```\nchown -R root file\n# 修改file文件所属用户为root\n```\n\n```\nchown -R :root file\n# 修改file文件所属用户为root\nchgrp -R root file\n# 修改file文件所属组为root\n```\n\n### 二进制权限\n\n> linux下具备权限设置为1，反之为0，那么一个权限按照二进制位数来计算，如下所示\n\n```\n--x: 001 1\n-wx: 011 3\nrwx: 111 7\n```\n\n> 很清晰，对应的权限位置所代表的数字分别是：**r=4**，**w=2**，**x=1**\n\n```\nchmod 775 file\n# 修改file文件权限为 rwxrwxr-x\n```\n\n### 权限掩码\n\n> 神奇的事情需要我们考虑，每次创建文件，默认都会具备一定的权限，而这个权限是如何分配而来的呢？\n>\n> 是通过一个叫做权限掩码的东西来维护的，这个码可以通过**umask**命令看到\n>\n> 默认系统的掩码是**022**\n\n- 文件权限由默认权限减去掩码\n\n> 文件默认权限：666\n> 那么创建一个文件真实的权限是：666-022=644\n\n> 目录的默认权限：777\n>\n> 一个目录的真实权限是：777-022=755\n\n- 设置默认掩码\n\n```\numask -S 011\n```\n\n### 特殊权限\n\n| 权限       | 对文件的影响                                 | 对目录的影响                                                 |\n| ---------- | -------------------------------------------- | ------------------------------------------------------------ |\n| **suid**   | 以文件的所属用户身份执行，而非执行文件的用户 | 无                                                           |\n| **sgid**   | 以文件所属组身份去执行                       | 在该目录中创建任意新文件的所属组与该目录的所属组相同         |\n| **sticky** | 无                                           | 对目录拥有写入权限的用户仅可以删除其拥有的文件，无法删除其他用户所拥有的文件 |\n","tags":["Linux"]},{"title":"python基础","url":"/2019/12/23/pout/python中高级面试题/python基础/","content":"\n\n<!-- toc -->\n\n# 解释一下Python中的三元运算\n\n```\n[on true] if [expression] else [on false]\n```\n\n如果表达式为True，就执行[on true]中的语句。否则，就执行[on false]中的语句\n\n```\na,b=2,3\nmin=a if a<b else b\nmin\n```\n\n## GIL: 全局解释器锁（英语：Global Interpreter Lock，缩写GIL），是计算机程序设计语言解释器用于同步线程的一种机制，它使得任何时刻仅有一个线程在执行。[1]即便在多核心处理器上，使用 GIL 的解释器也只允许同一时间执行一个线程。\n\n# 解释一下Python中的继承\n\n当一个类继承自另一个类，它就被称为一个子类/派生类，继承自父类/基类/超类。它会继承/获取所有类成员（属性和方法）。\n\n继承能让我们重新使用代码，也能更容易的创建和维护应用。Python支持如下种类的继承：\n\n单继承：一个类继承自单个基类 多继承：一个类继承自多个基类 多级继承：一个类继承自单个基类，后者则继承自另一个基类 分层继承：多个类继承自单个基类 混合继承：两种或多种类型继承的混合\n\n# 解释Python中的help()和dir()函数\n\nHelp()函数是一个内置函数，用于查看函数或模块用途的详细说明：\n\n```\nimport copy\nhelp(copy.copy)\n```\n\nDir()函数也是Python内置函数，dir() 函数不带参数时，返回当前范围内的变量、方法和定义的类型列表；带参数时，返回参数的属性、方法列表。\n\n```\ndir(copy.copy)\n```\n\n# 什么是猴子补丁？\n\n在运行期间动态修改一个类或模块。\n\n```\nclass A:\n    def func(self):\n        print(\"Hi\")\ndef monkey(self):\n    print(\"Hi, monkey\")\nm.A.func = monkey\na = m.A()\na.func()\n```\n\n# 请解释使用*args和\\**kwargs的含义\n\n当我们不知道向函数传递多少参数时，比如我们向传递一个列表或元组，我们就使用*args。\n\n```\n>>> def func(*args):\n    for i in args:\n        print(i)  \n>>> func(3,2,1,4,7)\n```\n\n在我们不知道该传递多少关键字参数时，使用**kwargs来收集关键字参数。\n\n```\n>>> def func(**kwargs):\n    for i in kwargs:\n        print(i,kwargs[i])\n>>> func(a=1,b=2,c=7)\n```\n\n# 什么是负索引？\n\n负索引和正索引不同，它是从右边开始检索。\n\n它也能用于列表中的切片：\n\n```\nmylist=[0,1,2,3,4,5,6,7,8]\nmylist[-3]\nmylist[-6:-1]\n```\n\n# 解释Python中的join()和split()函数\n\nJoin()能让我们将指定字符添加至字符串中。\n\nSplit()能让我们用指定字符分割字符串。\n\n# 怎么移除一个字符串中的前导空格？\n\n字符串中的前导空格就是出现在字符串中第一个非空格字符前的空格。我们使用方法Istrip()可以将它从字符串中移除。\n\n```\n'   Ayushi '.lstrip()\n```\n\n可以看到，该字符串既有前导字符，也有后缀字符，调用Istrip()去除了前导空格。如果我们想去除后缀空格，就用rstrip()方法。\n\n```\n'   Ayushi '.rstrip()\n```\n\n# Python中的pass语句是什么？\n\n在用Python写代码时，有时可能还没想好函数怎么写，只写了函数声明，但为了保证语法正确，必须输入一些东西，在这种情况下，我们会使用pass语句。\n\n```\ndef func(*args):\n    pass\n```\n\n同样，break语句能让我们跳出循环。\n\n```\nfor i in range(7):\n    if i==3: break\n```\n\n最后，continue语句能让我们跳到下个循环。\n\n```\nfor i in range(7):\n    if i==3: continue\n    print(i)\n```\n\n# Python中的闭包是什么？\n\n当一个嵌套函数在其外部区域引用了一个值时，该嵌套函数就是一个闭包。其意义就是会记录这个值。\n\n```\ndef A(x):\n    def B():\n        print(x)\n    return B\n```\n\n# 谈一谈Python的装饰器（decorator）\n\n装饰器本质上是一个Python函数，它可以让其它函数在不作任何变动的情况下增加额外功能，装饰器的返回值也是一个函数对象。它经常用于有切面需求的场景。比如：插入日志、性能测试、事务处理、缓存、权限校验等。有了装饰器我们就可以抽离出大量的与函数功能无关的雷同代码进行重用。\n\n装饰器其实就是一个闭包，把一个函数当做参数然后返回一个替代版函数\n\n# 解释一下Python中的逻辑运算符\n\nPython中有3个逻辑运算符：and，or，not\n\n# Python支持什么数据类型？\n\n1. Numbers（数字）——用于保存数值\n2. Strings（字符串）——字符串是一个字符序列。我们用单引号或双引号来声明字符串。\n3. Lists（列表）——列表就是一些值的有序集合，我们用方括号声明列表。\n4. Tuples（元组）——元组和列表一样，也是一些值的有序集合，区别是元组是不可变的，意味着我们无法改变元组内的值。\n5. Dictionary（字典）——字典是一种数据结构，含有键值对。我们用大括号声明字典\n\n# 什么是切片？\n\n切片是Python中的一种方法，能让我们只检索列表、元素或字符串的一部分。在切片时，我们使用切片操作符[]。\n\n```\n(1,2,3,4,5)[2:4]\n```\n\n# Python中的不可变集合（frozenset）是什么？\n\n首先，我们讨论一下什么是集合。集合就是一系列数据项的合集，不存在任何副本。另外，集合是无序的。\n\n这就意味着我们无法索引它。\n\n不过，集合是可变的。而不可变集合却不可变，这意味着我们无法改变它的值，从而也使其无法作为字典的键值。\n\n```\nmyset=frozenset([1,3,2,2])\nmyset\n```\n\n# 解释lambda表达式，什么时候会用到它？\n\n如果我们需要一个只有单一表达式的函数，我们可以匿名定义它。拉姆达表达式通常是在需要一个函数，但是又不想费神去命名一个函数的场合下使用，也就是指匿名函数。\n\n```\n(lambda a,b:a if a>b else b)(3,3.5)\n```\n\n实现斐波那契数列\n\n```\nfib = lambda n : n if n <= 2 else fib(n-1)+fib(n-2)\n```\n\n# 什么是递归？\n\n在调用一个函数的过程中，直接或间接地调用了函数本身这个就叫递归。但为了避免出现死循环，必须要有一个结束条件\n\n```\ndef facto(n):\n    if n==1: return 1\n    return n*facto(n-1)\nfacto(4)\n```\n\n# 什么是生成器？\n\n生成器会生成一系列的值用于迭代，这样看它又是一种可迭代对象。它是在for循环的过程中不断计算出下一个元素，并在适当的条件结束for循环。\n\n# 什么是迭代器？\n\n迭代器是访问集合元素的一种方式。迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。迭代器只能往前不会后退。我们使用inter()函数创建迭代器。\n\n# 请说说生成器和迭代器之间的区别?\n\n在使用生成器时，我们创建一个函数；在使用迭代器时，我们使用内置函数iter()和next()。 在生成器中，我们使用关键字‘yield’来每次生成/返回一个对象。 生成器中有多少‘yield’语句，你可以自定义。 每次‘yield’暂停循环时，生成器会保存本地变量的状态。而迭代器并不会使用局部变量，它只需要一个可迭代对象进行迭代。 使用类可以实现你自己的迭代器，但无法实现生成器。 生成器运行速度快，语法简洁，更简单。 迭代器更能节约内存。\n\n# Python中的yield用法\n\nyield简单说来就是一个生成器，这样函数它记住上次返 回时在函数体中的位置。对生成器第 二次(或n 次)调用跳转至该函 次)调用跳转至该函数。\n\n# 解释Python的参数传递机制\n\nPython使用按引用传递（pass-by-reference）将参数传递到函数中。如果你改变一个函数内的参数，会影响到函数的调用。这是Python的默认操作。不过，如果我们传递字面参数，比如字符串、数字或元组，它们是按值传递，这是因为它们是不可变的。\n\npython不允许程序员选择采用传值还是传引用。Python参数传递采用的肯定是“传对象引用”的方式。这种方式相当于传值和传引用的一种综合。如果函数收到的是一个可变对象（比如字典或者列表）的引用，就能修改对象的原始值－－相当于通过“传引用”来传递对象。如果函数收到的是一个不可变对象（比如数字、字符或者元组）的引用，就不能直接修改原始对象－－相当于通过“传值'来传递对象。\n\n# 如何在Python中创建自己的包？\n\nPython中创建包是比较方便的，只需要在当前目录建立一个文件夹，文件夹中包含一个**init**.py文件和若干个模块文件，其中**init**.py可以是一个空文件，但还是建议将包中所有需要导出的变量放到**all**中，这样可以确保包的接口清晰明了，易于使用。\n\n# 元类\n\n元类是类的类对象，换言之类是元类的实例，Python中默认的元类为type，可以通过自定义元类的方式实现对类创建的控制。\n\n```\nclass Base:\n    a = 1\n    b = 2\n\n    print('class defined')\n\n    def __new__(cls, *args, **kwargs):\n        print(cls.__name__, 'class instance created')\n        return super().__new__(cls)\n\n    def __init__(self):\n        print(type(self).__name__, 'class instance inited')\n\n    def hello(self):\n        pass\n\n\nb = Base()\n```\n\n当调用print(type(b))，得到，可知b是Base类的实例。Python是纯面向对象语言，因此类也是对象，当调用print(type(Base))时得到，可知类Base是type的实例，type就是Python中的原生元类，用来控制、生成类这个对象。 一般地，在定义类时，默认的此类的元类是type，因此，如果我们想控制类的创建，需要将类的元类指为我们自定义的元类，这个自定义的元类需要继承type元类。\n","tags":["python中高级面试题"]},{"title":"云服务操作和Git基本操作","url":"/2019/12/23/pout/云主机和Get操作/","content":"\n\n<!-- toc -->\n\n<h2>云服务操作</h2>\nclear  ------清屏\n\nmkdir + 文件名 ----- 创建文件\n\nrm -rf +文件名 ------- 删除文件目录以及它所包含的所有内容\n\n端口被占用问题：sudo fuser -k 8000/tcp \n\npython3 manage.py runserver 0.0.0.0:8000\n\n<h2>Git基本操作</h2>\ngit init     #初始化\ngit pull     # 从远程仓库拉到本地\ngit remote add origin + 码云仓库地址   #  连接远程仓库\ngit checkout 分支名称       #切换分支\ngit add -A 文件名      #上传文件\ngit commit -m '1231'   # 备注\ngit push  #提交\n\ngit clone (拉取代码)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Git"]},{"title":"运维话术","url":"/2019/12/23/pout/运维/话术/","content":"\n\n<!-- toc -->\n\n# 搞清楚系统到底怎样支撑高并发以及架构图的绘制（面试向）\n\n[首页](https://v3u.cn/) - [Mac & Linux](https://v3u.cn/l_id_4) /2019-07-02\n\n​    大多数人面试的时候经常会被问到：你简历上有高负载高并发的经验，那到底你的系统是怎样设计的？\n\n​    如果没有过相关的项目经验，大多数同学被问到这个问题压根儿没什么思路去回答，不知道从什么地方说起，其实，就算没有相关的经验，只要事先编好话术，搞清楚架构图，回答此类问题也还是可以滴水不漏的。\n\n​    首先，在脑子里虚拟一个大用户量背景下的场景，如果我们手头有几台4核8g的服务器，假设一个平台用户量是500万。此时日活用户是50万，日访问量在600-700万左右，高峰期对系统每秒请求是500/s。然后对数据库的每秒请求数量是1500/s，这个时候会怎么样呢？如果系统内处理的是较为复杂的一些业务逻辑，是那种重业务逻辑的系统的话，是比较耗费CPU的。此时，4核8G的机器每秒请求达到500/s的时候，很可能你的机器CPU负载较高了。然后数据库层面，以上述的配置而言，其实基本上1500/s的高峰请求压力的话，还算可以接受。这个主要是要观察数据库所在机器的磁盘负载、网络负载、CPU负载、内存负载，按照我们的线上经验而言，那个配置的数据库在1500/s请求压力下是没问题的。所以此时需要做的一个事情，首先就是要支持你的系统集群化部署。可以在前面挂一个负载均衡层，把请求均匀打到系统层面，让系统可以用多台机器集群化支撑更高的并发压力。比如说这里假设给系统增加部署一台机器，那么每台机器就只有250/s的请求了。这样一来，两台机器的CPU负载都会明显降低，这个初步的“高并发”不就先抗住住了吗？要是连这个都不做，那单台机器负载越来越高的时候，极端情况下是可能出现机器上部署的系统无法有足够的资源响应请求了，然后出现请求卡死，甚至系统宕机之类的问题。所以，简单小结，第一步要做的：添加负载均衡层，将请求均匀打到系统层。系统层采用集群化部署多台机器，扛住初步的并发压力。\n\n​    架构图如下：\n\n​    ![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190702/20190702095226_73599.jpg)\n\n​    然后，经过了几个月的增长期，假设此时用户量继续增长，达到了1000万注册用户，然后每天日活用户是100万，日访问量在800-1000万。那么此时对系统层面的请求量会达到每秒1000/s，系统层面，你可以继续通过集群化的方式来扩容，反正前面的负载均衡层会均匀分散流量过去的。但是，这时数据库层面接受的请求量会达到3000/s，这个就有点问题了。此时数据库层面的并发请求翻了一倍，你一定会发现线上的数据库负载越来越高。每次到了高峰期，磁盘IO、网络IO、内存消耗、CPU负载的压力都会很高，大家很担心数据库服务器能否抗住。没错，一般来说，对那种普通配置的线上数据库，建议就是读写并发加起来，按照上述我们举例的那个配置，不要超过3000/s。因为数据库压力过大，首先一个问题就是高峰期系统性能可能会降低，因为数据库负载过高对性能会有影响。另外一个，压力过大把你的数据库给搞挂了怎么办？所以此时你必须得对系统做分库分表 + 读写分离，也就是把一个库拆分为多个库，部署在多个数据库服务上，这是作为主库承载写入请求的。然后每个主库都挂载至少一个从库，由从库来承载读请求。此时假设对数据库层面的读写并发是3000/s，其中写并发占到了1000/s，读并发占到了2000/s。那么一旦分库分表之后，采用两台数据库服务器上部署主库来支撑写请求，每台服务器承载的写并发就是500/s。每台主库挂载一个服务器部署从库，那么2个从库每个从库支撑的读并发就是1000/s。简单总结，并发量继续增长时，我们就需要专注在数据库层面：分库分表、读写分离。\n    如果注册用户量越来越大，此时你可以不停地加机器，比如说系统层面不停加机器，就可以承载更高的并发请求。然后数据库层面如果写入并发越来越高，就扩容加数据库服务器，通过分库分表是可以支持扩容机器的，如果数据库层面的读并发越来越高，就扩容加更多的从库。但是这里有一个很大的问题：数据库其实本身不是用来承载高并发请求的。所以通常来说，数据库单机每秒承载的并发就在几千的数量级，而且数据库使用的机器都是比较高配置，比较昂贵的机器，成本很高。如果不停地加机器，这是不对的。在高并发架构里通常都有缓存这个环节，缓存系统的设计就是为了承载高并发而生。单机承载的并发量都在每秒几万，甚至每秒数十万，对高并发的承载能力比数据库系统要高出一到两个数量级。可以根据系统的业务特性，对那种写少读多的请求，引入缓存集群。具体来说，就是在写数据库的时候同时写一份数据到缓存集群里，然后用缓存集群来承载大部分的读请求。这样的话，通过缓存集群，就可以用更少的机器资源承载更高的并发。比如说上面那个图里，读请求目前是每秒1000/s，两个从库各自抗了500/s读请求，但是其中可能每秒800次的读请求都是可以直接读缓存里的不怎么变化的数据的。那么此时你一旦引入缓存集群，就可以抗下来这800/s读请求，落到数据库层面的读请求就200/s。\n\n​    架构图如下：\n\n​    ![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190702/20190702095650_68435.jpg)\n\n​    初步来说，简单的一个高并发系统的阐述是说完了，是不是很简单呢？\n\n随机文章[使用ApacheBench来对美多商城的秒杀功能进行高并发压力测试](https://v3u.cn/a_id_65)2019-04-12[python操作excel](https://v3u.cn/a_id_7)2013-04-13[移动布局方案(转载)](https://v3u.cn/a_id_15)2017-03-22[在阿里云Centos7.6上面配置Mysql主从数据库(master/slave)，实现读写分离](https://v3u.cn/a_id_85)2019-05-29[Python3的原生协程和Tornado异步非阻塞](https://v3u.cn/a_id_113)2019-09-20[禁止爬虫爬你的页面](https://v3u.cn/a_id_10)2013-04-21[使用python3和高性能全文检索引擎Redisearch进行交互](https://v3u.cn/a_id_106)2019-08-30[关于mysql表引擎的问题](https://v3u.cn/a_id_21)2016-09-10[Python3利用ffmpeg针对视频进行一些操作](https://v3u.cn/a_id_74)2019-05-15[mysql终端查看中文乱码问题](https://v3u.cn/a_id_20)2015-02-20[python的dict中key为变量的使用技巧](https://v3u.cn/a_id_36)2015-03-22[15个在github上最受欢迎的py框架,记录一下](https://v3u.cn/a_id_6)2014-09-13[利用CSS3自定义属性来为网站添加“暗黑模式”](https://v3u.cn/a_id_114)2019-09-22[在阿里云Centos7.6上面部署基于redis的分布式爬虫scrapy-redis](https://v3u.cn/a_id_83)2019-05-27[Django通过xlwt用文件流的方式下载excel文档](https://v3u.cn/a_id_39)2017-04-24\n","tags":["运维"],"categories":["运维"]},{"title":"负载均衡","url":"/2019/12/23/pout/运维/负载均衡/","content":"\n\n<!-- toc -->\n\n## 修改nginx.conf配置文件\n\n```\nuser  root;\nworker_processes  1;\n\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\n\n\nevents {\n    worker_connections  1024;\n}\n\n\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n    upstream mytest {\n    server 114.55.39.15;  #负载均衡服务器群\n    server 47.98.237.5;\n\t}\n\n    include /etc/nginx/conf.d/*.conf;\n}\n```\n\n假如有default.conf修改为：\n\n```\nserver {\n    listen       80;\n    server_name  localhost;\n\n    access_log      /root/myweb_access.log;\n    error_log       /root/myweb_error.log;\n\n\n    client_max_body_size 75M;\n\n\n    location / {\n    proxy_pass http://mytest;\n    proxy_redirect default;\n        root /root/public;\n        index index.html;\n    }\n}\n```\n\n\n\n```\n1、轮询（默认）\n每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。\n\nupstream backserver {\n    server 192.168.0.14;\n    server 192.168.0.15;\n}\n\n\n2、权重 weight\n指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。\n\nupstream backserver {\n    server 192.168.0.14 weight=3;\n    server 192.168.0.15 weight=7;\n}\n\n\n3、ip_hash（ IP绑定）\n上述方式存在一个问题就是说，在负载均衡系统中，假如用户在某台服务器上登录了，那么该用户第二次请求的时候，因为我们是负载均衡系统，每次请求都会重新定位到服务器集群中的某一个，那么已经登录某一个服务器的用户再重新定位到另一个服务器，其登录信息将会丢失，这样显然是不妥的。\n\n我们可以采用ip_hash指令解决这个问题，如果客户已经访问了某个服务器，当用户再次访问时，会将该请求通过哈希算法，自动定位到该服务器。\n\n每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。\n\nupstream backserver {\n    ip_hash;\n    server 192.168.0.14:88;\n    server 192.168.0.15:80;\n}\n\n\n4、fair（第三方插件）\n按后端服务器的响应时间来分配请求，响应时间短的优先分配。\n\nupstream backserver {\n    server server1;\n    server server2;\n    fair;\n}\n\n\n5、url_hash（第三方插件）\n按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。\n\nupstream backserver {\n    server squid1:3128;\n    server squid2:3128;\n    hash $request_uri;\n    hash_method crc32;\n}\n\n```\n\n","tags":["运维"]},{"title":"曾经遇见过的问题","url":"/2019/12/23/pout/曾出现的问题/","content":"\n\n<!-- toc -->\n\n\n\n## 支付宝公钥和私钥遇到的问题\n\n首先，新版支付宝应用了新的的加密算法，所以生成应用公钥和私钥的时候要使用RSA2的算法\n\n之前我在请求支付接口的时候，误操作把应用公钥发送了过去，导致接口504，实际上发送给支付宝接口的私钥应该是我自己生成应用私钥，而公钥则应该使用支付宝的公钥，而不是我自己生成的应用公钥\n\n## 支付宝退款遇到的问题\n\n在开发支付宝退款的业务时，支付宝官网有两个订单字段 out_trade_no和trade_no，我误以为都是订单号，所以随便传了一个导致退款错误，实际上out_trade_no是商户订单号，而trade_no则是支付宝自己生成的支付宝交易号，用这两个号都可以退换成功，但是out_trade_no是商户自己的订单号，本身也存在数据库里，而支付宝交易号如果在回调方法内不入库，则无法在退款的时候传递给支付宝，所以建议使用商户订单号 out_trade_no\n\n## 支付时遇到的性能问题\n\n在支付宝支付功能上线后，发现在某些时间段内，服务器就会报警，查了一下，内存占用过高，理论上一个很普通的支付宝三方支付业务不应该会导致性能问题，仔细看了一下源代码，发现当时写支付类的时候很随意，没有考虑到每次支付都会在内存中新建实例，而python的内存管理机制导致支付结束后不会主动销毁，所以我改造了一下支付类，将其改造成了单例模式，这样就解决了支付频率过高导致内存占用过高的问题。\n\n```\nfrom functools import wraps\ndef singleton(cls):\n    instance = None\n    @wraps(cls)\n    def wrap(*args,**kwargs):\n        nonlocal instance\n        if instance is None:\n            instance = cls(*args,**kwargs)    #args,kwargs是用于将参数传递到__init__中\n        return instance\n    return wrap\n@singleton\nclass A:\n    pass\na = A()\na1 = A()\nid(a)\nid(a1)\n```\n\n## 使用Supervisor管理后台服务遇到的问题\n\n我想用Supervisor来监控后台的uwsgi服务，结果始终显示fatal报错，但是别的服务都可以，仔细研读文档才发现，原来Supervisor本身无法监控带有守护进程的服务，而wsgi本身的配置文件中有守护进程的属性，所以我在wsgi的配置文件中关闭了守护进程选项，直接依赖Supervisor的特性赋予uwsgi守护进程，如果uwsgi服务被kill则不需要依赖自身的守护进程拉起，而是变成依赖Supervisor拉起服务，这样就实现了Supervisor监控uwsgi服务\n\n## 使用Celery异步任务时遇到的问题\n\n遇到了celery无法启动的问题，报错：SyntaxError: invalid syntax ，这是因为我使用的python版本为最新3.7.3 ，而async已经作为关键字而存在了\n\n在 celery 官方的提议下，建议将 async.py 文件的文件名改成 asynchronous。所以我们只需要将 celery\\backends\\async.py 改成 celery\\backends\\asynchronous.py，并且把 celery\\backends\\redis.py 中的所有 async 改成 asynchronous\n\n另外虽然服务起来了，但是服务会不定期的假死\n\n```\n报错：Celery Process 'Worker' exited with 'exitcode 1' [duplicate]\n```\n\n经过搜索可以定位到问题所在，是因为celery依赖库billiard版本过低，导致任务发生了阻塞，所以最好的解决方案就是升级billiard\n\n执行 pip install --upgrade billiard\n\n官方的解释是，billiard最好>=3.5，所以如果不放心的话，还是指定版本号安装比较好\n\n## 使用轮询推送消息遇到的问题\n\n有个需求，有新的产品上架时需要通过发消息提醒用户，但是做这个功能时，前端使用的是轮询的机制，每隔几秒往后台发请求，来确认是否有新产品上架，但是经过测试发现这种方式非常浪费http连接，同时效率很低，严重占用带宽资源，同时浏览器也变得非常卡，内存占用飙升。\n\n对此，我将传统的http请求改成了websocket协议，由后端主动推送数据，前端并不需要判断逻辑，一次连接就可以完成功能，与此同时通过心跳包来保持长连接的稳定，完美的解决了这一问题。\n\n## 使用websocket遇到的问题（重点推荐）\n\n在做消息主动推送功能时，由于用到了websocket长连接，当前端和后端都为一台服务器时，一对一对应，没有出现问题，但是当我为后台做负载均衡时，后台此时大于一台服务器，导致websocket无法对应上长连接的对象，导致服务出问题，经过排查，我将nginx负载均衡的策略由默认的轮询策略改成ip_hash的方式，这样session_id是唯一且一一对应的，解决了这个问题。\n\n## 前端使用富文本编辑器kindeditor跨域上传文件的问题\n\nkindeditor在同域环境中是没有问题的，换句话说，也就是上传接口如果部署在前端页面同一个域名下是没有问题的，然而我的项目的系统架构是前后端分离，前端页面是vue.js服务，后端接口是django服务，分别部署在不同的服务器上，如果在vue.js页面中想要使用kindeditor中的上传文件功能，跨域请求django的接口就会报错。\n\n解决思路就是用重定向方法来伪造成同域环境\n\n我在前端新建一个redirect.html,用来伪造同域获取参数,后台django上传文件之后，不像之前那样返回json数据，而是带着参数直接重定向到做好的redirect.html页面，然后redirect.html页面利用js，再回到kindeditor页面，解决了跨域上传文件无法获取返回值的问题。\n\n## mysql保留关键字问题\n\n数据库查询时,由于表字段含有\"order\"/'comment'等和mysql自身的保留字名称一样;所以一查询就会报错\n\n所以我写了一个装饰器方法，如果侦测到这些保留关键字，就加上反撇号来进行转义比如\n\n```\nselect `order`,`comment`\n```\n\n这样就解决了这个问题\n\n## mysql order by rand() 优化\n\n当时有个需求是从数据库中随机取出一条数据作为展示，我理所当然的使用了 oder by rand(),结果导致慢查询非常多从而锁表影响了效率\n\n查了一下文档\n\nrand()会扫描整个表，然后再随机返回一个记录。对于比较小的表，通常不大于30万行记录的表，这种写法很实用。但是如果一旦记录大于了30万行，这个处理过程就会变得非常缓慢\n\n所以结论就是能不用rand()就不要用\n\n最后给出一种比较实用的替代方法的主要思想：\n\n假设id是主键 首先：SELECT MIN(id), MAX(id) FROM tablename 然后：$id=rand($min,$max); //通过rand返回刚才取到的最大id和最小id之间的一个id号。 最后：SELECT * FROM tablename WHERE id='$id' LIMIT 1\n","tags":["遇见过的问题"]},{"title":"用celery异步发邮件","url":"/2019/12/23/pout/后端技术/发邮件/","content":"\n\n<!-- toc -->\n\n\n##### 首先在项目settings下配置\n\n邮箱找到设置里面POP3/IMAP/SMTP/Exchange/CardDAV/CalDAV服务并开启\n\n```\n# 发邮件配置\nEMAIL_USE_SSL = True\nEMAIL_HOST = 'smtp.qq.com'  # 如果是 163 改成 smtp.163.com\nEMAIL_PORT = 465\nEMAIL_HOST_USER = '814972189@qq.com' # 帐号\nEMAIL_HOST_PASSWORD = 'ymdpxpfwkzbvbbag'  # 授权码\nDEFAULT_FROM_EMAIL = EMAIL_HOST_USER\n```\n\n#### 然后在views下引入\n\n```\nfrom django.core.mail import send_mail\nfrom eduapi.settings import DEFAULT_FROM_EMAIL\n```\n\n### 同步邮件发送并验证\n\n```\n# 连接服务器的memcache\nm = memcache.Client(['47.97.211.28'])\nsubject = '欢迎您'  # 标题\nmessage = '欢迎来到XXX！'       \nsend_mail(subject,message,DEFAULT_FROM_EMAIL,[r_email])\n# 将校验码存入服务器memcache中,并设置180s过期时间\nm.set(r_email,token,180)\n```\n\n### 异步邮件验证\n\n> 异步邮件验证需要使用celery以及django的celery框架\n\n```\npip install celery\npip install django-celery\n```\n\n> celery需要中间任务队列支持，这里使用rabbitmq\n\n#### rabbitmq\n\n> MQ全称为Message Queue, 是一种分布式应用程序的的通信方法\n>\n> 它是消费-生产者模型的一个典型的代表，producer往消息队列中不断写入消息，而另一端consumer则可以读取或者订阅队列中的消息\n>\n> RabbitMQ是MQ产品的典型代表，是一款基于AMQP协议可复用的企业消息系统\n>\n> 业务上，可以实现服务提供者和消费者之间的数据解耦，提供高可用性的消息传输机制，在实际生产中应用相当广泛\n\n- AMQP\n\n> AMQP，即`Advanced Message Queuing Protocol`，一个提供统一消息服务的应用层标准高级**消息队列**协议,是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/**中间件**不同产品，不同的开发语言等条件的限制。**Erlang**中的实现有 [RabbitMQ](https://baike.baidu.com/item/RabbitMQ)等\n\n- rabbitmq架构\n\n> `Rabbitmq`系统最核心的组件是`Exchange`和`Queue`\n>\n> `Exchange`和`Queue`是在`rabbitmq server`（又叫做`broker`）端，`producer`和`consumer`在应用端\n\n![img](http://null/)\n\n> 消息发送端先将消息发送给交换机，交换机再将消息发送到绑定的消息队列\n>\n> 而后每个接收端(consumer)都能从各自的消息队列里接收到信息。\n\n> centos安装办法\n\n```\nyum install rabbitmq-server \n```\n\n- 开启服务\n\n```\nsystemctl restart rabbitmq-server\n```\n\n- 默认rabbitmq的端口为5672，需要在阿里云主机后台开启端口\n- 打开可视化管理工具，默认的rabbitmq的可视化工具已经继承在了rabbitmq中，打开即可，可视化工具的端口为15672\n\n```\nrabbitmq-plugins enable rabbitmq_management\n```\n\n> 接着重启\n\n```\nsystemctl restart rabbitmq-server\n```\n\n- 浏览器中此时访问，已经可以看到效果\n\n```\nhttp://47.97.211.28:15672/\n```\n\n- 默认的账号密码为：guest/guest，需要修改默认密码\n\n```\nrabbitmqctl  change_password  username  newpassword\n```\n\n#### celery\n\n> Celery是基于Python开发的一个分布式任务队列框架，支持使用任务队列的方式在分布的机器/进程/线程上执行任务调度\n\n> Celery的架构，采用典型的生产者-消费者模式\n>\n> 主要由三部分组成：broker（消息队列）、workers（消费者：处理任务）、backend（存储结果）\n>\n> Celery的架构，它采用典型的生产者-消费者模式，主要由三部分组成：broker（消息队列）、workers（消费者：处理任务）、backend（存储结果）\n>\n> 我们只需要将请求所要处理的任务丢入任务队列broker中，由空闲的worker去处理任务即可，处理的结果会暂存在后台数据库backend中。我们可以在一台机器或多台机器上同时起多个worker进程来实现分布式地并行处理任务\n\n- celery-worker可视化工具\n\n```\npip install flower\n```\n\n- 启动flower可以在本地的5555端口查看到当前celery的信息\n\n```\npython manage.py celery flower\n```\n\n- django加入设置中加入djcelery\n\n```\n#settings.py\nINSTALLED_APPS = [\n    ...\n    'djcelery',\n]\n```\n\n- 配置基本连接信息\n\n```\n#settings.py\nimport djcelery\ndjcelery.setup_loader()\nBROKER_URL= 'amqp://guest:woaini21G@123.57.61.168:5672'\n```\n\n- celery与3.7版本兼容问题\n\n> 在 `celery` 官方的提议下，建议将 `async` 文件的文件名改成 `asynchronous`\n>\n> C:\\Python37\\Lib\\site-packages\\kombu\\async\n\n- 需要修改的文件\n\n> C:\\Python37\\Lib\\site-packages\\celery\\utils\\[timer2.py](http://timer2.py/)\n>\n> C:\\Python37\\lib\\site-packages\\celery\\concurrency\\[asynpool.py](http://asynpool.py/)\n>\n> C:\\Python37\\lib\\site-packages\\celery\\worker\\[components.py](http://components.py/)\n>\n> C:\\Python37\\lib\\site-packages\\celery\\worker\\[autoscale.py](http://autoscale.py/)\n>\n> C:\\Python37\\lib\\site-packages\\celery\\worker\\[consumer.py](http://consumer.py/)\n\n- 编写任务代码，在每个app下的tasks.py文件中\n\n> 其中，当djcelery.setup_loader()运行时\n>\n> Celery便会去查看INSTALLD_APPS下包含的所有app目录中的tasks.py文件\n>\n> 找到标记为task的方法，将它们注册为`celery task`\n\n```\n#tasks.py\nfrom django.core.mail import send_mail\nfrom celery import task\nfrom time import sleep\nfrom api_shop.settings import DEFAULT_FROM_EMAIL\n\n@task\ndef send_verify_email(email):\n    subject = '欢迎你'\n    message = '''\n            这是异步邮件的发送\n        '''\n    sleep(10)\n    try:\n        send_mail(subject, message, DEFAULT_FROM_EMAIL, [email])\n    except:\n        pass\n```\n\n- 在视图接口的地方使用\n\n```\nfrom . import tasks\nclass SendVerifyEmail(APIView):\n    def get(self,request):\n        tasks.send_verify_email.delay('295878828@qq.com')\n        return Response(\n            {'code':200}\n        )\n```\n\n- 开启celery\n\n```\npython manage.py celery worker\n```\n\n- 如果出错大概率需要这样，在manage.py文件前头加入这个\n\n```\n#manage.py\nimport django\nimport os\nos.environ['DJANGO_SETTINGS_MODULE'] = 'eduapi.settings'\ndjango.setup()\n```\n","tags":["后端技术"]},{"title":"彻底弄清楚session,cookie,sessionStorage,localStorage的区别及应用场景","url":"/2019/12/23/pout/后端技术/session,cookie,sessionStorage,localStorage/","content":"\n\n<!-- toc -->\n\n# 彻底弄清楚session,cookie,sessionStorage,localStorage的区别及应用场景（面试向）\n\n​    客户端状态保持是一个老生常谈的问题了，归根结底追踪浏览器的用户身份及其相关数据无非就是以下四种方式：session,cookie,sessionStorage,localStorage\n\n​    首先cookie和session:\n\n​    Cookie机制：如果不在浏览器中设置过期时间，cookie被保存在内存中，生命周期随浏览器的关闭而结束，这种cookie简称会话cookie。如果在浏览器中设置了cookie的过期时间，cookie被保存在硬盘中，关闭浏览器后，cookie数据仍然存在，直到过期时间结束才消失。\n    Cookie是服务器发给客户端的特殊信息，cookie是以文本的方式保存在客户端，每次请求时都带上它\n    Session机制：当服务器收到请求需要创建session对象时，首先会检查客户端请求中是否包含sessionid。如果有sessionid，服务器将根据该id返回对应session对象。如果客户端请求中没有sessionid，服务器会创建新的session对象，并把sessionid在本次响应中返回给客户端。通常使用cookie方式存储sessionid到客户端，在交互中浏览器按照规则将sessionid发送给服务器。如果用户禁用cookie，则要使用URL重写，可以通过response.encodeURL(url) 进行实现；API对encodeURL的结束为，当浏览器支持Cookie时，url不做任何处理；当浏览器不支持Cookie的时候，将会重写URL将SessionID拼接到访问地址后。\n    3、存储内容：cookie只能保存字符串类型，以文本的方式；session通过类似与Hashtable的数据结构来保存，能支持任何类型的对象(session中可含有多个对象)\n    4、存储的大小：cookie：单个cookie保存的数据不能超过4kb；session大小没有限制。\n    5、安全性：cookie：针对cookie所存在的攻击：Cookie欺骗，Cookie截获；session的安全性大于cookie。\n    原因如下：\n    （1）sessionID存储在cookie中，若要攻破session首先要攻破cookie；\n    （2）sessionID是要有人登录，或者启动session_start才会有，所以攻破cookie也不一定能得到sessionID；\n    （3）第二次启动session_start后，前一次的sessionID就是失效了，session过期后，sessionID也随之失效。\n    （4）sessionID是加密的\n    （5）综上所述，攻击者必须在短时间内攻破加密的sessionID，并非易事。\n    6、应用场景：\n    cookie：\n    （1）判断用户是否登陆过网站，以便下次登录时能够实现自动登录（或者记住密码）。如果我们删除cookie，则每次登录必须从新填写登录的相关信息。\n    （2）保存上次登录的时间等信息。\n    （3）保存上次查看的页面\n    （4）浏览计数\n    session：Session用于保存每个用户的专用信息，变量的值保存在服务器端，通过SessionID来区分不同的客户。\n    （1）网上商城中的购物车\n    （2）保存用户登录信息\n    （3）将某些数据放入session中，供同一用户的不同页面使用\n    （4）防止用户非法登录\n    7、缺点：cookie：\n    （1）大小受限\n    （2）用户可以操作（禁用）cookie，使功能受限\n    （3）安全性较低\n    （4）有些状态不可能保存在客户端。\n    （5）每次访问都要传送cookie给服务器，浪费带宽。\n    （6）cookie数据有路径（path）的概念，可以限制cookie只属于某个路径下。\n    session：\n    （1）Session保存的东西越多，就越占用服务器内存，对于用户在线人数较多的网站，服务器的内存压力会比较大。\n    （2）依赖于cookie（sessionID保存在cookie），如果禁用cookie，则要使用URL重写，不安全\n    （3）创建Session变量有很大的随意性，可随时调用，不需要开发者做精确地处理，所以，过度使用session变量将会导致代码不可读而且不好维护。\n\n​    说白了，这两种状态保持方式都差强人意，于是webStroage应运而生\n\n​    WebStorage的目的是克服由cookie所带来的一些限制，当数据需要被严格控制在客户端时，不需要持续的将数据发回服务器。\n    WebStorage两个主要目标：（1）提供一种在cookie之外存储会话数据的路径。（2）提供一种存储大量可以跨会话存在的数据的机制。\n    HTML5的WebStorage提供了两种API：localStorage（本地存储）和sessionStorage（会话存储）。\n    1、生命周期：localStorage:localStorage的生命周期是永久的，关闭页面或浏览器之后localStorage中的数据也不会消失。localStorage除非主动删除数据，否则数据永远不会消失。\n    sessionStorage的生命周期是在仅在当前会话下有效。sessionStorage引入了一个“浏览器窗口”的概念，sessionStorage是在同源的窗口中始终存在的数据。只要这个浏览器窗口没有关闭，即使刷新页面或者进入同源另一个页面，数据依然存在。但是sessionStorage在关闭了浏览器窗口后就会被销毁。同时独立的打开同一个窗口同一个页面，sessionStorage也是不一样的。\n    2、存储大小：localStorage和sessionStorage的存储数据大小一般都是：5MB\n    3、存储位置：localStorage和sessionStorage都保存在客户端，不与服务器进行交互通信。\n    4、存储内容类型：localStorage和sessionStorage只能存储字符串类型，对于复杂的对象可以使用ECMAScript提供的JSON对象的stringify和parse来处理\n    5、获取方式：localStorage：window.localStorage;；sessionStorage：window.sessionStorage;。\n    6、应用场景：localStoragese：常用于长期登录（+判断用户是否已登录），适合长期保存在本地的数据（令牌）。sessionStorage：敏感账号一次性登录；\n    WebStorage的优点：\n    （1）存储空间更大：cookie为4KB，而WebStorage是5MB；\n    （2）节省网络流量：WebStorage不会传送到服务器，存储在本地的数据可以直接获取，也不会像cookie一样美词请求都会传送到服务器，所以减少了客户端和服务器端的交互，节省了网络流量；\n    （3）对于那种只需要在用户浏览一组页面期间保存而关闭浏览器后就可以丢弃的数据，sessionStorage会非常方便；\n    （4）快速显示：有的数据存储在WebStorage上，再加上浏览器本身的缓存。获取数据时可以从本地获取会比从服务器端获取快得多，所以速度更快；\n    （5）安全性：WebStorage不会随着HTTP header发送到服务器端，所以安全性相对于cookie来说比较高一些，不会担心截获，但是仍然存在伪造问题；\n    （6）WebStorage提供了一些方法，数据操作比cookie方便；\n            setItem (key, value) —— 保存数据，以键值对的方式储存信息。\n","tags":["后端技术"]},{"title":"WebSocket","url":"/2019/12/23/pout/后端技术/websocker/","content":"\n\n<!-- toc -->\n\n# WebSocket心跳检测和重连机制\n\n## 1. 心跳重连原由\n\n心跳和重连的目的用一句话概括就是客户端和服务端保证彼此还活着，避免丢包发生。\n\n`websocket`连接断开有以下两证情况：\n\n### 前端断开\n\n在使用`websocket`过程中，可能会出现网络断开的情况，比如信号不好，或者网络临时关闭，这时候websocket的连接已经断开，而不同浏览器有不同的机制，触发`onclose`的时机也不同，并不会理想执行`websocket`的`onclose`方法，我们无法知道是否断开连接，也就无法进行重连操作。\n\n### 后端断开\n\n如果后端因为一些情况需要断开ws，在可控情况下，会下发一个断连的消息通知，之后才会断开，我们便会重连。\n如果因为一些异常断开了连接，我们是不会感应到的，所以如果我们发送了心跳一定时间之后，后端既没有返回心跳响应消息，前端又没有收到任何其他消息的话，我们就能断定后端主动断开了。\n\n因此需要一种机制来检测客户端和服务端是否处于正常连接的状态。通过在指定时间间隔发送心跳包来保证连接正常，如果连接出现问题，就需要手动触发`onclose`事件，这时候便可进行重连操作。因此`websocket`心跳重连就应运而生。\n\n## 2. 心跳重连的简单实现\n\n2.1 通过createWebSocket创建连接\n\n2.2 创建init方法，初始化一些监听事件，如果希望websocket连接一直保持, 我们会在close或者error上绑定重新连接方法。\n\n \n\n　2.3 重连操作，通过设置`lockReconnect`变量避免重复连接\n\n　2.4 心跳检测\n\n　　有的时候，客户端发送3次心跳包服务端均未回复才判定为失去连接，所以这时需要加上计数来判断。\n\n\n\n\n\n# **最后总结下 **\n\n**我们确认了后端单台服务器的处理能力有限，因此。我们需要做集群。其次我们为了不让前端关闭或回收，后端不响应。我们需要设置心跳，定时清除无关的连接。**\n**最后，我们需要有消息确认机制，做到保证消息的100%接收。**\n","tags":["后端技术"]},{"title":"具体如何优化前端性能的总结","url":"/2019/12/23/pout/前端技术/具体如何优化前端性能的总结/","content":"\n\n<!-- toc -->\n\n# 具体谈谈如何优化前端性能的总结\n\n[首页](https://v3u.cn/) - [Web Design](https://v3u.cn/l_id_2) /2019-05-30\n\n​    前端是庞杂的，包括 HTML、 CSS、 Javascript、Image 、Video等等各种各样的资源。前端优化是复杂的，针对方方面面的资源都有不同的方式。那么，前端优化的目的是什么 ?\n\n​    从用户角度而言，优化能够让页面加载得更快、对用户的操作响应得更及时，能够给用户提供更为友好的体验。  \n\n​    从服务商角度而言，优化能够减少页面请求数、或者减小请求所占带宽，能够节省可观的资源。  \n\n​    总之，说白了，恰当的优化不仅能够改善站点的用户体验并且能够节省相当的资源利用，就是又让用户用的爽，又省了钱。\n\n​    1.使用cdn加速，网站上静态资源即css、js全都使用cdn分发，图片亦然。具体来说，CDN就是采用更多的缓存服务器（CDN边缘节点），布放在用户访问相对集中的地区或网络中。当用户访问网站时，利用全局负载技术，将用户的访问指向距离最近的缓存服务器上，由缓存服务器响应用户请求\n\n​    ![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190530/20190530065911_52568.gif)\n\n​    目前国内比较靠谱的cdn服务商个人推荐七牛云 qiniu.com,他们家的cdn服务每个月有10g是免费的，配置也很简单，直接申请即可\n\n​    2.使用Gzip压缩网页\n\n​    Gzip压缩可以让你的页面体积变小，加快访问速度，使用nginx服务器可以简单的开启gzip压缩\n\n​    修改nginx配置文件 vim /etc/nginx/conf.d/default.conf \n\n​    \n\n```\nserver {\n     listen   80;\n     server_name  localhost;\n\n     access_log /root/js_front_access.log;\n     error_log /root/js_front_error.log;\n\n\n     client_max_body_size 75M;\n\n\n     location / {\n\n        root /root/public;\n        index index.html;\n        try_files $uri $uri/ /index.html;\n\n    }\n\n   #开启gzip压缩\n   location ~ .*.(jpg|gif|png|bmp)$ {\n                gzip on;\n                gzip_http_version 1.1;\n                gzip_comp_level 3;\n                gzip_types text/plain application/json application/x-javascript application/css application/xml application/xml+rss text/javascript application/x-httpd-php image/jpeg image/gif image/png image/x-ms-bmp;\n                }\n\n\n\n    error_log    /root/js_front/error.log    error;\n\n}\n```\n\n重启nginx \n\nsystemctl restart nginx.service\n\n​    如何判断是否开启成功呢？可以使用站长工具进行检测 <http://tool.chinaz.com/Gzips>\n\n​    ![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190530/20190530071716_74613.png)\n\n​    压缩比例非常惊人\n\n​    3 减少 HTTP请求数，如果可以的话，尽可能的将外部的脚本、样式进行合并，多个合为一个。另外， CSS、 Javascript、Image 都可以用相应的工具进行压缩，压缩后往往能省下不少空间，如何压缩以及合并外部脚本和样式请参照这篇文章 [利用grunt插件来压缩js和css文件用来减少http请求，提高页面效率](https://v3u.cn/a_id_38)\n\n​    4 避免空的src和href\n        当link标签的href属性为空、script标签的src属性为空的时候，浏览器渲染的时候会把当前页面的URL作为它们的属性值，从而把页面的内容加载进来作为它们的值。所以要避免犯这样的疏忽。\n\n​    5 把CSS放到顶部\n    网页上的资源加载时从上网下顺序加载的，所以css放在页面的顶部能够优先渲染页面，让用户感觉页面加载很快。\n    6 把JS放到底部\n    加载js时会对后续的资源造成阻塞，必须得等js加载完才去加载后续的文件 ，所以就把js放在页面底部最后加载。\n\n​    \n\n​    7 可缓存的AJAX\n\n​    异步请求同样的造成用户等待，所以使用ajax请求时，要主动告诉浏览器如果该请求有缓存就去请求缓存内容。如下代码片段, cache:true就是显式的要求如果当前请求有缓存的话，直接使用缓存\n\n​    \n\n```\n$.ajax(\n{\nurl : 'url',\n      dataType : \"json\",\n      cache: true,\n      success : function(son, status){\n}\n```\n\n​    8 减少作用域链查找，这一点在循环中是尤其需要注意的问题。如果在循环中需要访问非本作用域下的变量时请在遍历之前用局部变量缓存该变量，并在遍历结束后再重写那个变量，这一点对全局变量尤其重要，因为全局变量处于作用域链的最顶端，访问时的查找次数是最多的。\n\n```\n低效率的写法：\n\n// 全局变量 \nvar globalVar = 1; \nfunction myCallback(info){    \nfor( var i = 100000; i--;){       \n//每次访问 globalVar 都需要查找到作用域链最顶端，本例中需要访问 100000 次       \nglobalVar += i;    \n}\n}   \n\n更高效的写法：\n\n// 全局变量 \nvar globalVar = 1; \nfunction myCallback(info){    \n//局部变量缓存全局变量     \nvar localVar = globalVar;    \nfor( var i = 100000; i--;){       \n//访问局部变量是最快的        \nlocalVar += i;    }\n}\n```\n\n \n\n​    9 生成纯静态页，也就是把动态内容事先生成好，这样在前端就避免请求后端数据，加快了页面访问速度\n\n​    经过上面的几点优化之后，我们可以使用google的页面性能打分工具PageSpeedInsights对网站进行评测，由于众所周知的原因，使用google的产品需要科学上网   <https://developers.google.com/speed/pagespeed/insights/>\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190530/20190530073128_79609.png)\n\n​    可以看到，经过一番页面性能优化，访问速度得到了提升，带宽节约了不少，前端优化任重而道远\n\n随机文章[Python 的 MySQLdb 模块的 autocommit](https://v3u.cn/a_id_5)2012-05-20[关于mysql表引擎的问题](https://v3u.cn/a_id_21)2016-09-10[在阿里云Centos7.6上部署Supervisor来监控和操作各类服务](https://v3u.cn/a_id_76)2019-05-17[使用Redisearch实现的全文检索功能服务](https://v3u.cn/a_id_105)2019-08-29[在Mac和Linux以及Windows下删除pip安装包缓存用以节约空间](https://v3u.cn/a_id_70)2019-05-03[嘿，五年了](https://v3u.cn/a_id_34)2016-07-28[在阿里云Centos上配置nginx+uwsgi+负载均衡配置](https://v3u.cn/a_id_77)2019-05-17[Mac 上安装 Scrapy 报错，Operation not permitted](https://v3u.cn/a_id_30)2016-10-23[在Mac上用手机抓包软件Charles抓取微信小程序中的高清无水印视频](https://v3u.cn/a_id_118)2019-10-29[在阿里云Centos7.6上利用docker搭建Jenkins来自动化部署Django项目](https://v3u.cn/a_id_84)2019-05-28[在mac上配置rails开发环境](https://v3u.cn/a_id_40)2015-06-19[使用Docker-compose来封装celery4.1+rabbitmq3.7服务，实现微服务架构](https://v3u.cn/a_id_115)2019-09-28[关于mysql联合索引的最左前缀原则以及b+tree](https://v3u.cn/a_id_91)2019-06-10[Home Brew 常用命令总结](https://v3u.cn/a_id_52)2019-03-16[在Mac下使用MindMaster画思维导图(脑图)](https://v3u.cn/a_id_90)2019-06-06\n","tags":["前端技术"]},{"title":"上传图片","url":"/2019/12/23/pout/后端技术/上传图片/","content":"\n\n<!-- toc -->\n### 上传图片\n1. 在配置文件中，设置图片保存路径（建议放在static目录下）\n    + > STATICFILES_DIRS = [os.path.join(BASE_DIR, 'static')]\n2. 表单支持图片上传\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>图片上传</title>\n</head>\n<body>\n<form action=\"\" method=\"post\" enctype=\"multipart/form-data\">\n    <input type=\"file\" name=\"img\">\n    <br>\n    <button type=\"submit\">上传</button>\n</form>\n</body>\n</html>\n```\n3. 定义视图函数，并绑定一个路由地址\n4. 视图函数中，保存图片\n```\nclass UploadView(View):\n    \"\"\"上传图片\"\"\"\n    def get(self, request):\n        \"\"\"显示表单页面\"\"\"\n        # goods = models.Goods.objects.filter(id=3).first()\n        goods_list = models.Goods.objects.all()\n        return render(request, 'upload.html', {'goods_list': goods_list})\n\n    def post(self, request):\n        \"\"\"处理表单提交动作的\"\"\"\n        # 1、从页面请求中获取图片文件\n        image_file = request.FILES.get('img')\n        if not image_file:\n            return render(request, 'upload.html', {'errormsg': '请选择图片'})\n\n        # 2、生成图片的名称（防止新图片，与服务器中现有的图片重名）\n        # 图片文件名称的格式 ： 年月日时分秒毫秒+用户ID\n        image_name = datetime.now().strftime('%Y%m%d%H%M%S%f') + '10' + image_file.name\n\n        # 3、拼写出，图片存储的完整路径\n        image_path = os.path.join(settings.STATICFILES_DIRS[0], image_name)\n\n        # 4、将图片保存到服务器\n        f = open(image_path, 'wb')\n        for chunk in image_file.chunks():\n            f.write(chunk)\n\n        # 5、关闭文件\n        f.close()\n\n        # 6、将图片的相对路径，保存到 商品表的image 字段中\n        goods = models.Goods()\n        # goods.img = '/static/' + image_name\n        goods.img = settings.LOADFILES + image_name\n        goods.save()\n\n        return HttpResponse('保存成功')\n```\n5、展示图片\n```html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>图片上传</title>\n</head>\n<body>\n{{ errormsg }}\n<br>\n<form action=\"\" method=\"post\" enctype=\"multipart/form-data\">\n    {% csrf_token %}\n    <input type=\"file\" name=\"img\">\n    <br>\n    <button type=\"submit\">上传</button>\n</form>\n\n{% for goods in goods_list %}\n    <img src=\"{{ goods.img }}\" alt=\"图片\">\n    <br>\n{% endfor %}\n</body>\n</html>\n```\n","tags":["图片"],"categories":["图片"]},{"title":"Jenkins部署流程","url":"/2019/12/20/pout/运维/Jenkins部署流程/","content":"\n\n<!-- toc -->\n\n\n# 在阿里云Centos7.6上利用docker搭建Jenkins来自动化部署Django项目 \n\n \n\n​    需求分析—原型设计—开发代码—内网部署-提交测试—确认上线—备份数据—外网更新-最终测试，如果发现外网部署的代码有异常，需要及时回滚。\n\n​    整个过程相当复杂而漫长，其中还需要输入不少的命令，比如上传代码，git的拉取或者合并分支等等。\n\n​    Jenkins是目前非常流行的一款持续集成工具，可以帮助大家把更新后的代码自动部署到服务器上运行，整个流程非常自动化，你可以理解为部署命令操作的可视化界面。\n\n​    \n\n​    Jenkins主要有三种安装方式\n    下载官方war包，放到tomcat中直接运行。\n    yum安装。\n    使用官方docker镜像。\n\n​    \n\n​    毫无疑问，既然有docker这么简单方便的工具，就没必要选择前两种复杂的安装方式了。\n\n​    首先安装docker\n\n​    \n\n```\ncentos 安装docker\n1 docker 要求 CentOS 系统的内核版本高于 3.10 ，查看本页面的前提条件来验证你的CentOS 版本是否支持 Docker \n2、使用 root 权限登录 Centos。确保 yum 包更新到最新。\nsudo yum update\n3、卸载旧版本(如果安装过旧版本的话)\nsudo yum remove docker  docker-common docker-selinux docker-engine\n4、安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n5、设置yum源\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n6、可以查看所有仓库中所有docker版本，并选择特定版本安装\n yum list docker-ce --showduplicates | sort -r\n7、安装docker\nsudo yum install docker-ce \n8、启动并加入开机启动\nsudo systemctl start docker\nsudo systemctl enable docker\n9、验证安装是否成功(有client和service两部分表示docker安装启动都成功了)\ndocker version\n```\n\n然后下载jenkins官方docker镜像\n\n```\ndocker pull jenkins/jenkins\n```\n\n查看镜像 docker images\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528100231_11283.png)\n\n在主机上创建目录，并添加读写权限以便jenkins应用运行时读写文件\n\n```\nmkdir /root/j_node\nchmod 777 /root/j_node\n```\n\n后台将镜像以容器的形式起服务，对端口映射，同时把刚刚建立的目录挂载到容器中\n\n```\ndocker run -d --name jenkins -p 8081:8080 -p 50000:50000 -v /root/j_node:/var/jenkins_home jenkins/jenkins\n```\n\n这里注意，如果是阿里云的话，安全策略需要暴露8081端口\n\n通过网址访问 http://你的ip:8081\n\n然后通过命令获取安装秘钥\n\n```\ndocker logs jenkins\n```\n\n \n\n \n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528101043_90703.png)\n\n完毕后，根据提示设置登陆账户\n\n然后新建一个项目，在源代码控制那一栏，输入你的项目的线上git仓库地址，注意默认应该是master分支，因为生产环境部署的代码必须是主分支\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528101449_38616.png)\n\n保存后，点击Build Now进行部署，jenkins会自动去git版本库中抽取最新的master分支进行部署，同时每部署一次的历史记录都会被保存下来\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528101737_28436.png)\n\n此时，进入/root/j_node 目录下 发现项目已经部署在了workspace目录下\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528101900_12377.png)\n\n整个过程非常简单，每次上线之前，项目经理只需要检查各个组员的代码，然后统一合并到主分支master，最后进入jenkins点击部署按钮即可，节约了不少时间。\n\n","tags":["运维"]},{"title":"centos7彻底卸载mysql和通过yum安装mysql.md","url":"/2019/09/09/pout/Docker/centos7彻底卸载mysql和通过yum安装mysql/","content":"\n\n<!-- toc -->\n\n# 彻底卸载mysql\n\n- **查看是否有安装的mysql** \n\n  ```\n  rpm -qa | grep -i mysql // 查看命令1\n  yum list install mysql* // 查看命令2         二选一查看\n  ```\n\n- **卸载mysql安装包** \n\n  ```\n  yum remove mysql mysql-server mysql-libs compat-mysql51\n  yum remove mysql-community-release\n  rpm -e --nodeps mysql-community-libs-5.7.22-1.el7.x86_64\n  rpm -e –nodeps mysql57-community-release-el7-11.noarch\n  \n  \n  总之删到通过上面两种命令查不出来任何有关mysql的东西。\n  ```\n\n- **删除残留的mysql目录或文件：** \n\n  ```\n  查询mysql安装目录\n  whereis mysql  或   find / -name mysql\n  \n  删除查询出的目录\n  rm -rf /usr/lib64/mysql\n  rm -rf /usr/share/mysql\n  rm -rf /usr/bin/mysql\n  rm -rf /etc/logrotate.d/mysql\n  rm -rf /var/lib/mysql\n  rm -rf /var/lib/mysql/mysql\n  \n  总之删到通过上面两种命令查不出来任何有关mysql的东西。\n  ```\n\n- 继续删除\n\n  ```\n  删除mysql 配置文件\n  \n  rm –rf /usr/my.cnf\n  rm -rf /root/.mysql_sercret\n  \n  删除mysql开机自启动服务\n  chkconfig --list | grep -i mysql\n  chkconfig --del mysqld  // 服务名为你设置时候自己设置的名字\n  ```\n\n- 至此就卸载干净了 \n\n# 安装mysql\n\n- **下载并安装mysql的YUM源：** \n\n  ```\n  选择一个目录下载并安装\n  \n  mkdir soft\n  cd soft\n  \n  wget http://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm // 下载mysql yum源\n  rpm -ivh mysql57-community-release-el7-11.noarch.rpm // 安装yum源\n  ```\n\n- **接下在就是正式安装mysql了** \n\n  ```\n  yum install mysql-community-server\n  ```\n\n- **启动mysql** \n\n  ```\n  service mysqld start\n  \n  如果出现以下错误：\n  ERROR 1045 (28000): Access denied for user ‘root’@’localhost’ (using password: NO)\n  \n  首先停止mysql服务\n  service mysqld stop\n  \n  再以不检查权限的方式启动\n  mysqld --skip-grant-tables &\n  \n  又出现以下错误：\n  [ERROR] Fatal error: Please read “Security” section of the manual to find out how to run mysqld as root!\n  \n  执行命令以root权限启动\n  mysqld --user=root --skip-grant-tables &\n  ```\n\n- **如果没有报错，登录 mysql** ：\n\n  ```\n  mysql –uroot \n  如果报错试一下\n  mysql –uroot -p直接回车\n  ```\n\n- **设置密码** \n\n  ```\n  UPDATE mysql.user SET authentication_string=PASSWORD('密码') where USER='root';\n  ALTER USER 'root'@'localhost' IDENTIFIED BY '密码';\n  SET PASSWORD FOR root=PASSWORD('密码');\n  \n  flush privileges; // 刷新设置立即生效\n  \n  exit  // 退出,或者使用 quit 命令\n  ```\n\n- 再次进入\n\n  ```\n  mysql -uroot –p // 会提示输入密码\n  输入密码，成功则密码设置完成了\n  ```\n\n- **设置root权限的远程访问** \n\n  ```\n  mysql -u root -pvmware\n  mysql>use mysql;\n  mysql>update user set host = '%' where user = 'root';\n  mysql>flush privileges;\n  mysql>select host, user from user\n  \n  然后就可以通过navicat(或者其他工具)远程连接了\n  ```\n\n  \n\n","tags":["Docker"]},{"title":"八大查找","url":"/2019/08/20/pout/算法/八大查找/","content":"\n\n<!-- toc -->\n\n# 顺序查找\n\n算法简介 顺序查找又称为线性查找，是一种最简单的查找方法。适用于线性表的顺序存储结构和链式存储结构。该算法的时间复杂度为O(n)。 基本思路 从第一个元素m开始逐个与需要查找的元素x进行比较，当比较到元素值相同(即m=x)时返回元素m的下标，如果比较到最后都没有找到，则返回-1。 优缺点 缺点：是当n 很大时，平均查找长度较大，效率低； 优点：是对表中数据元素的存储没有要求。另外，对于线性链表，只能进行顺序查找。 算法实现\n\n```\ndef sequential_search(lis, key):\n  length = len(lis)\n  for i in range(length):\n    if lis[i] == key:\n      return i\n    else:\n      return False\n```\n\n# 折半查找\n\n二分查找（Binary Search），是一种在有序数组中查找某一特定元素的查找算法。查找过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则查找过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。 这种查找算法每一次比较都使查找范围缩小一半。\n\n算法描述 给予一个包含 个带值元素的数组A 1、 令 L为0 ， R为 n-1 2、 如果L>R，则搜索以失败告终 3、 令 m (中间值元素)为 ⌊(L+R)/2⌋ 4、 如果 AmT，令 R为 m - 1 并回到步骤二 复杂度分析 时间复杂度：折半搜索每次把搜索区域减少一半，时间复杂度为 O(logn) 空间复杂度：O(1)\n\n```\ndef binary_search(lis, key):\n  low = 0\n  high = len(lis) - 1\n  time = 0\n  while low < high:\n    time += 1\n    mid = int((low + high) / 2)\n    if key < lis[mid]:\n      high = mid - 1\n    elif key > lis[mid]:\n      low = mid + 1\n    else:\n      # 打印折半的次数\n      print(\"times: %s\" % time)\n      return mid\n  print(\"times: %s\" % time)\n  return False\n```\n\n# 插值查找\n\n算法简介\n\n插值查找是根据要查找的关键字key与查找表中最大最小记录的关键字比较后的 查找方法，其核心就在于插值的计算公式 (key-a[low])/(a[high]-a[low])*(high-low)。 时间复杂度o(logn)但对于表长较大而关键字分布比较均匀的查找表来说，效率较高。\n\n算法思想 基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，差值查找也属于有序查找。 注：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。\n\n复杂度分析 时间复杂性：如果元素均匀分布，则O（log log n）），在最坏的情况下可能需要O（n）。 空间复杂度：O（1）。\n\n```\ndef binary_search(lis, key):\n  low = 0\n  high = len(lis) - 1\n  time = 0\n  while low < high:\n    time += 1\n    # 计算mid值是插值算法的核心代码\n    mid = low + int((high - low) * (key - lis[low])/(lis[high] - lis[low]))\n    print(\"mid=%s, low=%s, high=%s\" % (mid, low, high))\n    if key < lis[mid]:\n      high = mid - 1\n    elif key > lis[mid]:\n      low = mid + 1\n    else:\n      # 打印查找的次数\n      print(\"times: %s\" % time)\n      return mid\n  print(\"times: %s\" % time)\n  return False\n\nif __name__ == '__main__':\n  LIST = [1, 5, 7, 8, 22, 54, 99, 123, 200, 222, 444]\n  result = binary_search(LIST, 444)\n  print(result)\n```\n","tags":["算法"]},{"title":"爬虫常用库及相关知识","url":"/2019/07/23/pout/爬虫/爬虫/","content":"\n\n<!-- toc -->\n\n\n## 爬虫常用库\n\nrequests、selenium、puppeteer，beautifulsoup4、pyquery、pymysql、pymongo、redis、lxml和scrapy框架\n\n其中发起请求课可以使用requests和scrapy\n\n解析内容可以用 beautifulsoup4,lxml,pyquery\n\n存储内容可以使用 mysql(清洗后的数据) redis(代理池) mongodb(未清洗的数据)\n\n抓取动态渲染的内容可以使用:selenium,puppeteer\n\n## 增量爬虫\n\n一个网站，本来一共有10页，过段时间之后变成了100页。假设，已经爬取了前10页，为了增量爬取，我们现在只想爬取第11-100页。\n\n因此，为了增量爬取，我们需要将前10页请求的指纹保存下来。以下命令是将内存中的set里指纹保存到本地硬盘的一种方式。\n\n```\nscrapy crawl somespider -s JOBDIR=crawls/somespider-1\n```\n\n但还有更常用的，是将scrapy中的指纹存在一个redis数据库中，这个操作已经有造好轮子了，即scrapy-redis库。\n\nscrapy-redis库将指纹保存在了redis数据库中，是可以持久保存的。（基于此，还可以实现分布式爬虫，那是另外一个用途了）scrapy-redis库不仅存储了已请求的指纹，还存储了带爬取的请求，这样无论这个爬虫如何重启，每次scrapy从redis中读取要爬取的队列，将爬取后的指纹存在redis中。如果要爬取的页面的指纹在redis中就忽略，不在就爬取。\n\n## Scrapy 相关\n\nscrapy基于twisted异步IO框架，downloader是多线程的。\n\n但是，由于python使用GIL（全局解释器锁，保证同时只有一个线程在使用解释器），这极大限制了并行性，在处理运算密集型程序的时候，Python的多线程效果很差，而如果开多个线程进行耗时的IO操作时，Python的多线程才能发挥出更大的作用。（因为Python在进行长时IO操作时会释放GIL） 所以简单的说，scrapy是多线程的，不需要再设置了，由于目前版本python的特性，多线程地不是很完全，但实际测试scrapy效率还可以。\n\nrequests 是一个基本库，目前只能用来发送http请求，所以涉及爬虫的多线程或者协程需要自己定制编写\n\n## Scrapy整体架构\n\n• 引擎(Scrapy Engine)，用来处理整个系统的数据流处理，触发事务 。\n\n• 调度器(Scheduler)，用来接受引擎发过来的请求，压入队列中，并在引擎再次请求的时候返回。\n\n• 下载器(Downloader)，用于下载网页内容，并将网页内容返回给蜘蛛。\n\n• 蜘蛛(Spiders)，蜘蛛是主要干活的，用它来制订特定域名或网页的解析规则。编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。每个spider负责处理一个特定(或一些)网站。\n\n• 项目管道(ItemPipeline)，负责处理有蜘蛛从网页中抽取的项目，他的主要任务是清晰、验证和存储数据。当页面被蜘蛛解析后，将被发送到项目管道，并经过几个特定的次序处理数据。\n\n• 下载器中间件(DownloaderMiddlewares)，位于Scrapy引擎和下载器之间的钩子框架，主要是处理Scrapy引擎与下载器之间的请求及响应。\n\n• 蜘蛛中间件(SpiderMiddlewares)，介于Scrapy引擎和蜘蛛之间的钩子框架，主要工作是处理蜘蛛的响应输入和请求输出。\n\n• 调度中间件(SchedulerMiddlewares)，介于Scrapy引擎和调度之间的中间件，从Scrapy引擎发送到调度的请求和响应。\n\n![img](../img/spider1.png)\n\n爬取流程：上图绿线是数据流向，\n\n首先从初始URL开始，Scheduler会将其交给Downloader进行下载，下载之后会交给Spider进行分析，\n\nSpider分析出来的结果有两种：\n\n一种是需要进一步抓取的链接，例如之前分析的“下一页”的链接，这些东西会被传回Scheduler；\n\n另一种是需要保存的数据，它们则被送到Item Pipeline那里，那是对数据进行后期处理（详细分析、过滤、存储等）的地方。\n\n另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。\n\n## 数据流（流程，类似抓取任务生命周期）\n\nScrapy中的数据流由执行引擎控制，其过程如下:\n\n1.引擎打开一个网站(open adomain)，找到处理该网站的Spider并向该spider请求第一个要爬取的URL(s)。\n\n2.引擎从Spider中获取到第一个要爬取的URL并在调度器(Scheduler)以Request调度。\n\n3.引擎向调度器请求下一个要爬取的URL。\n\n4.调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件(请求(request)方向)转发给下载器(Downloader)。\n\n5.一旦页面下载完毕，下载器生成一个该页面的Response，并将其通过下载中间件(返回(response)方向)发送给引擎。\n\n6.引擎从下载器中接收到Response并通过Spider中间件(输入方向)发送给Spider处理。\n\n7.Spider处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。\n\n8.引擎将(Spider返回的)爬取到的Item给ItemPipeline，将(Spider返回的)Request给调度器。\n\n9.(从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。\n\n### 安装\n\n```\npip install Scrapy\n```\n\n缺少twisted装不上的直接去网上下载动态库：<https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted>\n\n新建项目\n\n```\nscrapy startproject 'project_name'\n```\n\nscrapy 配置文件\n\n```\n#==>第一部分：基本配置<===\n#1、项目名称，默认的USER_AGENT由它来构成，也作为日志记录的日志名\nBOT_NAME = 'Amazon'\n\n#2、爬虫应用路径\nSPIDER_MODULES = ['Amazon.spiders']\nNEWSPIDER_MODULE = 'Amazon.spiders'\n\n#3、客户端User-Agent请求头\n#USER_AGENT = 'Amazon (+http://www.yourdomain.com)'\n\n#4、是否遵循爬虫协议\n# Obey robots.txt rules\nROBOTSTXT_OBEY = False\n\n#5、是否支持cookie，cookiejar进行操作cookie，默认开启\n#COOKIES_ENABLED = False\n\n#6、Telnet用于查看当前爬虫的信息，操作爬虫等...使用telnet ip port ，然后通过命令操作\n#TELNETCONSOLE_ENABLED = False\n#TELNETCONSOLE_HOST = '127.0.0.1'\n#TELNETCONSOLE_PORT = [6023,]\n\n#7、Scrapy发送HTTP请求默认使用的请求头\n#DEFAULT_REQUEST_HEADERS = {\n#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n#   'Accept-Language': 'en',\n#}\n\n\n\n#===>第二部分：并发与延迟<===\n#1、下载器总共最大处理的并发请求数,默认值16\n#CONCURRENT_REQUESTS = 32\n\n#2、每个域名能够被执行的最大并发请求数目，默认值8\n#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n\n#3、能够被单个IP处理的并发请求数，默认值0，代表无限制，需要注意两点\n#I、如果不为零，那CONCURRENT_REQUESTS_PER_DOMAIN将被忽略，即并发数的限制是按照每个IP来计算，而不是每个域名\n#II、该设置也影响DOWNLOAD_DELAY，如果该值不为零，那么DOWNLOAD_DELAY下载延迟是限制每个IP而不是每个域\n#CONCURRENT_REQUESTS_PER_IP = 16\n\n#4、如果没有开启智能限速，这个值就代表一个规定死的值，代表对同一网址延迟请求的秒数\n#DOWNLOAD_DELAY = 3\n\n\n\n#===>第三部分：智能限速/自动节流：AutoThrottle extension<===\n#一：介绍\nfrom scrapy.contrib.throttle import AutoThrottle #http://scrapy.readthedocs.io/en/latest/topics/autothrottle.html#topics-autothrottle\n设置目标：\n1、比使用默认的下载延迟对站点更好\n2、自动调整scrapy到最佳的爬取速度，所以用户无需自己调整下载延迟到最佳状态。用户只需要定义允许最大并发的请求，剩下的事情由该扩展组件自动完成\n\n\n#二：如何实现？\n在Scrapy中，下载延迟是通过计算建立TCP连接到接收到HTTP包头(header)之间的时间来测量的。\n注意，由于Scrapy可能在忙着处理spider的回调函数或者无法下载，因此在合作的多任务环境下准确测量这些延迟是十分苦难的。 不过，这些延迟仍然是对Scrapy(甚至是服务器)繁忙程度的合理测量，而这扩展就是以此为前提进行编写的。\n\n\n#三：限速算法\n自动限速算法基于以下规则调整下载延迟\n#1、spiders开始时的下载延迟是基于AUTOTHROTTLE_START_DELAY的值\n#2、当收到一个response，对目标站点的下载延迟=收到响应的延迟时间/AUTOTHROTTLE_TARGET_CONCURRENCY\n#3、下一次请求的下载延迟就被设置成：对目标站点下载延迟时间和过去的下载延迟时间的平均值\n#4、没有达到200个response则不允许降低延迟\n#5、下载延迟不能变的比DOWNLOAD_DELAY更低或者比AUTOTHROTTLE_MAX_DELAY更高\n\n#四：配置使用\n#开启True，默认False\nAUTOTHROTTLE_ENABLED = True\n#起始的延迟\nAUTOTHROTTLE_START_DELAY = 5\n#最小延迟\nDOWNLOAD_DELAY = 3\n#最大延迟\nAUTOTHROTTLE_MAX_DELAY = 10\n#每秒并发请求数的平均值，不能高于 CONCURRENT_REQUESTS_PER_DOMAIN或CONCURRENT_REQUESTS_PER_IP，调高了则吞吐量增大强奸目标站点，调低了则对目标站点更加”礼貌“\n#每个特定的时间点，scrapy并发请求的数目都可能高于或低于该值，这是爬虫视图达到的建议值而不是硬限制\nAUTOTHROTTLE_TARGET_CONCURRENCY = 16.0\n#调试\nAUTOTHROTTLE_DEBUG = True\nCONCURRENT_REQUESTS_PER_DOMAIN = 16\nCONCURRENT_REQUESTS_PER_IP = 16\n\n\n\n#===>第四部分：爬取深度与爬取方式<===\n#1、爬虫允许的最大深度，可以通过meta查看当前深度；0表示无深度\n# DEPTH_LIMIT = 3\n\n#2、爬取时，0表示深度优先Lifo(默认)；1表示广度优先FiFo\n\n# 后进先出，深度优先\n# DEPTH_PRIORITY = 0\n# SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleLifoDiskQueue'\n# SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.LifoMemoryQueue'\n# 先进先出，广度优先\n\n# DEPTH_PRIORITY = 1\n# SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'\n# SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'\n\n\n#3、调度器队列\n# SCHEDULER = 'scrapy.core.scheduler.Scheduler'\n# from scrapy.core.scheduler import Scheduler\n\n#4、访问URL去重\n# DUPEFILTER_CLASS = 'step8_king.duplication.RepeatUrl'\n\n\n\n#===>第五部分：中间件、Pipelines、扩展<===\n#1、Enable or disable spider middlewares\n# See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html\n#SPIDER_MIDDLEWARES = {\n#    'Amazon.middlewares.AmazonSpiderMiddleware': 543,\n#}\n\n#2、Enable or disable downloader middlewares\n# See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html\nDOWNLOADER_MIDDLEWARES = {\n   # 'Amazon.middlewares.DownMiddleware1': 543,\n}\n\n#3、Enable or disable extensions\n# See http://scrapy.readthedocs.org/en/latest/topics/extensions.html\n#EXTENSIONS = {\n#    'scrapy.extensions.telnet.TelnetConsole': None,\n#}\n\n#4、Configure item pipelines\n# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html\nITEM_PIPELINES = {\n   # 'Amazon.pipelines.CustomPipeline': 200,\n}\n\n\n\n#===>第六部分：缓存<===\n\"\"\"\n1. 启用缓存\n    目的用于将已经发送的请求或相应缓存下来，以便以后使用\n\n    from scrapy.downloadermiddlewares.httpcache import HttpCacheMiddleware\n    from scrapy.extensions.httpcache import DummyPolicy\n    from scrapy.extensions.httpcache import FilesystemCacheStorage\n\"\"\"\n# 是否启用缓存策略\n# HTTPCACHE_ENABLED = True\n\n# 缓存策略：所有请求均缓存，下次在请求直接访问原来的缓存即可\n# HTTPCACHE_POLICY = \"scrapy.extensions.httpcache.DummyPolicy\"\n# 缓存策略：根据Http响应头：Cache-Control、Last-Modified 等进行缓存的策略\n# HTTPCACHE_POLICY = \"scrapy.extensions.httpcache.RFC2616Policy\"\n\n# 缓存超时时间\n# HTTPCACHE_EXPIRATION_SECS = 0\n\n# 缓存保存路径\n# HTTPCACHE_DIR = 'httpcache'\n\n# 缓存忽略的Http状态码\n# HTTPCACHE_IGNORE_HTTP_CODES = []\n\n# 缓存存储的插件\n# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n```\n\n新建抓取脚本\n\n```\n#导包\nimport scrapy\nimport os\n\n#定义抓取类\nclass Test(scrapy.Spider):\n\n    #定义爬虫名称，和命令行运行时的名称吻合\n    name = \"test\"\n\n    #定义头部信息\n    haders = {\n        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/73.0.3683.86 Chrome/73.0.3683.86 Safari/537.36'\n    }\n\n    #定义回调方法\n    def parse(self, response):\n        #将抓取页面保存为文件\n        page = response.url.split(\"/\")[-2]\n        filename = 'test-%s.html' % page\n        if not os.path.exists(filename):\n            with open(filename, 'wb') as f:\n                f.write(response.body)\n        self.log('Saved file %s' % filename)\n\n\n        #匹配规则\n\n        content_left_div = response.xpath('//*[@id=\"content-left\"]')\n        content_list_div = content_left_div.xpath('./div')\n\n        for content_div in content_list_div:\n            yield {\n                'author': content_div.xpath('./div/a[2]/h2/text()').get(),\n                'content': content_div.xpath('./a/div/span/text()').getall(),\n            }\n\n    #定义列表方法\n    def start_requests(self):\n        urls = [\n            'https://www.qiushibaike.com/text/page/1/',\n            'https://www.qiushibaike.com/text/page/2/',\n        ]\n        for url in urls:\n            #如果想使用代理 可以加入代理参数 meta\n            #meta={'proxy': 'http://proxy.yourproxy:8001'}\n\n            #抓取方法\n            yield scrapy.Request(url=url, callback=self.parse,headers=self.haders)\n```\n\n执行抓取脚本 注意脚本名称和上文定义的name变量要吻合\n\n```\nscrapy crawl test\n```\n\n## scrapy 中间件\n\n下载器中间件是介于Scrapy的request/response处理的钩子框架，是用于全局修改Scrapy request和response的一个轻量、底层的系统。\n\n开发代理中间件\n\n在爬虫开发中，更换代理IP是非常常见的情况，有时候每一次访问都需要随机选择一个代理IP来进行。\n\n中间件本身是一个Python的类，只要爬虫每次访问网站之前都先“经过”这个类，它就能给请求换新的代理IP，这样就能实现动态改变代理。\n\n在创建一个Scrapy工程以后，工程文件夹下会有一个middlewares.py文件\n\n在middlewares.py中添加下面一段代码：\n\n```\nimport random\nfrom scrapy.conf import settings\n\nclass ProxyMiddleware(object):\n\n    def process_request(self, request, spider):\n        proxy = random.choice(settings['PROXIES'])\n        request.meta['proxy'] = proxy\n```\n\n进入settings，开启中间件\n\n```\nDOWNLOADER_MIDDLEWARES = {\n  'AdvanceSpider.middlewares.ProxyMiddleware': 543,\n}\n```\n\n配置好后运行爬虫，scrapy会在每次请求之前随机分配一个代理，可以请求下面的网址查看是否用了代理\n\n<http://exercise.kingname.info/exercise_middleware_ip>\n\n## 分布式爬虫\n\nScrapy-Redis是一个基于Redis的Scrapy分布式组件。它利用Redis对用于爬取的请求(Requests)进行存储和调度(Schedule)，并对爬取产生的项目(items)存储以供后续处理使用。scrapy-redi重写了scrapy一些比较关键的代码，将scrapy变成一个可以在多个主机上同时运行的分布式爬虫。\n\n具体部署和使用攻略：<https://v3u.cn/Index_a_id_83>\n\n![img](../img/scrapy_redis.jpeg)\n\n说白了，就是使用redis来维护一个url队列,然后scrapy爬虫都连接这一个redis获取url,且当爬虫在redis处拿走了一个url后,redis会将这个url从队列中清除,保证不会被2个爬虫拿到同一个url,即使可能2个爬虫同时请求拿到同一个url,在返回结果的时候redis还会再做一次去重处理,所以这样就能达到分布式效果,我们拿一台主机做redis 队列,然后在其他主机上运行爬虫.且scrapy-redis会一直保持与redis的连接,所以即使当redis 队列中没有了url,爬虫会定时刷新请求,一旦当队列中有新的url后,爬虫就立即开始继续爬\n\n## 应对反爬\n\n### headers头文件\n\n有些网站对爬虫反感，对爬虫请求一律拒绝，这时候我们需要伪装成浏览器，通过修改http中的headers来实现\n\n```\nheaders = {\n            'Host': \"bj.lianjia.com\",\n            'Accept': \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n            'Accept-Encoding': \"gzip, deflate, sdch\",\n            'Accept-Language': \"zh-CN,zh;q=0.8\",\n            'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.87 Safari/537.36\",\n            'Connection': \"keep-alive\",\n        }\np = requests.get(url, headers=headers)\nprint(p.content.decode('utf-8'))\n```\n\n### 伪造Cookie\n\n模拟登陆\n\n一般登录的过程都伴随有验证码，这里我们通过selenium自己构造post数据进行提交，将返回验证码图片的链接地址输出到控制台下，点击图片链接识别验证码，输入验证码并提交，完成登录\n\n```\nfrom selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys    #\nfrom selenium.webdriver.support.ui import WebDriverWait   # WebDriverWait的作用是等待某个条件的满足之后再往后运行\nfrom selenium.webdriver import ActionChains\nimport time\nimport sys\ndriver = webdriver.PhantomJS(executable_path='C:\\PyCharm 2016.2.3\\phantomjs\\phantomjs.exe')  # 构造网页驱动\n\ndriver.get('https://www.zhihu.com/#signin')       # 打开网页\ndriver.find_element_by_xpath('//input[@name=\"password\"]').send_keys('your_password')\ndriver.find_element_by_xpath('//input[@name=\"account\"]').send_keys('your_account')\ndriver.get_screenshot_as_file('zhihu.jpg')                   # 截取当前页面的图片\ninput_solution = input('请输入验证码 :')\ndriver.find_element_by_xpath('//input[@name=\"captcha\"]').send_keys(input_solution)\ntime.sleep(2)\n\ndriver.find_element_by_xpath('//form[@class=\"zu-side-login-box\"]').submit()  # 表单的提交  表单的提交，即可以选择登录按钮然后使用click方法，也可以选择表单然后使用submit方法\nsreach_widonw = driver.current_window_handle     # 用来定位当前页面\n# driver.find_element_by_xpath('//button[@class=\"sign-button submit\"]').click()\ntry:\n    dr = WebDriverWait(driver,5)\n    # dr.until(lambda the_driver: the_driver.find_element_by_xpath('//a[@class=\"zu-side-login-box\"]').is_displayed())\n    if driver.find_element_by_xpath('//*[@id=\"zh-top-link-home\"]'):\n        print('登录成功')\nexcept:\n    print('登录失败')\n    driver.save_screenshot('screen_shoot.jpg')     #截取当前页面的图片\n    sys.exit(0)\ndriver.quit()   #退出驱动\n```\n\n### 代理ip\n\n当爬取速度过快时，当请求次数过多时都面临ip被封的可能。因此使用代理也是必备的。\n\n### 代理池的概念\n\n抓取市面上所有免费代理网站的ip，比如西刺代理，快代理等\n\n代理池维护存储 redis 因为代理ip生命周期很短，属于热数据，不适合持久化存储\n\n使用时随机取出一个代理ip使用\n\n使用request加代理\n\n```\nimport requests\nproxies = { \"http\": \"http://10.10.1.10:3128\",\n            \"https\": \"http://10.10.1.10:1080\",}\np = request.get(\"http://www.baidu.com\", proxies = proxies)\nprint(p.content.decode('utf-8'))\n```\n\n## 抓取App端数据\n\n使用Charles抓包\n\n软件地址 <https://www.charlesproxy.com/download/>\n\n为什么选择Charles 跨平台，方便好用，可以抓取Android应用也可以抓取Ios\n\n可以抓取http https\n\n## 抓取视频\n\n使用三方库 you-get\n\n配合Fiddler抓包来抓取视频流\n","tags":["爬虫"]},{"title":"Redis简介","url":"/2019/07/23/pout/数据库/Redis/Redis/","content":"\n\n<!-- toc -->\n\n## Redis的特点\n\nRedis本质上是一个Key-Value类型的内存数据库，很像memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。\n\n因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。\n\nRedis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像 memcached只能保存1MB的数据，因此Redis可以用来实现很多有用的功能。\n\n比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当作一 个功能加强版的memcached来用。\n\nRedis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。\n\n## 使用redis有哪些好处？\n\n1.速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)\n\n2.支持丰富数据类型，支持string，list，set，sorted set，hash\n\n3.支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行\n\n4.丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除\n\n## 为什么redis需要把所有数据放到内存中?\n\nRedis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度为严重影响redis的性能。在内存越来越便宜的今天，redis将会越来越受欢迎。\n\n如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。\n\n## Redis是单进程单线程的\n\nredis利用队列技术将并发访问变为串行访问，消除了传统数据库串行控制的开销\n\n## 单线程的redis为什么这么快\n\n回答:主要是以下三点\n\n(一)纯内存操作\n\n(二)单线程操作，避免了频繁的上下文切换\n\n(三)采用了非阻塞I/O多路复用机制\n\n## redis持久化的几种方式\n\n1、快照（snapshots）\n\n缺省情况情况下，Redis把数据快照存放在磁盘上的二进制文件中，文件名为dump.rdb。你可以配置Redis的持久化策略，例如数据集中每N秒钟有超过M次更新，就将数据写入磁盘；或者你可以手工调用命令SAVE或BGSAVE。\n\n工作原理\n\nRedis forks.\n\n子进程开始将数据写到临时RDB文件中。\n\n当子进程完成写RDB文件，用新文件替换老文件。\n\n这种方式可以使Redis使用copy-on-write技术。\n\n2、AOF\n\n快照模式并不十分健壮，当系统停止，或者无意中Redis被kill掉，最后写入Redis的数据就会丢失。\n\n这对某些应用也许不是大问题，但对于要求高可靠性的应用来说，Redis就不是一个合适的选择。Append-only文件模式是另一种选择。你可以在配置文件中打开AOF模式\n\n3、虚拟内存方式\n\n当你的key很小而value很大时,使用VM的效果会比较好.因为这样节约的内存比较大.\n\n当你的key不小时,可以考虑使用一些非常方法将很大的key变成很大的value,比如你可以考虑将key,value组合成一个新的value.\n\nvm-max-threads这个参数,可以设置访问swap文件的线程数,设置最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的.可能会造成比较长时间的延迟,但是对数据完整性有很好的保证.\n\n自己测试的时候发现用虚拟内存性能也不错。如果数据量很大，可以考虑分布式或者其他数据库。\n\n## 使用过Redis分布式锁么，\n\n它是什么回事？先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。这时候对方会告诉你说你回答得不错，然后接着问如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？这时候你要给予惊讶的反馈：唉，是喔，这个锁就永远得不到释放了。紧接着你需要抓一抓自己得脑袋，故作思考片刻，好像接下来的结果是你主动思考出来的，然后回答：我记得set指令有非常复杂的参数，这个应该是可以同时把setnx和expire合成一条指令来用的！对方这时会显露笑容，心里开始默念：摁，这小子还不错。\n\n## 假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？\n\n使用keys指令可以扫出指定模式的key列表。对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。\n\n## 使用过Redis做异步队列么，你是怎么用的？\n\n一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。如果对方追问可不可以不用sleep呢？list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。 如果对方追问能不能生产一次消费多次呢？使用pub/sub主题订阅者模式，可以实现1:N的消息队列。如果对方追问pub/sub有什么缺点？在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如rabbitmq等。如果对方追问redis如何实现延时队列？我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这么详细。但是你很克制，然后神态自若的回答道：使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。到这里，面试官暗地里已经对你竖起了大拇指。但是他不知道的是此刻你却竖起了中指，在椅子背后。\n\n## 如果有大量的key需要设置同一时间过期，一般需要注意什么？\n\n如果大量的key过期时间设置的过于集中，到过期的那个时间点，redis可能会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间分散一些。\n\n## Pipeline有什么好处，为什么要用pipeline？\n\n可以将多次IO往返的时间缩减为一次，前提是pipeline执行的指令之间没有因果相关性。使用redis-benchmark进行压测的时候可以发现影响redis的QPS峰值的一个重要因素是pipeline批次指令的数目。\n\n## Redis的同步机制了解么？\n\nRedis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。\n\n## 是否使用过Redis集群，集群的原理是什么？\n\nRedis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。\n\n## 什么是Redis的并发竞争问题\n\nRedis的并发竞争问题，主要是发生在并发写竞争。\n\n考虑到redis没有像db中的sql语句，update val = val + 10 where ...，无法使用这种方式进行对数据的更新。\n\n假如有某个key = \"price\"， value值为10，现在想把value值进行+10操作。正常逻辑下，就是先把数据key为price的值读回来，加上10，再把值给设置回去。如果只有一个连接的情况下，这种方式没有问题，可以工作得很好，但如果有两个连接时，两个连接同时想对还price进行+10操作，就可能会出现问题了。\n\n例如：两个连接同时对price进行写操作，同时加10，最终结果我们知道，应该为30才是正确。\n\n考虑到一种情况：\n\nT1时刻，连接1将price读出，目标设置的数据为10+10 = 20。\n\nT2时刻，连接2也将数据读出，也是为10，目标设置为20。\n\nT3时刻，连接1将price设置为20。\n\nT4时刻，连接2也将price设置为20，则最终结果是一个错误值20。\n\n### 解决方案\n\n使用乐观锁的方式进行解决（成本较低，非阻塞，性能较高）\n\n如何用乐观锁方式进行解决？\n\n本质上是假设不会进行冲突，使用redis的命令watch进行构造条件。伪代码如下：\n\n```\nwatch price\n\nget price $price\n\n$price = $price + 10\n\nmulti\n\nset price $price\n\nexec\n```\n\nwatch这里表示监控该key值，后面的事务是有条件的执行，如果从watch的exec语句执行时，watch的key对应的value值被修改了，则事务不会执行。\n\n方案2 这个是针对客户端来的，在代码里要对redis操作的时候，针对同一key的资源，就先进行加锁（java里的synchronized或lock）。\n\n方案3 利用redis的setnx实现内置的锁。\n\n## redis和memcached的区别（总结）\n\n1、Redis和Memcache都是将数据存放在内存中，都是内存数据库。不过memcache还可用于缓存其他东西，例如图片、视频等等；\n\n2、Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，hash等数据结构的存储；\n\n3、虚拟内存--Redis当物理内存用完时，可以将一些很久没用到的value 交换到磁盘；\n\n4、过期策略--memcache在set时就指定，例如set key1 0 0 8,即永不过期。Redis可以通过例如expire 设定，例如expire name 10；\n\n5、分布式--设定memcache集群，利用magent做一主多从;redis可以做一主多从。都可以一主一从；\n\n6、存储数据安全--memcache挂掉后，数据没了；redis可以定期保存到磁盘（持久化）；\n\n7、灾难恢复--memcache挂掉后，数据不可恢复; redis数据丢失后可以通过aof恢复；\n\n8、Redis支持数据的备份，即master-slave模式的数据备份；\n\n应用场景\n\nredis：数据量较小的更性能操作和运算上\n\nmemcache：用于在动态系统中减少数据库负载，提升性能;做缓存，提高性能（适合读多写少，对于数据量比较大，可以采用sharding）\n\nMongoDB:主要解决海量数据的访问效率问题\n","tags":["数据库"]},{"title":"深入Vue源代码解决时序问题一","url":"/2019/07/06/pout/前端技术/深入Vue源代码解决时序问题一/","content":"\n>viola 是一个支持 Vue 的动态化框架，其 Vue 版本在 Vue 官方版本 2.5.7 上进行了少量改写，本文针对其进行具体分析。\n\n最初，有使用者报告一个错误：在 iOS 系统，退出页面的时候，框架报错：\n\n```\nTypeError: undefined is not an object(evaluating 'e.isDestroyed\"\n```\n\n接到这个错误之后，我首先进入 Vue 的 debug 版本，尝试获取更详细的信息：\n\n```\nTypeError: undefined is not an object(evaluating 'componentInstance.isDestroyed\"\n```\n\n我们顺利地拿到了报错的变量名称，去 Vue 源代码中搜索，我们可以发现报错之处：\n\n```javascript\ndestroy: function destroy (vnode) {\n    var componentInstance = vnode.componentInstance;\n    if (!componentInstance._isDestroyed) { // 这里报错\n      if (!vnode.data.keepAlive) {\n        componentInstance.$destroy();\n      } else {\n        deactivateChildComponent(componentInstance, true /* direct */);\n      }\n    }\n  }\n```\n\n这里是 `componentInstance` 为 undefined，这个实际上是 vnode 的实例，其为 undefined，说明该 vue 组件在之前的阶段就已经出错不正常了，这里并不是错误的根源所在，我们需要再次进行寻找报错原因。\n\n于是我们查看业务代码的所有日志，又发现了这样一条报错：\n\n```\n[Vue warn]: Error in nextTick: \"TypeError: undefined is not an object (evaluating 'vm.$options')\" \n```\n\n初始化阶段出现这样一个错误，我们怀疑 `vm` 就是上文的 `componentInstance`，于是，我们打印报错堆栈：\n\n```javascript\n 调用栈:\nfunction updateChildComponent(\n    vm,\n    propsData,\n    listeners,\n    parentVnode,\n    renderChildren\n  ) {\n        //...\n        var hasChildren = !!(\n              renderChildren ||\n              vm.$options._renderChildren || // 这里报错\n              parentVnode.data.scopedSlots ||\n              vm.$scopedSlots !== emptyObject\n            );\n    }\n\nfunction prepatch(oldVnode, vnode) {\n      var options = vnode.componentOptions;\n      var child = vnode.componentInstance = oldVnode.componentInstance;\n      updateChildComponent(\n        child,\n        options.propsData,\n        options.listeners,\n        vnode,\n        options.children\n      );\n    }\n\nfunction patchVnode(oldVnode, vnode, insertedVnodeQueue, removeOnly) {}\nfunction patch(oldVnode, vnode, hydrating, removeOnly) {}\nfunction (vnode, hydrating) {}\nfunction () {\n        vm._update(vm._render(), hydrating);\n      }\nfunction get() {}\nfunction getAndInvoke(cb) {}\nfunction run() {}\nfunction flushSchedulerQueue() {}\nfunction flushCallbacks() {}\n```\n\n调用栈实际上有点冗长，不过我们还是能发现两个有用的信息：\n\n* 初始化阶段为 `undefined` 的 `vm`，就是 `componentInstance`，也就是和 destroy 阶段的报错属于同一个原因。\n* 根据调用栈发现，这是一个更新阶段的报错。\n\n这引发了我们的思考：更新阶段找不到 `componentInstance` 报错。\n\n这里实际上有点阻塞了，因为一般来说，Vue 的源代码经过测试，应该不会出现这种问题的，那是不是我们的问题呢，我们回归到业务代码：\n\n```\ncreated() {\n    this.getFeedsListFromCache();\n},\nmethods: {\n    getFeedsListFromCache() {\n        viola.requireAPI(\"cache\").getItem(this.cacheKey_feeds, data => {\n            this.processData(data.list);\n        });\n    },\n    processData(list = [], opt = {}) {\n        if (this.list.length < cacehFeedsLength) {\n        }\n        this.list = [];\n    },\n}\n```\n\n我们对业务代码进行了抽象简化，上面是我们的最小问题 Demo，实际上我们就做了这样一件事情：\n\n* 在 created 执行方法，调用端的接口，再回调函数里面更新某个 data 中声明的数据。\n\n首先，我们可以梳理下对一般 vue 组件的初始化更新，vue 是如何做的：\n\n* created 时实际上 vnode 已经建立完成，这个时候还没有 mount，但是数据监听已经建立了，这个时候如果改动数据，会把相关 update 函数放在一个名为 flushCallbacks 的函数队列中。\n* 该函数队列会通过默认为 `Promise.then` 的 microtask 方式来调度，当前阶段的 mount 流程会继续，mount 结束后，会执行 flushCallbacks 队列中的更新操作。\n\n从代码层面上来讲，这几个流程应该是这样的：\n\n```\n ├── callHook(vm, 'created'); // 执行created 钩子\n ├── proxySetter(val); // 改变数据，调用 proxy\n ├── Watcher.prototype.update; // 调用 Watcher，将 update 操作入栈\n ├── vm.$mount(vm.$options.el); // 执行 mount 流程\n ├── callHook(vm, 'beforeMount');\n ├──  callHook(vm, 'mounted'); // 依次调用 beforeMount 和 mounted\n └── flushCallbacks // 执行 更新\n```\n\n然后我们分析我们这里的流程，首先值得强调的是这个函数 `viola.requireAPI(\"cache\").getItem`，这个函数是端注入的函数，但我们不能将其当作异步函数来对待，实际上，**这是一个同步函数**，（至于这个同步函数和 js 中的普通函数，是否有区别，还有待商榷，不过应该是有区别的，因为如果我们不用此函数的话，就不会出现该问题。）\n\n接下来，我们打出详细的调用栈，根据顺序来分析实际的执行流程：\n\n```\n ├── callHook(vm, 'created'); // 执行created 钩子\n ├── proxySetter(val); // 改变数据，调用 proxy\n ├── Watcher.prototype.update; // 调用 Watcher，将 update 操作入栈\n ├── flushCallbacks // 执行 更新\n ├── vm.$mount(vm.$options.el); // 执行 mount 流程 \n ├── callHook(vm, 'beforeMount');\n └── callHook(vm, 'mounted'); // 依次调用 beforeMount 和 mounted\n```\n\n我们发现，我们的执行流程出现了很大问题：**在 mount 阶段未完成的时候就执行了 flushCallbacks，先执行更新操作，这里的顺序错乱导致了后续问题**。\n\n我们可看下调用 `flushCallbacks` 的代码：\n\n```javascript\nif (typeof Promise !== 'undefined' && isNative(Promise)) {\n  var p = Promise.resolve();\n  microTimerFunc = function () {\n    p.then(flushCallbacks);\n    // in problematic UIWebViews, Promise.then doesn't completely break, but\n    // it can get stuck in a weird state where callbacks are pushed into the\n    // microtask queue but the queue isn't being flushed, until the browser\n    // needs to do some other work, e.g. handle a timer. Therefore we can\n    // \"force\" the microtask queue to be flushed by adding an empty timer.\n    if (isIOS) { setTimeout(noop); }\n  };\n} \n```\n\n这里 `microTimerFunc` 的 `p.then`，被同步执行了，也就是说，这里的微任务优先于当前事件循环的函数执行了（此时由于 mount 流程是同步的，mount 流程的相关函数**理应**在该事件循环中，优先于微任务执行）。\n\n我们找到了根源，接下来就是分析解决方案和根本原因。\n\n由于我们的问题在于 update 流程执行太快了，所以采用一种方式放慢一点即可：\n\n* 将 vue 的微任务模式（默认）改成宏任务模式：`var useMacroTask = false; => true`。\n* 在 created 阶段的加一个 `setTimeout(0)`。\n\n不过对于根本原因，实际上本次仍然没有完全分析透彻，还留有如下疑问：\n\n* `viola.requireAPI(\"cache\").getItem` 这个函数到底做了什么？其对事件循环有什么影响？\n* 在执行 `microTimerFunc` 的时候，为什么 `p.then` 优先于 `vm.$mount` 执行了？\n* 该错误仅在 iOS 系统出现，iOS 系统是否会在某些情况将微任务的优先级变高？\n\n对于这些疑问，Vue 源代码中也做了一些评论：\n\n```\n// Here we have async deferring wrappers using both microtasks and (macro) tasks.\n// In < 2.4 we used microtasks everywhere, but there are some scenarios where\n// microtasks have too high a priority and fire in between supposedly\n// sequential events (e.g. #4521, #6690) or even between bubbling of the same\n// event (#6566). However, using (macro) tasks everywhere also has subtle problems\n// when state is changed right before repaint (e.g. #6813, out-in transitions).\n// Here we use microtask by default, but expose a way to force (macro) task when\n// needed (e.g. in event handlers attached by v-on).\n```\n\n不过，这里始终都没有找到最本质的原因，也许这和 iOS JSCore 的微任务/宏任务的处理机制有关，具体原因，待下次探究。\n\n\n\n","tags":["viola"]},{"title":"Python内存管理","url":"/2019/07/03/pout/python中高级面试题/Python内存管理/","content":"\n\n<!-- toc -->\n\n\n# 在Python中是如何管理内存的\n\nPython有一个私有堆空间来保存所有的对象和数据结构。作为开发者，我们无法访问它，是解释器在管理它。但是有了核心API后，我们可以访问一些工具。Python内存管理器控制内存分配。\n\n另外，内置垃圾回收器会回收使用所有的未使用内存，所以使其适用于堆空间。\n\n一、垃圾回收：python不像C++，Java等语言一样，他们可以不用事先声明变量类型而直接对变量进行赋值。对Python语言来讲，对象的类型和内存都是在运行时确定的。这也是为什么我们称Python语言为动态类型的原因(这里我们把动态类型可以简单的归结为对变量内存地址的分配是在运行时自动判断变量类型并对变量进行赋值)。\n\n二、引用计数：Python采用了类似Windows内核对象一样的方式来对内存进行管理。每一个对象，都维护这一个对指向该对对象的引用的计数。当变量被绑定在一个对象上的时候，该变量的引用计数就是1，(还有另外一些情况也会导致变量引用计数的增加),系统会自动维护这些标签，并定时扫描，当某标签的引用计数变为0的时候，该对就会被回收。\n\n1 对象存储\n\n在Python中万物皆对象\n\n不存在基本数据类型，`0, 1.2, True, False, \"abc\"`等，这些全都是对象\n\n所有对象, 都会在内存中开辟一块空间进行存储\n\n2.1 会根据不同的类型以及内容, 开辟不同的空间大小进行存储 2.2 返回该空间的地址给外界接收(称为\"引用\"), 用于后续对这个对象的操作 2.3 可通过 id() 函数获取内存地址(10进制) 2.4 通过 hex() 函数可以查看对应的16进制地址\n\n```\nclass Person:\n    pass\n\np = Person()\nprint(p)\nprint(id(p))\nprint(hex(id(p)))\n\n\n>>>> 打印结果\n\n<__main__.Person object at 0x107030470>\n4412605552\n0x107030470\n```\n\n对于整数和短小的字符, Python会进行缓存; 不会创建多个相同对象\n\n此时, 被多次赋值, 只会有多份引用\n\n```\nnum1 = 2\nnum2 = 2\nprint(id(num1), id(num2))\n\n>>>> 打印结果\n\n4366584464 4366584464\n```\n\n容器对象, 存储的其他对象, 仅仅是其他对象的引用, 并不是其他对象本身\n\n4.1 比如字典, 列表, 元组这些\"容器对象\" 4.2 全局变量是由一个大字典进行引用 4.3 可通过 global() 查看\n\n2 对象回收 2.1 引用计数器 2.1.1概念\n\n一个对象, 会记录着自身被引用的个数 每增加一个引用, 这个对象的引用计数会自动+1 每减少一个引用, 这个对象的引用计数会自动-1\n\n引用计数+1场景\n\n```\n1、对象被创建\n    p1 = Person()\n2、对象被引用\n    p2 = p1\n3、对象被作为参数，传入到一个函数中\n    log(p1)\n    这里注意会+2, 因为内部有两个属性引用着这个参数\n4、对象作为一个元素，存储在容器中\n    l = [p1]\n```\n\n引用计数-1场景\n\n```\n1、对象的别名被显式销毁\n    del p1\n2、对象的别名被赋予新的对象\n    p1 = 123\n3、一个对象离开它的作用域\n    一个函数执行完毕时\n    内部的局部变量关联的对象, 它的引用计数就会-1\n4、对象所在的容器被销毁，或从容器中删除对象\n```\n\n查看引用计数\n\n```\nimport sys\n\nclass Person:\n    pass\n\np1 = Person() # 1\n\nprint(sys.getrefcount(p1)) # 2\n\np2 = p1 # 2\n\nprint(sys.getrefcount(p1)) # 3\n\ndel p2 # 1\nprint(sys.getrefcount(p1)) # 2\n\ndel p1\n# print(sys.getrefcount(p1)) #error，因为上一行代码执行类p1对象已经销毁\n\n>>>> 打印结果\n\n2\n3\n2\n```\n\n循环引用\n\n```\n# 循环引用\nclass Person:\n    pass\n\nclass Dog:\n    pass\n\np = Person() \nd = Dog()   \n\np.pet = d \nd.master = p\n```\n\n对象间互相引用，导致对象不能通过引用计数器进行销毁\n\n手动触发垃圾回收，挥手循环引用\n\n```\nimport objgraph\nimport gc\n\nclass Person:\n    pass\n\nclass Dog:\n    pass\n\np = Person()\nd = Dog()\n\np.pet = d\nd.master = p\n\n\ndel p\ndel d\n\ngc.collect() #手动触发垃圾回收\n\nprint(objgraph.count(\"Person\"))\nprint(objgraph.count(\"Dog\"))\n\n>>>> 打印结果\n0\n0\n```\n","tags":["python中高级面试题"]},{"title":"web应用开发与部署——你必须掌握的内容","url":"/2019/06/16/pout/后端技术/web应用开发与部署——你必须掌握的内容/","content":"\n\n\n本文基于笔者在腾讯的项目经验，从真实场景出发分析一个中型 Web 应用从立项到上线稳定运行的平稳解决方案，力求既不太空泛以至于看完了仍然找不到落地的点，也尽量不会特别纠结于个别细节导致没有相关使用经历的同学无法感同身受，而是从宏观到方法论，分析整个流程中我们需要用到的工具、方法与规范，给大家提供一个参考。\n\n本文适合具有一定经验的初中级前端开发者，如果有相关问题，也欢迎与我交流。\n\n目录\n\n* 项目构建的搭建，关键词：**webpack**、**react/vue cli**，**steamer**，**组件库**\n* 代码的规范约束，关键词：**typescript**、**eslint**、**prettier**\n* 测试与测试部署，关键词：**测试部署方案**、**docker**\n* 日志查看与脚本错误的监控，关键词：**sentry**、**vconsole**、**mlogger**\n* 版本发布更新，关键词：**发布系统**、**灰度发布**\n* 访问量实时监控\n\n### 起步：项目构建的搭建\n\n#### 使用 webpack 搭建脚手架\n\n目前在一般的项目中，我们都会使用 webpack 作为搭建开发环境的基础，而 react 和 vue 也各自提供了 cli 工具用于开发一个中小型项目，react 提供了 eject 功能让我们可以更加自由的配置 webpack，而 vue 脚手架虽然没有提供类似命令，但是借助 webpack 工具链我们几乎也可以自由定制每一个角落。\n\n不过，这里我的建议是，如果是个人项目或小型项目，我们可以基于 react 或 vue 的脚手架进行更改使用，对于一个具备一定规模的项目团队，建议还是自己维护一套单独的 webpack 构建环境，原因如下：\n\n* 由于我们一般需要在项目中接入各类司内工具、支持高级API和语法、同时支持 react/vue、构建目录定制化等各类工作，实际上 80% 以上的工作我们都需要在模版之上自行添加，这个时候我们再用脚手架带来的收益已经非常小了，反而还会受制于项目的初始目录结构。\n\n我们在自定义脚手架的 webpack 构建的时候，也需要梳理出一定的目录规范与约束，这样也有利于提高后期脚手架的可维护性和扩展性，一般来说，我们也要对脚手架中的公共部分和项目私有部分进行分离，对于一个具体项目而言，可以不用改动 webpack 的项目公共部分，这样也有利于减少不同项目之间的切换成本，对于我们目前的项目，一般会有如下两个目录：\n\n```\n- project\n\t- project.js\n- config\n\t- feature\n\t- plugins\n\t- rules\n\t- script.js\n\t- webpack.base.js \t\n```\n\n对于一个项目，只需更改 project 下的配置。\n\n这里我也推荐一个前同事做的[steamer研发体系](https://github.com/steamerjs)，在从中也可以找到很多相关参考，最简单的方式，就是直接在[steamer-simple](https://github.com/steamerjs/steamer-simple) 的基础上进行扩展。\n\n#### 定制生成目录\n\n生成目录的格式，这里需要单独讲一下。\n\n一般来说，我们生成目录的格式都是要跟发布系统进行结合的，不过也有的时候，我们做项目的时候还没有明确要接入发布系统，或者尚不知道发布系统的格式要求，但是一般情况下我们应当遵循下面的约定：\n\n* js/css/img 等资源文件和 html 文件分离，前者发布到 CDN，后者发布到 http 服务器。\n* html 中引入的文件地址，应当是在构建过程中更新的 CDN 地址，而不是相对路径地址。\n* 如果有离线包（offline 能力需要对应的客户端 webview 支持）等，需要单独一个目录。\n\n对于我们目前的项目而言，一般情况下会有三个生成目录：\n\n```\n- cdn\n- offline # 需要客户端支持该能力\n- webserver\n```\n\n如果一开始我们把所有内容生成到一个目录中了，这给我们后期的改动和维护，都带来很大的隐患。\n\n#### 组件库\n\n组件库这一部分，适合项目开始变得复杂的情况下进行启动，而不建议一开始进行过渡设计，建设组件库能够通过组件复用降低我们的开发成本，同时，组件库也需要专人维护，保持更新。\n\n### 开发：代码的规范约束\n\n对于 js 文件的代码格式，诸如要不要分号、两个还是四个字符缩进等，一只争议不断，本文也不对此进行讨论，但是对于一个团队的项目集合（而不是单个项目）而言，代码风格的统一，是一个非常有必要而且必须要做的事情。\n\n#### typescript\n\n关于 typescript 的相关文章实在太多了，这里也不对此进行详细的说明，其对代码的可读性、规范约束、降低报错风险等都有很好的改进，对于越复杂的项目其效果越明显。\n\n另外， [typescript 入门教程](https://ts.xcatliu.com/)的作者也在我们团队中，这里我想说，如果现在还没有开始使用 typescript，请开始学习并使用 typescript 吧。\n\n#### eslint 与 prettier\n\n除了 typescript 以外，在代码格式方面还建议使用 eslint 和 prettier 来进行代码格式的约束，这里虽然 eslint 和 prettier 两者在某些情景下会有些重叠，但是两者的侧重点不同，eslint 侧重于代码质量的检查并且给出提示，在某种层面上，可以看作是 typescript 的补充，而 prettier 在格式化代码方面更具有优势，并且 prettier 在设计之初就考虑了和 eslint 的集成，所以你可以完全放心的在项目中使用两者，而不用担心出现无法解决的冲突。\n\n另外，eslint 社区中已经形成了很多种最佳实践，而我们团队也设计出了自己的一套 eslint 规则，可以按需[取用](https://github.com/AlloyTeam/eslint-config-alloy)\n\np.s. 目前 tslint 后续计划不在维护，转向 eslint 增强，因此我们在项目中可以不再使用 tslint。\n\n以上这几种代码风格方面的约束，适合项目之初即开始约束，这样在中后期会有巨大的时间成本的节省和效率的提升。\n\n### 协作：使用 git\n\n使用 git 进行协作这里其实包括两个点，使用 git 管理项目与自建 gitlab，后者是一个比较基础性的工作，并且实际上难度并不大，我认为每一个公司都可以使用自建的 gitlab 进行版本管理，这个实际上难度并不大，并且可以有效的保护公司的代码财产，如果你所在的公司还没有，那么这也是你的机会。\n\n在具体的使用 git 中，对于git的分支/TAG管理、PR规范、提交文件约束等都应当有一套合理的流程，这里我对几点比较重要的进行说明：\n\n* 锁定主干与分支开发，我们在日常开发中禁止直接提交主干，而是只能在分支中进行开发，并且通过 MR 的方式提交到主干。\n* git hooks 检查：我们应该通过 git hooks 进行必要的检查，比如自动化测试、eslint 检查、以及一些项目中必要的检查。\n* MR 检查与 Code Review，这里建议在 Merge Request 的时候做两件事情，一件是 Code Review，不过这个在某些特殊情况下不具备条件，尤其是团队人力紧张的时候，另外一个则是 MR 的 HOOK 触发检查，这个一般需要借助一些持续集成工具来完成，可以说是我们代码在合并主干之前的最后一个关卡。\n\n### 测试：测试与测试部署\n\n测试是代码开发中重要的一个环节，但实际上对于前端开发来说，前端开发工程师一般较少书写测试用例，也并没有专业的测试开发工程师来辅助工作，不过，一般会有配备系统测试工程师在黑盒的情况下进行冒烟测试和功能测试以及整体链路的验收，俗称“点点点”。而这个时候，前端开发要做的就是把程序代码部署到测试服务器上，同时提供一个尽可能真实的场景供测试进行测试。\n\n在笔者经历的项目中，虽然也使用了单元测试、端对端测试，不过这一部分体系并不十分完备，并且可能也不是大多数前端开发者感兴趣的内容，所以这里主要总结如何进行高效的测试部署与发布对接。\n\n一般来说，我们一般会有一台到多台 Linux 测试机，供测试环境部署使用，对于前端项目而言，一般不需要特殊环境，可以进行 webpack 构建以及有 nginx 进行转发即可。\n\n而测试环境的部署，如果是让我们手动登录去部署，显然是不合理的，如果我们纯粹使用 CI 来完成这件事，则对 CI 工具的能力和项目人员素质有一定要求，并且不具备可视化管理能力，容易出错，这里我建议可以维护一个可视化系统来进行测试环境的部署和管理，其整个环节应该是这样的：\n\n```\n本地代码 -> gitlab -> 测试系统部署 -> 对接发布系统 \n```\n\n这里的测试系统，实际上是从 gitlab 拉取代码，并且本地执行 build 命令（一般是 `npm run build`）并把构建结果存储在 nginx 可代理的目录即可，出于系统完备性考虑，一般我们会有多台测试机，这里我建议一般拿其中的一台作为构建机，其他的测试机仅提供 nginx 代理能力即可，我们在一台构建机中进行构建，并且将构建结果通过系统命令发送到其他的测试机。\n\n一台构建机可以服务于所有的项目，这里还可能涉及到 webpack、nodejs 版本的维护，我们需要约束各个测试项目构建处在一个相对独立的环境中，我们也可以使用过 Docker 来进行构建，保证隔离。\n\n构建完成后，一般我们借助 Fiddler、Charles、Whistle 等任意一款代理工具，即可以进行测试。\n\n### 监控：日志查看与脚本错误的监控\n\n对于前端项目而言，即使我们已经使用了 typescript、eslint 并且也有了一些测试脚本和系统测试工程师进行的功能测试，我们还是免不了会出现 js 脚本错误，特别是 js 代码的运行环境和设备的多样化，很多错误我们并没有发现，但是产品、运营同学却出现了，或者到了线上在用户设备上出现了。\n\n所以，有两个事情我们必须要做：\n\n1. 日志查看功能（手机端）：现在我们写的大多数 TO C 页面都是在手机端进行，查看 console 非常不方便，我们需要一个线上实时查看 console 的工具。\n2. 我们需要脚本错误日志统计系统来进行错误统计管理与具体错误查看。\n\n对于第一个功能，进行细分，我们需要做这样几件事情：\n\n* 嵌入一个 console 和 网络请求查看器，并且只在特殊情况下才能触发（比如连续点击标题十次、或者使用特定交互手势）\n* 在触发查看器的时候，可以将日志完整地进行上传并分析。\n* 同时可以对该用户进行染色，会自动上传并记录该用户一定时间内后续刷新后操作的全部日志。\n\n不过这里并没有完全实现以上三点的开源库推荐，可以在 [vconsole](https://github.com/Tencent/vConsole) 或者 [mlogger](https://github.com/AlloyTeam/MLogger) 的基础上进行适当扩展，完成相关功能。\n\n对于第二个功能，我们需要一个完整的统计分析与管理的错误系统，这个如果自行开发的话，难度会比较大，这里强烈推荐 [sentry](https://sentry.io/welcome/)，可以非常方便的使用 Docker 部署到服务器端，并且拥有非常强大的日志错误分析与处理能力，通过结合 JavaScript 的 sourcemap ，可以给我们的错误定位带来极大的方便。\n\n总之，日志查看与脚本错误监控，是比较重要但是同时容易被忽视的地方，很多时候，我们的系统在线上使用了几个月了，真正有问题反馈了，我们才会考虑加上这些功能，但这个时候通常已经造成了损失。\n\n### 发布：版本发布更新\n\n发布系统，一般作为前端最后环节的系统，通常会和测试部署系统打通（或合二为一），一般的发布系统的必要功能如下：\n\n* 对于前端的发布，每次只发布有改变的文件，无变动的文件则无需发布。\n* 每次发布先发布 js/css/img 等资源文件，生效之后再发布 html 文件。\n* 发布系统保留线上旧版代码，出问题后可以快速一键回滚。\n\n至于一些其他的日志、报表等辅助性功能，则根据需要满足，这里不再赘述。\n\n#### 灰度发布\n\n灰度发布是大型项目在发布时的常见方法，指在发布版本时，初始情况下，只允许小比例（比如1-5%比例的用户使用），若出现问题时，可以快速回滚使用老版本，适用于主链路和访问量较大的页面。\n\n对于前端的灰度，实际上有以下几种方案：\n\n* 在代码层面进行灰度，即通过 if/else 进行判断，这样无需发布系统关注，也可以自由配置规则、比例与白名单/黑名单。\n* 在入口层面进行灰度，比如 App 内嵌的 H5 则在客户端的对应入口进行回复，这样通常也无需发布系统关注。\n* 通过发布系统，按照比例灰度，比如我们有 10 台 webserver，如果我们先发布 1 台，这样我们的灰度比例为 10%。\n\n### 访问量实时监控\n\n最后一点，我们还需要一个访问量实时监控系统，我们上述有了错误查看与脚本监控系统，但是对于我们的各个页面的访问量、点击率等指标，通常是产品/运营同学比较关心的，同时访问量的波动情况也是项目健康度的一个表征（访问量突然大幅上涨或下跌，一般都有其特定原因），所以我们需要访问量实时监控系统。\n\n而实际上访问量监控系统也有两种不同形态：\n\n* 对于每一个上报 key，只进行数量上的统计\n* 对于每一个上报 key，可以携带一些信息，对携带信息进行统计分析。\n\n通常情况下，前者的功能是实时或者低延时的，而后者由于需要一部分统计分析，通常可以接受非实时情况（一般每天出前一天的报表）。\n\n这部分内容，需要较强的后端接口稳定性，通常前端需要和对应岗位的同学共建。\n\n### 总结\n\n总结下来，我们一个稳定的前端项目，至少涉及到以下环节：\n\n* 完善的项目脚手架与代码约束规范\n* 内部 gitlab\n* 可视化管理的测试部署系统\n* 实时日志查看工具\n* 脚本错误统计管理系统\n* 发布管理系统\n* 访问量实时监控系统\n\n如果你所在的团队哪个环节还没有或者不完善，那么这也是你的机会。\n","tags":["前端构建"]},{"title":"MySQL聚集索引和非聚集索引","url":"/2019/05/23/pout/数据库/mysql/聚集索引和非聚集索引/","content":"\n\n<!-- toc -->\n\n### MySQL聚集索引和非聚集索引\n\n**正文内容按照一个特定维度排序存储，这个特定的维度就是聚集索引；**\n\nInnodb存储引擎中行记录就是按照**聚集索引维度顺序**存储的，Innodb的表也称为**索引表**；因为行记录只能按照一个维度进行排序，所以一张表只能有一个聚集索引。\n\n**非聚集索引索引项顺序存储，但索引项对应的内容却是随机存储的；**\n\n举个例子说明下：\n\ncreate table student (\n\n`id` INT UNSIGNED AUTO_INCREMENT,\n\n`name` VARCHAR(255),\n\nPRIMARY KEY(`id`),\n\nKEY(`name`)\n\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n**该表中主键id是该表的聚集索引、name为非聚集索引；**表中的每行数据都是按照**聚集索引id排序存储的**；比如要查找name='Arla'和name='Arle'的两个同学，他们在name索引表中位置可能是相邻的，但是实际存储位置可能差的很远。**name索引表节点按照name排序，检索的是每一行数据的主键。聚集索引表按照主键id排序，检索的是每一行数据的真实内容。**\n\n也就是说查询name='Arle'的记录时，首相通过name索引表查找到Arle的主键id（可能有多个主键id，因为有重名的同学），再根据主键id的聚集索引找到相应的行记录；\n\n**聚集索引一般是表中的主键索引，如果表中没有显示指定主键，则会选择表中的第一个不允许为NULL的唯一索引，如果还是没有的话，就采用Innodb存储引擎为每行数据内置的6字节ROWID作为聚集索引。**\n\n每张表只有**一个聚集索引**，因为聚集索引在精确查找和范围查找方面良好的性能表现（相比于普通索引和全表扫描），聚集索引就显得弥足珍贵，聚集索引选择还是要慎重的（一般不会让没有语义的自增id充当聚集索引）。\n\n从宏观上分析下聚集索引和普通索引的性能差异，还是针对上述student表：\n\n（1）select * from student where id >5000 and id <20000;\n\n（2）select * from student where name > 'Alie' and name < 'John';\n\n第一条SQL语句根据id进行范围查询，因为(5000, 20000)范围内的记录在磁盘上按顺序存储，顺序读取磁盘很快就能读到这批数据。\n\n第二条SQL语句查询（'Alie', 'John'）范围内的记录，主键id分布可能是离散的1，100，20001，5000.....；增加了随机读取数据页几率；所以普通索引的范围查询效率被聚集索引甩开几条街都不止；非聚集索引的精确查询效率还是可以的，比聚集索引查询只增加了一次IO开销。\n\n","tags":["数据库"]},{"title":"MySQL底层的B树和B+数","url":"/2019/05/23/pout/数据库/mysql/MySQL底层的B树和B+数/","content":"\n\n<!-- toc -->\n\n\n### MySQL底层的B树和B+数\n\n\n\n## 01 B树\n\n### 1.1 B树概念\n\nB树也称B-树,它是一颗**多路平衡查找树**。二叉树我想大家都不陌生，其实，B树和后面讲到的B+树也是从最简单的二叉树变换而来的，并没有什么神秘的地方，下面我们来看看B树的定义。\n\n- 每个节点最多有m-1个**关键字**（可以存有的键值对）。\n- 根节点最少可以只有1个**关键字**。\n- 非根节点至少有m/2个**关键字**。\n- 每个节点中的关键字都按照**从小到大**的顺序排列，每个关键字的左子树中的所有关键字都小于它，而右子树中的所有关键字都大于它。\n- 所有叶子节点都位于同一层，或者说根节点到每个叶子节点的长度都相同。\n- 每个节点都存有索引和数据，也就是对应的key和value。\n\n所以，根节点的**关键字**数量范围：`1 <= k <= m-1`，非根节点的**关键字**数量范围：`m/2 <= k <= m-1`。\n另外，我们需要注意一个概念，描述一颗B树时**需要指定它的阶数**，阶数表示了**一个节点最多有多少个孩子节点**，一般用字母m表示阶数。\n我们再举个例子来说明一下上面的概念，比如这里有一个5阶的B树，根节点数量范围：1 <= k <= 4，非根节点数量范围：2 <= k <= 4。\n下面，我们通过一个插入的例子，讲解一下B树的插入过程，接着，再讲解一下删除关键字的过程。\n\n### 1.2 B树插入\n\n插入的时候，我们需要记住一个规则：**判断当前结点key的个数是否小于等于m-1，如果满足，直接插入即可，如果不满足，将节点的中间的key将这个节点分为左右两部分，中间的节点放到父节点中即可。**\n\n例子：在5阶B树中，结点最多有4个key,最少有2个key（注意：下面的节点统一用一个节点表示key和value）。\n\n- 插入18，70，50,40\n\n![img](https://pic1.zhimg.com/80/v2-b4f5dc1d1bf60ec3d2b431a08ecb5cbc_hd.jpg)\n\n- 插入22\n\n![img](https://pic1.zhimg.com/80/v2-c75ce532b1a0de70a46addcbeb3eaad8_hd.jpg)\n\n插入22时，发现这个节点的关键字已经大于4了，所以需要进行分裂，分裂的规则在上面已经讲了，分裂之后，如下。\n\n![img](https://pic2.zhimg.com/80/v2-af9426b0bea00785aca8233750acf059_hd.jpg)\n\n- 接着插入23，25，39\n\n![img](https://pic2.zhimg.com/80/v2-e2dce90ee182b4a9a2dfc50baf4c4b75_hd.jpg)\n\n分裂，得到下面的。\n\n![img](https://pic1.zhimg.com/80/v2-a4d98275d5295fe4b20f74af61edeb64_hd.jpg)\n\n### 1.3 B树的删除操作\n\nB树的删除操作相对于插入操作是相对复杂一些的，但是，你知道记住几种情况，一样可以很轻松的掌握的。\n\n- 现在有一个初始状态是下面这样的B树，然后进行删除操作。\n\n![img](https://pic1.zhimg.com/80/v2-d667e9bfbb439ed3aa02c6c58aa9c3cc_hd.jpg)\n\n- 删除15，这种情况是删除叶子节点的元素，如果删除之后，节点数还是大于`m/2`，这种情况只要直接删除即可。\n\n![img](https://pic2.zhimg.com/80/v2-ec6a9d20c73edccdc7399a94be2b990d_hd.jpg)\n\n![img](https://pic1.zhimg.com/80/v2-9ca16f6b7de12a09300d68425c53ecd8_hd.jpg)\n\n- 接着，我们把22删除，这种情况的规则：22是非叶子节点，**对于非叶子节点的删除，我们需要用后继key（元素）覆盖要删除的key，然后在后继key所在的子支中删除该后继key**。对于删除22，需要将后继元素24移到被删除的22所在的节点。\n\n![img](https://pic1.zhimg.com/80/v2-b42631e43d99ca679dacae61e5f6d12c_hd.jpg)\n\n![img](https://pic2.zhimg.com/80/v2-772635b378f840d16f0583995139a4bd_hd.jpg)\n\n此时发现26所在的节点只有一个元素，小于2个（m/2），这个节点不符合要求，这时候的规则（向兄弟节点借元素）：**如果删除叶子节点，如果删除元素后元素个数少于（m/2），并且它的兄弟节点的元素大于（m/2），也就是说兄弟节点的元素比最少值m/2还多，将先将父节点的元素移到该节点，然后将兄弟节点的元素再移动到父节点**。这样就满足要求了。\n\n我们看看操作过程就更加明白了。\n\n![img](https://pic2.zhimg.com/80/v2-14d47bbc49f7241ba96c24403075072d_hd.jpg)\n\n![img](https://pic4.zhimg.com/80/v2-231ddbb155462cd674c37b00370f233f_hd.jpg)\n\n删除就只有上面的几种情况，根据不同的情况进行删除即可。\n\n## 02 B+树\n\n### 2.1 B+树概述\n\nB+树其实和B树是非常相似的，我们首先看看**相同点：**\n\n- 根节点至少一个元素\n- 非根节点元素范围：m/2 <= k <= m-1\n\n**不同点：**\n\n- B+树有两种类型的节点：**内部结点**（也称**索引结点**）和**叶子结点**。内部节点就是非叶子节点，内部节点不存储数据，**只存储索引**，**数据**都存储在叶子节点。\n- 内部结点中的key都按照**从小到大的顺序排列**，对于内部结点中的一个key，左树中的所有key都小于它，右子树中的key都大于等于它。叶子结点中的记录也按照key的大小排列。\n- 每个叶子结点都存有相邻叶子结点的指针，叶子结点本身**依关键字的大小自小而大顺序链接**。\n- 父节点存有右孩子的第一个元素的索引。\n\n下面我们看一个B+树的例子，感受感受它吧！\n\n![img](https://pic2.zhimg.com/80/v2-4d76b42872b98964a33930f00307ef75_hd.jpg)\n\n### 2.2 插入操作\n\n对于插入操作很简单，只需要记住一个技巧即可：**当节点元素数量大于m-1的时候，按中间元素分裂成左右两部分，中间元素分裂到父节点当做索引存储，但是，本身中间元素还是分裂右边这一部分的**。\n\n下面以一颗5阶B+树的插入过程为例，5阶B+树的节点最少2个元素，最多4个元素。\n\n- 插入5，10，15，20\n\n![img](https://pic1.zhimg.com/80/v2-cc558b9acd550ddf81417dba5ad9f618_hd.jpg)\n\n- 插入25，此时元素数量大于4个了，分裂\n\n![img](https://pic4.zhimg.com/80/v2-b450f0e6300f84ac626874d476677e87_hd.jpg)\n\n- 接着插入26，30，继续分裂\n\n![img](https://pic2.zhimg.com/80/v2-59220cf7096626c8e62a2715bb919ded_hd.jpg)\n\n![img](https://pic2.zhimg.com/80/v2-8b1e35e12cdeb3b29af8b5f1ea8e2015_hd.jpg)\n\n有了这几个例子，相信插入操作没什么问题了，下面接着看看删除操作。\n\n### 2.3 删除操作\n\n对于删除操作是比B树简单一些的，因为**叶子节点有指针的存在，向兄弟节点借元素时，不需要通过父节点了，而是可以直接通过兄弟节移动即可（前提是兄弟节点的元素大于m/2），然后更新父节点的索引；如果兄弟节点的元素不大于m/2（兄弟节点也没有多余的元素），则将当前节点和兄弟节点合并，并且删除父节点中的key**，下面我们看看具体的实例。\n\n- 初始状态\n\n![img](https://pic3.zhimg.com/80/v2-ce2b8eff8c1b35cb01eb43891e1ac97e_hd.jpg)\n\n- 删除10，删除后，不满足要求，发现左边兄弟节点有多余的元素，所以去借元素，最后，修改父节点索引\n\n![img](https://pic1.zhimg.com/80/v2-c795a94ed5c7123dcadfe228b015f1a4_hd.jpg)\n\n- 删除元素5，发现不满足要求，并且发现左右兄弟节点都没有多余的元素，所以，可以选择和兄弟节点合并，最后修改父节点索引\n\n![img](https://pic3.zhimg.com/80/v2-ab509486fc34208bdce4eef6fc9d365e_hd.jpg)\n\n- 发现父节点索引也不满足条件，所以，需要做跟上面一步一样的操作\n\n![img](https://pic3.zhimg.com/80/v2-0338708bbaccb594a48e731d0dacdb46_hd.jpg)\n\n这样，B+树的删除操作也就完成了\n\n> \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["数据库"]},{"title":"MySql忘记密码","url":"/2019/05/23/pout/数据库/mysql/mysql忘记密码/","content":"\n\n<!-- toc -->\n\n\n# Windows\n\n```\n1. 关闭正在运行的MySQL服务\n2. 打开DOS窗口，转到mysql\\bin目录\n3. 输入mysqld --skip-grant-tables 回车       --skip-grant-tables 的意思是启动MySQL服务的时候跳过权限表认证\n4. 再开一个DOS窗口（因为刚才那个DOS窗口已经不能动了），转到mysql\\bin目录 \n5. 输入mysql回车，如果成功，将出现MySQL提示符 >\n6. 连接权限数据库： use mysql;\n6. 改密码：update user set password=password(\"root\") where user=\"root\"; \n   mysql5.7以上版本-->    UPDATE user SET authentication_string=PASSWORD(\"root\") WHERE User=\"root\";\n7. 刷新权限（必须步骤）：flush privileges;　\n8. 退出 quit。 \n9. 注销系统，再进入，使用用户名root和刚才设置的新密码123登录。\n```\n\n# Centos7\n\n1.修改MySQL的配置文件（默认为/etc/my.cnf）,在[mysqld]下添加一行skip-grant-tables \n\n```\nskip-grant-tables \n```\n\n2.保存配置文件后，重启MySQL服务 \n\n```\nservice mysqld restart（注：根据自己的情况重启mysql )\n\nerror:Redirecting to /bin/systemctl restart  mysqld.service\nservice mysqld status\n\nerror:Failed to restart mysqld.drtvice.service: Unit not found.\n请重装\n```\n\n3.再次进入MySQL命令行 mysql -uroot -p,输入密码时直接回车，就会进入MySQL数据库了，这个时候按照常规流程修改root密码即可。 \n\n```\n>use mysql;    更改数据库\n\n>UPDATE user SET PASSORD =password(\"自己重新设置的密码写此处\") WHERE USER= 'root';  重设密码\n```\n\n4.如果是 centos 7以上的；可能会失败；修改密码操作改为：\n\n```\nupdate mysql.user set authentication_string=password('自己重新设置的密码写此处') where user='root' ;\n```\n\n5.刷新MySQL的系统权限相关表，以防止更改后拒绝访问；或者重启MySQL服务器 \n\n```\nflush privileges; \n```\n\n6.密码修改完毕后，再按照步骤1中的流程，删掉配置文件中的那行，并且重启MySQL服务，新密码就生效了。 \n\n##### 后续解密\n\nERROR 1820 (HY000): Unknown error 1820\n\n```\nSET PASSWORD = PASSWORD('新密码');\n```\n\n","tags":["数据库"]},{"title":"Mysql慢查询","url":"/2019/05/23/pout/数据库/mysql/MySQL慢查询/","content":"\n\n<!-- toc -->\n\n\n## 一. 设置方法\n\n使用慢查询日志里捕获\n\n启用之前需要先进行一些设置\n\n 开启通用日志查询： set global general_log=on;\n\n关闭通用日志查询： set globalgeneral_log=off;\n\n设置通用日志输出为表方式： set globallog_output=’TABLE’;\n\n设置通用日志输出为文件方式： set globallog_output=’FILE’;\n\n设置通用日志输出为表和文件方式：set global log_output=’FILE,TABLE’;\n\n\n### 方法一：全局变量设置\n\n \n\n#### 设置慢查询日志的日志文件位置\n\n```\nset global slow_query_log_file = \"D:/slow_log/slow_log.log\" ;\n```\n\n \n\n#### 设置是否对未使用索引的SQL进行记录\n\n```\nset global log_queries_not_using_indexes = on;\n```\n\n \n\n#### 设置只要SQL执行时间超过n秒的就记录\n\n```\nset global long_query_time = 0.001 ;\n```\n\n此处设置的0.001秒，便于测试，一般情况比这个大\n\n \n\n#### 启用mysql慢查询日志\n\n```\nset global slow_query_log = on;\n```\n\n \n\n### 方法二：配置文件设置\n\n修改配置文件my.cnf，在[mysqld]下的下方加入\n\n```\n[mysqld]\nslow_query_log = ON\nlog_queries_not_using_indexes = ON;\nslow_query_log_file = /usr/local/mysql/data/slow.log\nlong_query_time = 1\n```\n\n \n\n### 查看设置后的参数\n\n```\nshow variables like 'slow_query%';\nshow variables like 'long_query__time';\n```\n\n## 二. 慢查询日志记录的内容\nmore /var/lib/mysql/izbp12xu0u9ja736dkjgcsz-slow.log\n\n```\nTime                 Id Command    Argument\n# Time: 2019-01-08T04:12:09.269315Z \n# User@Host: h5_test[h5_test] @ localhost [::1]  Id:    12  \n# Query_time: 0.000831  Lock_time: 0.000198 Rows_sent: 1  Rows_examined: 3  \nuse mc_productdb;\nSET timestamp=1546920729;\nSELECT t.customer_id,t.title,t.content \nFROM (\nSELECT customer_id  FROM product_comment WHERE  product_id =199726 AND audit_status = 1 LIMIT 0,15\n)a JOIN product_comment t \nON a.customer_id = t.comment_id;\n```\n\nTime：执行查询的日期时间\nUser@Host：执行查询的用户和客户端IP\nId：是执行查询的线程Id\nQuery_time：SQL执行所消耗的时间\nLock_time：执行查询对记录锁定的时间\nRows_sent：查询返回的行数\nRows_examined：为了返回查询的数据所读取的行数\n","tags":["数据库"]},{"title":"redis持久化","url":"/2019/05/23/pout/数据库/Redis/redis持久化/","content":"\n\n<!-- toc -->\n\n## 什么叫持久化？\n\n用一句话可以将持久化概括为：将数据（如内存中的对象）保存到可永久保存的存储设备中。持久化的主要应用是将内存中的对象存储在数据库中，或者存储在磁盘文件中、 XML 数据文件中等等。\n\n> 从应用层与系统层理解持久化\n\n同时，也可以从应用层和系统层这两个层面来理解持久化：\n\n**应用层**：如果关闭( `Close` )你的应用然后重新启动则先前的数据依然存在。\n\n**系统层**：如果关闭( `Shutdown` )你的系统（电脑）然后重新启动则先前的数据依然存在。\n\n\n\n说白了，就是在指定的时间间隔内,将内存当中的数据快照写入磁盘,它恢复时是拷快照文件直接读到内存\n\n什么意思呢?  我们都知道, 内存当中的数据, 如果我们一断电,那么数据必然会丢失,但是玩过redis的同学应该都知道,我们一关机之后再启动的时候数据是还在的,所以它必然是在redis启动的时候重新去加载了持久化的文件\n\n**redis提供两种方式进行持久化**\n\n一种是RDB持久化默认,\n\n另一种 AOF (append only file) 持久化.\n\n## Redis 为什么要持久化？\n\nRedis 中的数据类型都支持 push/pop、add/remove 及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，Redis 支持各种不同方式的排序。与 Memcached 一样，为了保证效率，数据都是缓存在内存中。\n\n对，数据都是缓存在内存中的，当你重启系统或者关闭系统后，缓存在内存中的数据都会消失殆尽，再也找不回来了。所以，为了让数据能够长期保存，就要将 Redis 放在缓存中的数据做持久化存储。\n\n## redis持久化的意义，在于故障恢复\n\n比如你部署了一个redis，作为cache缓存，当然也可以保存一些较为重要的数据\n\n如果没有持久化的话，redis遇到灾难性故障的时候，就会丢失所有的数据\n\n如果通过持久化将数据搞一份儿在磁盘上去，然后定期比如说同步和备份到一些云存储服务上去，那么就可以保证数据不丢失全部，还是可以恢复一部分数据回来的\n\n\n\n\n\n## 1.RDB\n\n**是什么？**\n\n原理是redis会单独创建(fork函数)（复制）一个与当前进程一模一样的子进程来进行持久化,这个子线程的所有数据(变量,环境变量,程序,程序计数器等)都和原进程一模一样,会先将数据写入到一个临时文件中,待持久化结束了,再用这个临时文件替换上次持久化好的文件,整个过程中,主进程不进行任何的IO操作,（用到了fork子进程来进行持久化）这就确保了极高的性能\n\n**1.这个持久化文件在哪里**\n\nvi /usr/local/redis/etc/redis.conf\n\n找dbfilename dump.rdb\n\n默认就是dump.rdb\n\ndir ./  (包括很多例如redis实例，只要是redis产生的实例都会丢到)\n\n./   =====  哪里启动，哪里生成。\n\n注意：\n\n要么写死目录\n\n要么启动的位置就在同一个地方，地方不一样可能造成数据丢失。\n\n**2.他是什么时候fork子进程，或者什么时候触发rdb持久化机制**\n\n```python\n1./usr/local/redis/bin/redis-cli\n```\n\n```python\n2.SHUTDOWN \n```\n\n<img src=\"C:\\Users\\Lenovo\\AppData\\Roaming\\Typora\\typora-user-images\\image-20191217092426326.png\" alt=\"image-20191217092426326\" style=\"zoom:50%;\" />\n\n```python\n/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf ## 开启redis\n```\n\nshutdown时,如果没有开启aof,会触发\n\n配置文件中默认的快照配置\n\n<img src=\"C:\\Users\\Lenovo\\AppData\\Roaming\\Typora\\typora-user-images\\image-20191217092743078.png\" alt=\"image-20191217092743078\" style=\"zoom:50%;\" />\n\n执行命令save成者bgsave ,\n\nsave是只管保存,其他不管,全部阻塞  \n\nredis会在后台异步进行快照操作,\n\n同时可以响应客户端的请求(默认调用bgsave来进行持久化)\n\n```python\n rm -f ./dump.rdb \n```\n\n```python\n/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf \n## 开启redis\n```\n\n```python\n/usr/local/redis/bin/redis-cli  \n## 连接客户端\n```\n\n```python\nset k1 v1\n```\n\n```python\nkeys *\n```\n\n```python\nsave     ## dump.rdb  是只管保存,其他不管,全部阻塞\n```\n\n```python\nbgsave   ## dump.rdb  redis会在后台异步进行快照操作\n```\n\n执行flushall命令但是里面是空的,无意义\n\n```python\nFLUSHALL   ## 清空\n```\n\n**在redis.conf中rdb持久化策略**\n\n集群save 900 1                ##  900秒之内执行了一次增删改操作就会触发 ， 查不算\n\nsave 300 10             \n\nsave 60    10000  \n\n默认配置\n\nredis 性能调优\n\n集群  master节点肯定会把rdb\n\n实际上关不掉的在主从复制上\n\n要么就是就写一个save\n\n要么就注掉\n\n\n\n## 2.aof\n\n执行set k1 v1 \n\n保存命令\n\n```python\nset k1 v1\nset k2 v1   \n## 保存到文件中  保存的是命令\n```\n\n**是什么？**\n\n原理是将Reids的操作日志以追加的方式写入文件,读操作是不记录的\n\n**1.这个持久化文件在哪里**\n\n产生的位置还是 ./dir \n\n```python\nvi /usr/local/redis/etc/redis.conf\n```\n\n```python\nappendfilename \"appendonly.aof\"   ## 文件名\n```\n\n redis 默认关闭，开启需要手动把no改为yes\n\n```python\nappendonly yes\n```\n\n```python\n/usr/local/redis/bin/redis-cli  \n```\n\n```python\nset k1 v1 \n```\n\n```python\nSHUTDOWN\n```\n\n```python\n/usr/local/redis/bin/redis-server /usr/local/redis/etc/redis.conf \n## 开启redis\n```\n\n```\nll\n```\n\n\n\n‘*’  开头代表下面有两组命令\n\n$  字符串的长度\n\n日志追加的方式保存到文件里。\n\n\n\n\n\n\n\n\n\n**2.触发机制（根据配置文件配置项）**\n\nAOF_FSYNC_NO:表示等操作系统进行数据缓存同步到磁盘(快,持久化没保证) -----不能配置不可控 \n\nAOF_FSYNC_ALWAYS: 同步持久化,每次发生数据变更时,立即记录到磁盘(慢,安全) ----消耗性能\n\nAOF_FSYNC_EVERYSEC: 表示每秒同步一次(默认值很快,但可能会丢失一秒以内的数据)--所以默认\n\n\n\n## 同步策略\n\n在了解同步策略之前，需要先来了解两个三方法flushAppendOnlyFile、write和save：\n\n- redis的服务器进程是一个事件循环，文件事件负责处理客户端的命令请求，而时间事件负责执行serverCron函数这样的定时运行的函数。在处理文件事件执行写命令，使得命令被追加到aof_buf中，然后在处理时间事件执行serverCron函数会调用flushAppendOnlyFile函数进行文件的写入和同步\n- write：根据条件，将aof_buf中的缓存写入到AOF文件\n- save：根据条件，调用fsync或fdatasync函数将AOF文件保存到磁盘\n\n下面来介绍Redis支持的三种同步策略：\n- AOF_FSYNC_NO：不保存（write和read命令都由主进程执行）\n- AOF_FSYNC_EVERYSEC：每一秒钟保存一次（write由主进程完成，save由子进程完成）\n- AOF_FSYNC_ALWAYS：每执行一个命令保存一次（write和read命令都由主进程执行）\n\n**AOF_FSYNC_NO**\n在这种策略下，每次flushAppendOnlyFile函数被调用的时候都会执行一次write方法，但是不会执行save方法。\n\n只有下面三种情况下才会执行save方法：\n- Redis被关闭\n- AOF功能被关闭\n- 系统的写缓存被刷新（可能是缓存已经被写满，或者定期保存操作被执行）\n\n这三种情况下的save操作都会引起Redis主进程阻塞，并且由于长时间没有执行save命令，所以save命令执行的时候，阻塞时间会很长\n\n**AOF_FSYNC_EVERYSEC**\n在这种策略下，save操作原则上每隔一秒钟就会执行一次， 因为save操作是由后台子线程调用的， 所以它不会引起服务器主进程阻塞。\n\n其实根据Redis的状态，每当 flushAppendOnlyFile函数被调用时，write命令和save命令的执行又分为四种不同情况：\n\n![AOF everysec](https://lebron-youdao-img.oss-cn-hangzhou.aliyuncs.com/AOF%20everysec.png)\n\n\n\n根据以上图知道，在AOF_FSYNC_EVERYSEC策略下， 如果在情况1时发生故障停机， 那么用户最多损失小于2秒内所产生的数据；而如果在情况2时发生故障停机，堆积了很多save命令，那么用户损失的数据是可以超过 2 秒的。\n\nAOF_FSYNC_ALWAYS\n在这种模式下，每次执行完一个命令之后，write和save命令都会被执行。\n\n另外，因为save命令是由Redis主进程执行的，所以在save命令执行期间，主进程会被阻塞。\n\n三种策略的优缺点\nAOF_FSYNC_NO策略虽然表面上看起来提升了性能，但是会存在每次save命令执行的时候相对长时间阻塞主进程的问题。并且数据的安全性的不到保证，如果Redis服务器突然宕机，那么没有从AOF缓存中保存到硬盘中的数据都会丢失。\n\nAOF_FSYNC_ALWAYS策略的安全性的到了最大的保障，理论上最多丢失最后一次写操作，但是由于每个写操作都会阻塞主进程，所以Redis主进程的响应速度受到了很大的影响。\n\nAOF_FSYNC_EVERYSEC策略是比较建议的配置，也是Redis的默认配置，相对来说兼顾安全性和性能。\n\n\n## AOF执行流程\n\n* 所有的写命令都会追加到aof_buf（缓冲区）中。\n* 可以使用不同的策略将AOF缓冲区中的命令写到AOF文件中。\n* 随着AOF文件的越来越大，会对AOF文件进行重写。\n* 当服务器重启的时候，会加载AOF文件并执行AOF文件中的命令用于恢复数据。\n\n简单分析一下AOF执行流程中的一些问题：\n\n- 因为Redis为了效率，使用单线程来响应命令，如果每次写命令都追加写硬盘的操作，那么Redis的响应速度还要取决于硬盘的IO效率，显然不现实，所以Redis将写命令先写到AOF缓冲区。\n\n- 写道缓冲区还有一个好处是可以采用不同的策略来实现缓冲区到硬盘的同步，可以让用户自行在安全性和性能方面做出权衡。\n  \n\n## 3.aof重写机制\n\n我们以日志的方式,记录我们的命令记录到文件当中\n\n如果我操作的特别频繁，文件肯定会特别大。\n\n我写100万数据，持久化文件也会特别大\n\n```\n/usr/local/redis/bin/redis-cli\n```\n\n```python\nFLUSHALL  ## 清空数据\n```\n\n```python\nkeys *   ## 查看是否有数据\n```\n\n```python\nINCR lock   ## 加操作\n```\n\n```python\nexit    ## 退出\n```\n\n```python\nvi appendonly.aof  ## 查看文件\n```\n\n记录着\n\n我可以直接执行一条命令set lock 11\n\n重写就是将重复的剔除掉瘦身\n\n```python\n/usr/local/redis/bin/redis-cli\n```\n\n```python\nBGREWRITEAOF  ##手动调用重写命令\n```\n\n```python\nexit  ## 退出\n```\n\n```python\nvi dump.rdb   \n```\n\n是因为我这个版本是5.0的\n\n当AOF文件增长到一定大小的时候Redis能够调用bgrewriteaof对日志文件进行重写。当AOF文件大小的增长率大于该配置项时自动开启重写(这里指超过原大小的100%) .\n\nauto-aof-rewrite-percentage 100\n\n当AOF文件增长到一定大小的时候Redis能够调用bgrewriteaof对日志文件进行重写,当AOF文件大小大于该配置项时自动开启重写\n\nauto-aof-rewrite-min-size 64mb\n\nincr lock \n\n重写就是将重复的剔除掉瘦身\n\n## 4.redis4.0后混合持久化机制\n\n**开启混合持久化**\n\n4.0 版本是 像set lock 11\n\n4.0版本的混合持久化默认关闭的,通过aof-use-rdb-preamble配置参数控制, yes则表示开启, no表示禁用, 5.0之后默认开启。\n\n混合持久化是通过bgrewriteaof完成的,不同的是当开启混合持久化时, fork出的子进程先将共享的内存副本全量 以RDB方式写入aof文件,然后在将重写缓冲区的增量命令以AOF方式写入到文件,写入完成后通知主进程更新统计信息,并将新的含有RDB格式和AOF格式的AOF文件替换旧的AOF文件。\n\n简单的说:新的AOF文件前半段是 RDB格式的全量数据后半段是AOF格式的增量数据,\n\n优点:混合持久化结合了RDB持久化和AOF持久化的优点由于绝大部分都是RDB格式,加载速度快,同时结合 AOF,增量的数据以AOF方式保存了,数据更少的丢失.\n\n缺点:兼容性差,一旦开启了混合持久化,在4.0之前版本都不识别该aof文件,同时由于前部分是RDB格式,阅读性较差\n\n二进制方式存储  更小\n\n什么时候下会自动重写\n\n看\n\nauto-aof-rewrite-percentage\n\nauto-aof-rewrite-min-size\n\n\n\n## 小总结：\n\n**1.redis提供了rdb持久化方案，为什么还要aof？**\n\nrdb 是跟据save触发持久化，所以会照成数据的丢失\n\naof持久化是1秒执行一次\n\n在数据aof\n\n在性能rdb高于aof\n\n> 优化数据丢失问题，rdb会丢失最后一次快照后的数据，aof丢失不会超过2秒的数据\n\n**2.如果aof和rdb同时存在，听谁的？**\n\naof数据准确率更高\n\n**3.rdb和aof优势劣势**\n\nrdb适合大规模的数据恢复,对数据完整性和一致性不高，在一定间隔时间做一次备份,如果redis意外宕机的话,就会丢失最后一次快照后的所有操作\n\naof根据配置项而定\n\n1.官方建议两种持久化机制同时开启,如果两个同时开启优先使用aof久化机制\n\n在生产环境中用集群，哨兵什么的\n\n主机开aof\n\n**性能建议（这里只针对单机版redis持久化做性能建议）：**\n\n因为RDB文件只用作后备用途,只要15分钟备份一次就够了,只保留save 900 1这条规则. 因为开启aof持久化安全。\n\n如果Enalbe AOF, 好处是在最恶劣情况下也只会丢失不超过两秒数据,启动脚本较简单只load自己的AOF文件就可以了.\n\n代价一是带来了持续的IO,二是AOF rewrite的最后将rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。\n\n只要硬盘许可,应该尽量减少AOF rewrite的频率, AOF重写的基础大小默认值64M太小了,可以设到5G以上.默认超过原大小100%大小时重写可以改到适当的数值。\n\n看硬盘\n\n\n\n<img src=\"C:\\Users\\Lenovo\\Desktop\\20191127113644133169.png\" alt=\"20191127113644133169\" style=\"zoom: 50%;\" />\n\n\n\n1）AOF持久化开启且存在AOF文件时，优先加载AOF文件。\n\n2）AOF关闭或者AOF文件不存在时，加载RDB文件。\n\n3）加载AOF/RDB文件成功后，Redis启动成功。\n\n4）AOF/RDB文件存在错误时，Redis启动失败并打印错误信息。\n\n\n\n![img](https://ss2.bdstatic.com/70cFvnSh_Q1YnxGkpoWK1HF6hhy/it/u=762237719,3005011713&fm=15&gp=0.jpg)\n\n1.从主进程中fork出子进程，并拿到fork时的AOF文件数据写到一个临时AOF文件中\n\n2.在重写过程中，redis收到的命令会同时写到AOF缓冲区和重写缓冲区中，这样保证重写不丢失，重写过程中的命令\n\n3.重写完成后通知主进程，主进程会将AOF缓冲区中的数据追加到子进程生成的文件中\n\n4.redis会原子的将旧文件替换为新文件，并开始将数据写入到新的aof文件上\n\n\n\n* 执行bgrewriteaof命令的时候，如果当前有进程正在执行AOF重写，那么直接返回；如果有进程正在执行bgsave，那么等待bgsave执行完毕再执行AOF重\n* Redis主进程会fork一个子进程执行AOF重写。\n* AOF重写过程中，不影响Redis原有的AOF过程，包括写消息到AOF缓存以及同步AOF缓存中的数据到硬盘。\n* AOF重写过程中，主进程收到的写操作还会将命令写到AOF重写缓冲区，注意和AOF缓冲区分开。\n* 由于AOF重写过程中原AOF文件还在陆续写入数据，所以AOF重写子进程只会拿到fork子进程时的AOF文件进行重写。\n* 子进程拿到原AOF文件中的数据写道一个临时的AOF文件中。\n* 子进程完成AOF重写后会发消息给主进程，主进程会把AOF重写缓冲区中的数据写道AOF缓冲区，并且用新的AOF文件替换旧的AOF文件。\n  \n\n\n\n## 总结\n\n1. Redis 默认开启RDB持久化方式，在指定的时间间隔内，执行指定次数的写操作，则将内存中的数据写入到磁盘中。\n2. RDB 持久化适合大规模的数据恢复但它的数据一致性和完整性较差。\n3. Redis 需要手动开启AOF持久化方式，默认是每秒将写操作日志追加到AOF文件中。\n4. AOF 的数据完整性比RDB高，但记录内容多了，会影响数据恢复的效率。\n5. Redis 针对 AOF文件大的问题，提供重写的瘦身机制。\n6. 若只打算用Redis 做缓存，可以关闭持久化。\n7. 若打算使用Redis 的持久化。建议RDB和AOF都开启。其实RDB更适合做数据的备份，留一后手。AOF出问题了，还有RDB。\n\n\n\n#### RDB和AOF的优缺点\n\n------\n\n \n\nRDB的优缺点：\n\n- 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数 据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。\n- 相对于 AOF 持久化机制来说，直接基于 RDB 数据文件来重启和恢复 redis 进程，更加快速。\n- RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis 保持高性能，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。\n- 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。\n- 如果想要在 redis 故障时，尽可能少的丢失数据，那么 RDB 没有 AOF 好。一般来说，RDB 数据快照文件，都是每隔 5 分钟，或者更长时间生成一次，这个时候就得接受一旦 redis 进程宕机，那么会丢失最近 5 分钟的数据。\n- RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。\n\n \n\nAOF的优缺点：\n\n- AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次fsync操作，最多丢失 1 秒钟的数据。\n- AOF 日志文件以append-only模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损。 如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据 一致性的问题。\n- AOF 日志文件即使过大的时候，出现后台重写操作，也不会影响客户端的读写。因为在rewrite log 的时候，会对其中的指令进行压缩，创建出一份需要恢复数据的最小日志出来。在创建新日志文件的时候，老的日志文件还是照常写入。当新的 merge 后的日志文件 ready 的时候，再交换新老日志文件即可。 因此在进行rewrite切换时可以更好的保证数据安全性。\n- AOF以一个格式清晰、易于理解的日志文件用于记录所有的修改操作， 非常适合做灾难性的误删除的紧急恢复。 比如有人不小心用flushall命令清空了所有数据，只要这个时候后台rewrite还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条flushall命令给删了，然后再将该aof文件放回去，就可以通过恢复机制，自动恢复所有数据。\n- 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。\n- AOF 开启后，支持的写 QPS 会比 RDB 支持的写 QPS 低， 因为 AOF 一般会配置成每秒 fsync 一次日志文件。\n- 类似 AOF 这种较为复杂的基于命令日志 / merge / 回放的方式，比基于 RDB 每次持久化一份完整的数据快照文件的方式，更加脆弱一些，容易有 bug。\n\n#### 如何选择？\n\n------\n\n \n\nRDB和AOF如何选择？\n\n-  不要仅仅使用 RDB，因为那样会导致你丢失很多数据；\n- 也不要仅仅使用 AOF，因为那样有两个问题：第一，你通过 AOF 做冷备，没有 RDB 做冷备来的恢复速度更快；第二，RDB 每次简单粗暴生成数据快照，更加健壮，可以避免 AOF 这种复杂的备份和恢复机制的 bug；\n- redis 支持同时开启开启两种持久化方式，我们可以综合使用 AOF 和 RDB 两种持久化机制，用 AOF 来保证数据不丢失，作为数据恢复的第一选择; 用 RDB 来做不同程度的冷备，在 AOF 文件都丢失或损坏不可用的时候，还可以使用 RDB 来进行快速的数据恢复。\n","tags":["数据库"]},{"title":"redis缓存雪崩","url":"/2019/05/20/pout/数据库/Redis/redis雪崩/","content":"\n\n<!-- toc -->\n\n\n# 缓存雪崩\n\n> 缓存雪崩通俗简单的理解就是：由于原有缓存失效（或者数据未加载到缓存中），新缓存未到期间（缓存正常从Redis中获取，如下图）所有原本应该访问缓存的请求都去查询数据库了，而对数据库CPU和内存造成巨大压力，严重的会造成数据库宕机，造成系统的崩溃。\n>\n> ![redis-caching-avalanche](https://github.com/doocs/advanced-java/raw/master/images/redis-caching-avalanche.png)\n\n* 解决方法：\n  * 事前：redis 高可用，主从+哨兵，redis cluster，避免全盘崩溃。\n  * 事中：本地 ehcache 缓存 + hystrix 限流&降级，避免 MySQL 被打死。\n  * 事后：redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。\n\n![redis-caching-avalanche-solution](https://github.com/doocs/advanced-java/raw/master/images/redis-caching-avalanche-solution.png)\n\n\n\n### 好处：\n\n- 数据库绝对不会死，限流组件确保了每秒只有多少个请求能通过。\n- 只要数据库不死，就是说，对用户来说，2/5 的请求都是可以被处理的。\n- 只要有 2/5 的请求可以被处理，就意味着你的系统没死，对用户来说，可能就是点击几次刷不出来页面，但是多点几次，就可以刷出来一次。\n\n# 缓存穿透\n\n> 缓存穿透是指查询一个一定**不存在的数据**。由于缓存不命中，并且出于容错考虑，如果从**数据库查不到数据则不写入缓存**，这将导致这个不存在的数据**每次请求都要到数据库去查询**，失去了缓存的意义。\n\n![redis-caching-penetration](https://github.com/doocs/advanced-java/raw/master/images/redis-caching-penetration.png)\n\n对于系统A，假设一秒 5000 个请求，结果其中 4000 个请求是黑客发出的恶意攻击。\n\n黑客发出的那 4000 个攻击，缓存中查不到，每次你去数据库里查，也查不到。\n\n举个栗子。数据库 id 是从 1 开始的，结果黑客发过来的请求 id 全部都是负数。这样的话，缓存中不会有，请求每次都“视缓存于无物”，直接查询数据库。这种恶意攻击场景的缓存穿透就会直接把数据库给打死。\n\n解决方式很简单，每次系统 A 从数据库中只要没查到，就写一个空值到缓存里去，比如 `set -999 UNKNOWN`。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。\n\n# 缓存击穿\n\n> ​     缓存击穿，是指一个key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就像在一个屏障上凿开了一个洞。\n\n* 解决方法：\n  * 可以将热点数据设置为永远不过期；或者基于redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其他请求才能通过该key访问数据。\n","tags":["数据库"]},{"title":"在阿里云Centos上配置nginx+uwsgi+负载均衡配置","url":"/2019/05/13/pout/运维/Centos上配置nginx+uwsgi+负载均衡配置/","content":"\n\n<!-- toc -->\n\n# 在阿里云Centos上配置nginx+uwsgi+负载均衡配置\n\n\n\n​    负载均衡在服务端开发中算是一个比较重要的特性。因为Nginx除了作为常规的Web服务器外，还会被大规模的用于反向代理后端，Nginx的异步框架可以处理很大的并发请求，把这些并发请求hold住之后就可以分发给后台服务端(backend servers, 后面简称backend)来做复杂的计算、处理和响应，并且在业务量增加的时候可以方便地扩容后台服务器。\n\n​    说白了就是，随着业务和用户规模的增长，仅仅一台服务器无法肩负起高并发的响应，所以需要两台以上的服务器共同分担压力，而分担压力的媒介就是万能的Nginx。\n\n​    ![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190517/20190517162157_34533.jpg)\n\n​    首先，利用wsgi在不同的端口上起两个Django服务，比如8002和8003\n\n​    然后修改nginx网站配置 vim /etc/nginx/conf.d/default.conf，将原uwsgi_pass注释，改成变量绑定\n\n​    \n\n```\nserver {\n    listen       80;\n    server_name  localhost;\n\n    access_log      /root/myweb_access.log;\n    error_log       /root/myweb_error.log;\n\n\n    client_max_body_size 75M;\n\n\n    location / {\n        include uwsgi_params;\n        #uwsgi_pass 127.0.0.1:8000;\n        uwsgi_pass mytest;\n        uwsgi_param UWSGI_SCRIPT mypro.wsgi;\n        uwsgi_param UWSGI_CHDIR  /root/mypro;\n\n    }\n\n    location /static {\n        alias /root/mypro/static;\n    }\n}\n```\n\n```\n\n```\n\n然后修改主配置文件 vim /etc/nginx/nginx.conf，在http配置内添加负载均衡配置\n\n```\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n\n    log_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                      '$status $body_bytes_sent \"$http_referer\" '\n                      '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n\n    access_log  /var/log/nginx/access.log  main;\n\n    sendfile        on;\n    #tcp_nopush     on;\n\n    keepalive_timeout  65;\n\n    #gzip  on;\n\n    include /etc/nginx/conf.d/*.conf;\n\n\n    upstream mytest {\n    server 127.0.0.1:8002;  #负载均衡服务器群\n    server 127.0.0.1:8003;\n\t}\n}\n```\n\n然后重启服务即可：\n\n```\nsystemctl restart nginx.service\n```\n\n```\n \n```\n\n```\n值得注意的是常用的负载均衡策略有以下几种：\n```\n\n```\n1、轮询（默认）\n每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。\n\nupstream backserver {\n    server 192.168.0.14;\n    server 192.168.0.15;\n}\n\n\n2、权重 weight\n指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。\n\nupstream backserver {\n    server 192.168.0.14 weight=3;\n    server 192.168.0.15 weight=7;\n}\n\n\n3、ip_hash（ IP绑定）\n上述方式存在一个问题就是说，在负载均衡系统中，假如用户在某台服务器上登录了，那么该用户第二次请求的时候，因为我们是负载均衡系统，每次请求都会重新定位到服务器集群中的某一个，那么已经登录某一个服务器的用户再重新定位到另一个服务器，其登录信息将会丢失，这样显然是不妥的。\n\n我们可以采用ip_hash指令解决这个问题，如果客户已经访问了某个服务器，当用户再次访问时，会将该请求通过哈希算法，自动定位到该服务器。\n\n每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。\n\nupstream backserver {\n    ip_hash;\n    server 192.168.0.14:88;\n    server 192.168.0.15:80;\n}\n\n\n4、fair（第三方插件）\n按后端服务器的响应时间来分配请求，响应时间短的优先分配。\n\nupstream backserver {\n    server server1;\n    server server2;\n    fair;\n}\n\n\n5、url_hash（第三方插件）\n按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。\n\nupstream backserver {\n    server squid1:3128;\n    server squid2:3128;\n    hash $request_uri;\n    hash_method crc32;\n}\n\n\n\t\n```\n\n \n\n","tags":["运维"]},{"title":"深拷备浅拷备","url":"/2019/05/02/pout/python中高级面试题/深拷备浅拷备/","content":"\n\n<!-- toc -->\n\n# 深拷贝和浅拷贝之间的区别是什么？\n\n深拷贝就是将一个对象拷贝到另一个对象中，这意味着如果你对一个对象的拷贝做出改变时，不会影响原对象。在Python中，我们使用函数deepcopy()执行深拷贝\n\n```\nimport copy\nb=copy.deepcopy(a)\n```\n\n![avatar](../img/deepcopy.jpg)\n\n而浅拷贝则是将一个对象的引用拷贝到另一个对象上，所以如果我们在拷贝中改动，会影响到原对象。我们使用函数function()执行浅拷贝\n\n```\nb=copy.copy(a)\n```\n\n![img](../img/copy.jpg)\n","tags":["python中高级面试题"]},{"title":"爬虫自动化","url":"/2019/05/01/pout/爬虫/自动化爬虫/","content":"\n\n<!-- toc -->\n\n\n# 爬虫自动化\n\n## 自动化对比\n\n| 模块   | UIAutomation | appium       | selenium | pyppeteer         | 终极武器(chrome-devtools-protocol) |\n| ------ | ------------ | ------------ | -------- | ----------------- | ---------------------------------- |\n| 跨平台 | 否           | Android，ios | 是       | chrome 浏览器就行 | 是                                 |\n| 缺点   | 无法跨平台   | adb驱动      | 太强大   | chrome 协议       | 底层                               |\n| driver | google原生   | UIAutomation | #很多    | chrome            | all                                |\n\n# 很多：\n\n1. selenium可支持的PC浏览器驱动包括：\n\n   safari driver【包含在selenium server中】\n\n   ie driver\n\n   chrome driver 【第三方】\n\n   opera driver【第三方]\n\n2. selenium可支持的伪浏览器驱动：\n\n   PhantomJS Driver【第三方,停止更新】\n\n   HtmlUnit Driver【包含在selenium server中】\n\n3. selenium可支持的移动端驱动：\n\n   Windows Phone driver\n\n   Selendroid-Selenium for Android【第三方】\n\n   ios-driver【第三方】\n\n   Appium 支持iphone、ipad、android、FirefoxOS【第三方】\n\n## 驱动代码\n\n- ### UIAutomation\n\n```\npython 端安装 uiautomator2 client：\npip install uiautomator2\nAndroid端安装 ATX server\n安装方法\n离线\n    将init_machine.sh,及android_package.zip上传至/data/local/tmp目录下\n    给手机安装termux，此为命令行工具，打开软件\n    输入su,切换至管理员账户\n    输入cd data/local/tmp,因为云手机不能打/符，所有用tab键来凑齐。\n    输入sh init_machine.sh,即可安装好软件和服务\n    \nusb线连\n    在电脑上输入python -m uiautomator2 init即可\n    ### 2、打开ATX\n    打开ATX 点击\"启动uiautomator\"\n    ### 3、打开QpythonL\n    一定要先打开这个软件，不然你python脚本会上传不成功\n```\n\n```\nimport uiautomator2 as u2\n\n# 0.0.0.0 为本机，127.0.0.1这个也行，这个连接是在手机上运行的\nd = u2.connect('http://0.0.0.0:7912')\n# 如果远程操作，就填入手机ATX显示的ip，下面有图\n# d = u2.connect('http://172.17.2.237:7912')\n# 字符串为包名，下面有获取包名教程\nsess = d.session(\"com.tencent.wework\")\n\ndef my_click(sess, ele, timeout=3):\n    ele.wait(timeout=timeout) # 等待超时，因为可能此时界面动画，或者反映迟钝，该控件还没出现\n    x, y = ele.center() # 找到该空间的中间坐标\n    sess.touch.down(x, y) # 在此坐标按下\n    time.sleep(0.3)\n    sess.touch.up() #等待3秒后抬起\n\n\ndef click_search_btn(sess):\n    ele = sess(resourceId=package_name+\":id/e3g\", #找到该空间的id\n               className=\"android.widget.TextView\",# 找到该空间的类名\n               instance=0) #如果根据属性查找有多个控件，则找第一个\n    my_click(sess, ele, 20)\n\n\nclick_search_btn(sess)\nprint('begin to input')\nele = sess(resourceId=\"com.tencent.wework:id/dpu\", className=\"android.widget.EditText\")\nele.set_text(phone) # 将手机号填入\nprint('end to input')\ntime.sleep(2)\n# print(u\"点击搜索\")\n# 点击搜索\nsess(resourceId=\"com.tencent.wework:id/azq\",\n           className=\"android.widget.RelativeLayout\").child(className=\"android.widget.TextView\").click() # 因为该控件没有id，所以先找它爹，再找它爹的儿子控件，.click()直接触发点击事件。\nele1 = sess(resourceId=\"com.tencent.wework:id/sa\",\n               className=\"android.widget.TextView\")\nele1.get_text() # 获取标签文本内容\n```\n\n- ### Seleinum\n\n```\n安装：\npip install selenium\n自行下载需要驱动的driver：如chromedriver\n实用executable_path 引入driver路径\n```\n\n```\nfrom selenium import webdriver\nimport time\n \ndrivers = ['HtmlUnit', 'PhantomJS', 'Chrome', 'FF', 'IE'] \n \ndervers_time = {\n    'HtmlUnit' : 0,\n    'PhantomJS' : 0,\n    'Chrome' : 0,\n    'FF' : 0,\n    'IE' : 0,\n}\ntimes = 50\ndef run_with_Chrome():\n    common_step(webdriver.Chrome())\n \ndef run_with_FF():\n    common_step(webdriver.Firefox())\n    \ndef run_with_IE():\n    common_step(webdriver.Ie())\n \ndef run_with_PhantomJS():\n    common_step(webdriver.PhantomJS(executable_path=r'C:\\Python27\\Scripts\\phantomjs.exe'))\n    \ndef run_with_HtmlUnit():\n    driver = webdriver.Remote(\"http://localhost:4444/wd/hub\", \n                                desired_capabilities=webdriver.DesiredCapabilities.HTMLUNIT)\n    common_step(driver)\n    \ndef common_step(driver):\n    driver.get('http://www.baidu.com')\n    ele = driver.find_element_by_id('su')\n    print ele.get_attribute('value')\n    driver.quit()\n```\n\n- ### Appium\n\n```\n1.node.js 安装\nbrew install node\nnpm install -g appium  # get appium     \nnpm install wd         # get appium client\n\n直接用npm下载往往不成功，这是需要通过代理来下载\n具体方法如下：\nnpm i cnpm -g --registry=http://registry.npm.taobao.org\ncnpm i appium -g       # get appium\ncnpm i wd -g　　       # get appium client\ncnpm i appium-doctor 　# get appium-doctor\n\n2.检查Appium成功安装\nappium-doctor\n更具提示配置 xcode 和 Android SDK\n\n3.下载appium客户端更加方便地址 github\n4.python client 安装 pip install Appium_Python_Client\n5.启动appium server .用客户端appium 启动即可\n```\n\n```\n# encoding: utf-8\n\"\"\"\n--------------------------------------\n@describe 自动化微信添加好友\n@version: 1.0\n@project: operator_spider\n@file: app_chrome.py\n@author: yuanlang \n@time: 2019-02-19 10:14\n---------------------------------------\n\"\"\"\nimport unittest\nfrom time import sleep\n\nfrom appium import webdriver\n\n\nclass MyTestCase(unittest.TestCase):\n\n    def setUp(self):\n        capabilities = {\n            'platformName': 'Android',\n            'platformVersion': \"5.1\",\n            'deviceName': \"mx4\",\n            'appPackage': 'com.tencent.mm',\n            'appActivity': '.ui.LauncherUI',\n            'automationName': 'Uiautomator2',\n            'unicodeKeyboard': True,\n            'resetKeyboard': True,\n            'noReset': True,\n            'chromeOptions': {'androidProcess': 'com.tencent.mm:tools'},\n        }\n        self.driver = webdriver.Remote(\"http://127.0.0.1:4723/wd/hub\", capabilities)\n        self.driver.implicitly_wait(30)\n\n        sleep(5)\n\n    def test_chromeApp(self):\n        # 点击加号\n        print(\"search\")\n        driver = self.driver\n        el1 = driver.find_element_by_id(\"com.tencent.mm:id/gd\")\n        el1.click()\n        sleep(5)\n        el2 = driver.find_element_by_xpath(\n            \"/hierarchy/android.widget.FrameLayout/android.widget.ListView/android.widget.LinearLayout[2]/android.widget.LinearLayout/android.widget.TextView\")\n        el2.click()\n        sleep(5)\n        el3 = driver.find_element_by_id(\"com.tencent.mm:id/hx\")\n        el3.click()\n        sleep(5)\n        driver.find_element_by_id(\"com.tencent.mm:id/hx\").send_keys(\"15775691981\")\n        sleep(5)\n        driver.find_element_by_id(\"com.tencent.mm:id/l4\").click()\n        sleep(5)\n        print(driver.context)\n        name = driver.find_element_by_id(\"com.tencent.mm:id/ang\").text\n        print(name)\n\n    def tearDown(self):\n        self.driver.close_app()\n        self.driver.quit()\n\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n## pyppeteer\n\n```\n原生是node.js 开发基于chrome-devtools-protocol + websocket\npython也有对应改写版\n项目由google维护\n\npip install pyppeteer\n```\n\n```\n# encoding: utf-8\n\"\"\"\n--------------------------------------\n@describe \n@version: 1.0\n@project: operator_spider\n@file: dddd.py\n@author: yuanlang \n@time: 2019-02-21 19:11\n---------------------------------------\n\"\"\"\n\nimport asyncio\nimport pyppeteer\nfrom collections import namedtuple\n\nResponse = namedtuple(\"rs\", \"title url html cookies headers history status\")\n\n\nasync def get_html(url, timeout=30):\n    # 默认30s\n    #,executablePath=\"\"\n    browser = await pyppeteer.launch(headless=False, args=['--no-sandbox'])\n    page = await  browser.newPage()\n    js=\"\"\"\n    function sniffDetector() {\n    const userAgent = window.navigator.userAgent;\n    const platform = window.navigator.platform;\n\n    window.navigator.__defineGetter__('userAgent', function() {\n      window.navigator.sniffed = true;\n      return userAgent;\n    });\n\n    window.navigator.__defineGetter__('platform', function() {\n      window.navigator.sniffed = true;\n      return platform;\n    });\n    //自动化反反爬虫，反自动化检测\n    Object.defineProperty(navigator, 'webdriver', {\n        get: () => false,\n    });\n  }\n    \"\"\"\n    await page.evaluateOnNewDocument(js)\n    res = await page.goto(url, options={'timeout': int(timeout * 1000)})\n    await asyncio.sleep(5)\n    data = await page.content()\n    title = await page.title()\n    resp_cookies = await page.cookies()\n    resp_headers = res.headers\n    resp_history = None\n    resp_status = res.status\n    response = Response(title=title, url=url,\n                        html=data,\n                        cookies=resp_cookies,\n                        headers=resp_headers,\n                        history=resp_history,\n                        status=resp_status)\n    return response\n\n\nif __name__ == '__main__':\n    url_list = [\"https://jx.ac.10086.cn\"]\n    task = (get_html(url) for url in url_list)\n\n    loop = asyncio.get_event_loop()\n    results = loop.run_until_complete(asyncio.gather(*task))\n    for res in results:\n        print(res)\n```\n\n## chrome-devtools-protocol\n\n```\n底层采用websocket 协议控制浏览器。[协议连接](https://chromedevtools.github.io/devtools-protocol/)\n安装\n1.pip install pychrom\n2./Chromium --remote-debugging-port=9222\n```\n\n```\nimport pychrome\n\nbrowser = pychrome.Browser(url=\"http://127.0.0.1:9222\")\ntab = browser.new_tab()\n\ndef request_will_be_sent(**kwargs):\n    print(\"loading: %s\" % kwargs.get('request').get('url'))\n\n\ntab.set_listener(\"Network.requestWillBeSent\", request_will_be_sent)\n\ntab.start()\ntab.call_method(\"Network.enable\")\ntab.call_method(\"Page.navigate\", url=\"https://jx.ac.10086.cn\", _timeout=5)\nimport time\n# print(\"ok\")\ntime.sleep(5)\nresult=tab.call_method(\"Runtime.evaluate\",expression=\"document.documentElement.outerHTML\")\nprint(result)\ntab.stop()\n```\n\n# 自动化反反爬虫\n\n```\nObject.defineProperty(navigator, 'webdriver', {\n        get: () => false,\n});\n```\n\n```\n'''\n// overwrite the `languages` property to use a custom getter\nObject.defineProperty(navigator, \"languages\", {\n  get: function() {\n    return [\"zh-CN\",\"zh\",\"zh-TW\",\"en-US\",\"en\"];\n  }\n});\n\n// Overwrite the `plugins` property to use a custom getter.\nObject.defineProperty(navigator, 'plugins', {\n  get: () => [1, 2, 3, 4, 5],\n});\n\n// Pass the Webdriver test\nObject.defineProperty(navigator, 'webdriver', {\n  get: () => false,\n});\n\n\n// Pass the Chrome Test.\n// We can mock this in as much depth as we need for the test.\nwindow.navigator.chrome = {\n  runtime: {},\n  // etc.\n};\n\n// Pass the Permissions Test.\nconst originalQuery = window.navigator.permissions.query;\nwindow.navigator.permissions.query = (parameters) => (\n  parameters.name === 'notifications' ?\n    Promise.resolve({ state: Notification.permission }) :\n    originalQuery(parameters)\n);\n'''\n# 修改chromedriver\n$ hexedit chromedriver \n\n        # 操作\n        1. tab 跳转到string栏\n        2. ctrl+S 查找 var key = '$cdc_asdjflasutopfhvcZLmcfl_'（对于2.40版本）\n        3. 替换'$cdc_asdjflasutopfhvcZLmcfl_'为任意值\n        4. ctrl+X 保存\n\n# 移动chromedriver 到 /usr/bin\n$ mv chromedriver /usr/bin\n\n[详细连接](\"https://zhuanlan.zhihu.com/p/43581988?utm_source=wechat_session&utm_medium=social&utm_oi=32546582691840&from=groupmessage\")\n```\n\n总结：seleium 是采用webdriver 协议 ,seleium server端也是使用chrome-devtools-protocol。只不过为了跨平台采用webdriver restful api 形式开发。chrome-devtools-protocol 可以使用 ./chromium --remote-debug 自动化测试\n","tags":["爬虫"]},{"title":"八大排序","url":"/2019/04/22/pout/算法/八大排序/","content":"\n\n<!-- toc -->\n\n## 插入排序\n\n插入排序：插入排序的基本操作就是将一个数据插入到已经排好序的有序数据中，从而得到一个新的、个数加一的有序数据，算法适用于少量数据的排序；首先将第一个作为已经排好序的，然后每次从后的取出插入到前面并排序；\n\n时间复杂度：O(n²)\n\n空间复杂度：O(1)\n\n稳定性：稳定\n\n```\ndef insert_sort(ilist):\n    for i in range(len(ilist)):\n        for j in range(i):\n            if ilist[i] < ilist[j]:\n                ilist.insert(j, ilist.pop(i))\n                break\n    return ilist\n```\n\n## 冒泡排序\n\n冒泡排序：它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成\n\n时间复杂度：O(n²)\n\n空间复杂度：O(1)\n\n稳定性：稳定\n\n```\ndef bubble_sort(blist):\n    count = len(blist)\n    for i in range(0, count):\n        for j in range(i + 1, count):\n            if blist[i] > blist[j]:\n                blist[i], blist[j] = blist[j], blist[i]\n    return blist\n\nblist = bubble_sort([4,5,6,7,3,2,6,9,8])\nprint(blist)\n```\n\n## 快排\n\n快速排序：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列\n\n时间复杂度：O(nlog₂n)\n\n空间复杂度：O(nlog₂n)\n\n稳定性：不稳定\n\n```\ndef quick_sort(qlist):\n    if qlist == []:\n        return []\n    else:\n        qfirst = qlist[0]\n        qless = quick_sort([l for l in qlist[1:] if l < qfirst])\n        qmore = quick_sort([m for m in qlist[1:] if m >= qfirst])\n        return qless + [qfirst] + qmore\n\nqlist = quick_sort([4,5,6,7,3,2,6,9,8])\n```\n\n## 选择排序\n\n选择排序：第1趟，在待排序记录r1 ~ r[n]中选出最小的记录，将它与r1交换；第2趟，在待排序记录r2 ~ r[n]中选出最小的记录，将它与r2交换；以此类推，第i趟在待排序记录r[i] ~ r[n]中选出最小的记录，将它与r[i]交换，使有序序列不断增长直到全部排序完毕\n\n时间复杂度：O(n²)\n\n空间复杂度：O(1)\n\n稳定性：不稳定\n\n```\ndef select_sort(slist):\n    for i in range(len(slist)):\n        x = i\n        for j in range(i, len(slist)):\n            if slist[j] < slist[x]:\n                x = j\n        slist[i], slist[x] = slist[x], slist[i]\n    return slist\n\nslist = select_sort([4,5,6,7,3,2,6,9,8])\n```\n\n## 归并排序\n\n归并排序：采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并\n\n时间复杂度：O(nlog₂n)\n\n空间复杂度：O(1)\n\n稳定性：稳定\n\n```\ndef merge_sort(array):\n    def merge_arr(arr_l, arr_r):\n        array = []\n        while len(arr_l) and len(arr_r):\n            if arr_l[0] <= arr_r[0]:\n                array.append(arr_l.pop(0))\n            elif arr_l[0] > arr_r[0]:\n                array.append(arr_r.pop(0))\n        if len(arr_l) != 0:\n            array += arr_l\n        elif len(arr_r) != 0:\n            array += arr_r\n        return array\n\n    def recursive(array):\n        if len(array) == 1:\n            return array\n        mid = len(array) // 2\n        arr_l = recursive(array[:mid])\n        arr_r = recursive(array[mid:])\n        return merge_arr(arr_l, arr_r)\n\n    return recursive(array)\n```\n","tags":["算法"]},{"title":"Scrapy爬取Boss直聘","url":"/2019/02/25/pout/爬虫/scrapy框架实例爬取Boss/","content":"\n\n<!-- toc -->\n\n\n# 爬虫项目——Scrapy爬取Boss直聘\n\n### Scrapy添加代理爬取boss直聘，并存储到mongodb\n\n- [最终爬取截图]\n\n- [项目创建]\n\n- - items\n  - Spider\n  - Middleware添加ip代理\n  - Pipeline添加mongodb存储\n\n# 最终爬取截图\n\n![在这里插入图片描述](https://img-blog.csdn.net/20180919104939869?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgzNjM1MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n# 项目创建\n\n本项目使用的是Windows系统下的Pycharm平台，Python版本为3.6\n\n使用`scrapy startproject scrapy_boss`创建scrapy项目\n\n## items\n\n```\nfrom scrapy import Item, Field\n\nclass BossItem(Item):\n    company_name = Field()  #公司名称\n    company_status = Field()  #公司规模\n    company_address = Field()  #公司地址\n\n    job_title = Field()  #职位名称\n    job_salary = Field()  #薪酬\n    job_detail = Field()  #职位描述、详情要求\n\n    job_experience = Field()  #工作经验\n    job_education = Field()  #学历要求\n    job_url = Field()  #发布页面\n12345678910111213141234567891011121314\n```\n\n## Spider\n\n- 参数\n\n```\nclass BossSpider(Spider):\n   name = 'boss'\n   allowed_domains = ['www.zhipin.com']\n   # start_urls = ['http://www.zhipin.com/']\n\n   headers = {\n       'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n       'accept-encoding': 'gzip, deflate, br',\n       'accept-language': 'zh-CN,zh;q=0.9',\n       'cookie': '__c=1535347437; __g=-; __l=l=%2Fwww.zhipin.com%2Fc101010100%2Fh_101010100%2F%3Fquery%3D%25E7%2588%25AC%25E8%2599%25AB%25E5%25B7%25A5%25E7%25A8%258B%25E5%25B8%2588%26page%3D4%26ka%3Dpage-4&r=; Hm_lvt_194df3105ad7148dcf2b98a91b5e727a=1533949950,1534555474,1535095432,1535347437; lastCity=101010100; toUrl=https%3A%2F%2Fwww.zhipin.com%2Fjob_detail%2F%3Fquery%3Dpython%26scity%3D101010100%26industry%3D%26position%3D; JSESSIONID=\"\"; Hm_lpvt_194df3105ad7148dcf2b98a91b5e727a=1535348378; __a=5534803.1512627994.1535095432.1535347437.264.8.4.260',\n       'referer': 'https://www.zhipin.com/job_detail/?query=python&scity=101010100&industry=&position=',\n       'upgrade-insecure-requests': '1',\n       'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'\n   }\n12345678910111213141234567891011121314\n```\n\n- 解析index页\n\n```\n    def start_requests(self):\n        queries = ['hadoop', 'spark', 'hbase', 'hive', r'大数据',\n                    r'数据分析', r'数据运营', r'数据挖掘', r'爬虫', r'抓取', r'可视化', r'数据开发', r'数据工程',\n                    r'数据处理', r'数据科学家', r'数据工程师', r'数据架构师', r'数据采集', r'数据建模', r'数据平台',\n                    r'数据研发', r'数据管理', r'数据统计', r'数据产品', r'数据方向', r'数据仓库', r'数据研究',\n                    r'数据算法', r'数据蜘蛛', r'数据应用', r'数据技术', r'数据运维', r'数据支撑', r'数据安全',\n                    r'数据爬取', r'数据经理', r'金融数据', r'数据专员', r'数据主管', r'数据项目经理', r'数据整合',\n                    r'数据模型', r'财务数据', r'数据专家', r'数据报送', r'数据中心', r'数据移动', r'数据标准',\n                    r'数据推广', r'数据质量', r'数据检索', r'数据服务', r'数据搭建', r'数据实施', r'数据风控']\n\n        urls = []\n        for query in queries:\n            url = 'https://www.zhipin.com/job_detail/?query=%s&scity=101010100'%query\n            urls.append(url)\n        for url in urls:\n            yield Request(url=url, headers=self.headers, callback=self.parse_index)\n\n12345678910111213141516171234567891011121314151617\n```\n\n- 解析index页面构造url，并实现翻页\n\n```\n    def parse_index(self, response):\n        sel = Selector(response)\n        url_prefix = 'https://www.zhipin.com'\n        job_urls = sel.xpath('//a[@data-index]/@href').extract()\n        for job_url in job_urls:\n            url = url_prefix + job_url\n            yield Request(url=url,headers=self.headers, callback=self.parse_detail)\n\n        next_page_url = sel.xpath('//a[@class=\"next\"]/@href').extract()\n        if len(next_page_url) != 0:\n            next_page_url = url_prefix + ''.join(next_page_url)\n            yield Request(url=next_page_url, headers=self.headers, callback=self.parse_index)\n123456789101112123456789101112\n```\n\n- 解析详情页\n\n```\n\t def parse_detail(self,response):\n\t    item = BossItem()\n\t    \n\t    item['company_name'] = response.xpath('//a[@ka=\"job-detail-company\"]/text()').extract_first()\n        item['company_status'] =  response.xpath('//div[@class=\"info-company\"]/p[1]/text()[1]').extract_first()\n        item['company_address'] = response.xpath('//div[@class=\"location-address\"]/text()').extract_first()\n\n        item['job_title'] = response.xpath('//div[@class=\"name\"]/h1/text()').extract_first()\n        item['job_experience'] = response.xpath('//div[@class=\"job-primary detail-box\"]/div[@class=\"info-primary\"]/p/text()[2]').extract_first()\n        item['job_education'] = response.xpath('//div[@class=\"job-primary detail-box\"]/div[@class=\"info-primary\"]/p/text()[3]').extract_first()\n        item['job_salary'] = response.xpath('//span[@class=\"badge\"]/text()').extract_first().replace(\"\\n\", \"\").strip()\n        item['job_detail'] = response.xpath('//div[@class=\"job-sec\"]/div[@class=\"text\"]/text()').extract()\n        \n        item['job_url'] = response.url\n\n\n        yield item\n \n123456789101112131415161718123456789101112131415161718\n```\n\n## Middleware添加ip代理\n\n此处借助了一个IP池，在本地提供了一个随机提取可用代理的端口。\n\n```\nclass ProxyMiddleware(object):\n\n    def process_request(self,request,spider):\n        response = requests.get('http://localhost:5555/random')\n        proxy = response.text\n        request.meta['proxy'] = \"http://\"+proxy\n123456123456\n```\n\n- settings\n  需要在settings中打开DownloaderMiddleware\n  ![在这里插入图片描述](https://img-blog.csdn.net/20180919104821182?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgzNjM1MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n\n## Pipeline添加mongodb存储\n\n```\nimport pymongo\n\nclass scrapy_bossPipeline(object):\n    def process_item(self, item, spider):\n        return item\n\nclass MongoPipeline(object):\n    def __init__(self,mongo_uri,mongo_db):\n        self.mongo_uri = mongo_uri\n        self.mongo_db = mongo_db\n\n    @classmethod\n    def from_crawler(cls,crawler):\n        return  cls(\n            mongo_uri = crawler.settings.get('MONGO_URI'),\n            mongo_db = crawler.settings.get('MONGO_DB')\n        )\n\n    def open_spider(self,spider):\n        self.client = pymongo.MongoClient(self.mongo_uri)\n        self.db = self.client[self.mongo_db]\n\n    def process_item(self,item,spider):\n        name = item.__class__.__name__\n        self.db[name].insert(dict(item))\n        return item\n12345678910111213141516171819202122232425261234567891011121314151617181920212223242526\n```\n\n- settings：\n\n需要在settings中添加\n\n```\nSPIDER_MODULES = ['scrapy_boss.spiders']\nNEWSPIDER_MODULE = 'scrapy_boss.spiders'\n\nMONGO_URI = 'localhost'\nMONGO_DB = 'scrapy_boss'\n1234512345\n```\n\n并打开itemspipeline\n![在这里插入图片描述](https://img-blog.csdn.net/20180919104602640?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjgzNjM1MQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n","tags":["爬虫"]},{"title":"mysql事务特性及隔离级别","url":"/2019/02/23/pout/数据库/mysql/mysql事务特性及隔离级别/","content":"\n\n<!-- toc -->\n\n\n\n### 1.1.2、事务的特性\n\n1. 原子性\n\n   > 事务中的全部操作在数据库中是不可分割的，要么全部完成，要么全都不完成\n\n2. 一致性\n\n   > 几个并行执行的事务，其执行结果必须与按某一顺序串行执行的结果相一致\n\n3. 隔离性\n\n   > 事务的执行不受其他事务的干扰，事务执行的中间结果对其他事务必须是透明的\n\n4. 持久性\n\n   > 一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作\n\n\n\n![img](https://pic2.zhimg.com/80/v2-59ea2f0769e4e9ffbcdce938d306fae9_hd.png) \n\n### 1.1.3、事务隔离级别\n\n1. **未提交读：脏读（READ UNCOMMITTED）**\n\n   1. 事务2查询到的数据是事务1中修改但未提交的数据，但因为事务1回滚了数据\n   2. 所以事务2查询的数据是不正确的，因此出现了脏读的问题\n\n\n### READ UNCOMMITTED（读未提交）\n\n 该隔离级别的事务会读到其它未提交事务的数据，此现象也称之为**脏读**。 \n\n 两个命令行客户端分别为A，B；不断改变A的隔离级别，在B端修改数据。 \n\n```\n将A的隔离级别设置为read uncommitted(未提交读)\nA：SET @@session.transaction_isolation = 'READ-UNCOMMITTED';\n创建一张test\ncreate database test;\n\nuse test;\n\ncreate table test(id int primary key);\n\ninsert into test(id) values(1);\n\nA：启动事务，此时数据为初始状态\nstart transaction;\n\nB：启动事务，更新数据，但不提交\nstart transaction;\nupdate test set id = 2 where id = 1;\n\nA：再次读取数据，发现数据已经被修改了，这就是所谓的“脏读\nselect * from test;\n\nB:回滚事务\nrollback;\n\nA:再次读数据，发现数据变回初始状态\nselect * from test;\n\n\n\n```\n\n\n\n1. **提交读：不可重复读（READ COMMITTED）**\n\n   注：一个事务从开始到提交之前对数据所做的改变对其他事务是不可见的，这样就解决在READ-UNCOMMITTED级别下的脏读问题。\n\n   1. 事务2执行update语句但未提交前，事务1的前两个select操作返回结果是相同的\n   2. 但事务2执行commit操作后，事务1的第三个select操作就读取到事务2对数据的改变\n   3. 导致与前两次select操作返回不同的数据，因此出现了不可重复读的问题\n\n\n### READ COMMITTED（提交读）\n\n 一个事务可以读取另一个已提交的事务，多次读取会造成不一样的结果，此现象称为不可重复读问题，Oracle 和 SQL Server 的默认隔离级别。 \n\n```\nA:将客户端A的事务隔离级别设置为read committed(已提交读)\n\nSET @@session.transaction_isolation = 'READ-COMMITTED';\n创建test表\ncreate database test;\nuse test;\ncreate table test(id int primary key);\ninsert into test(id) values(1);\n\nA：启动事务，此时数据为初始状态\nstart transaction;\n\nB：启动事务，更新数据，但不提交\nstart transaction;\nupdate test set id = 2 where id = 1;\n\nA：再次读数据，发现数据未被修改\nselect * from test;\n\nB：提交事务\ncommit;\n\nA：再次读取数据，发现数据已发生变化，说明B提交的修改被事务中的A读到了，这就是所谓的“不可重复读”\nselect * from test;\n\n\n```\n\n\n\n1. **可重复读：幻读（REPEATABLE READ）：这是MySQL的默认事务隔离级别**\n   1. 事务每开启一个实例，都会分配一个版本号给它，如果读取的数据行正在被其他事务执行DELETE或UPDATE操作（既该行上有排他锁）\n   2. 这时该事务的读取操作不会等待行上的锁释放，而是根据版本号去读取行的快照数据（记录在undo log中）\n   3. 这样，事务中的查询操作返回的都是同一版本下的数据，解决了不可重复读问题。\n   4. 虽然该隔离级别下解决了不可重复读问题，但理论上会导致另一个问题：幻读（Phantom Read）。\n   5. 一个事务在执行过程中，另一个事务对已有数据行的更改，MVCC机制可保障该事务读取到的原有数据行的内容相同\n   6. 但并不能阻止另一个事务插入新的数据行，这就会导致该事务中凭空多出数据行，像出现了幻读一样，这便是幻读问题\n\n\n### REPEATABLE READ（可重复读）\n\n 该隔离级别是 MySQL 默认的隔离级别，在同一个事务里，`select` 的结果是事务开始时时间点的状态，因此，同样的 `select` 操作读到的结果会是一致的，但是，会有**幻读**现象 \n\n```\n将A的隔离级别设置为repeatable read(可重复读)\nSET @@session.transaction_isolation = 'REPEATABLE-READ';\ncreate database test;\nuse test;\ncreate table test(id int primary key,name varchar(20));\n\nA：登录 mysql 终端 A，开启一个事务。\nstart transaction;\nselect * from test; -- 无记录\n\nB：登录 mysql 终端 B，开启一个事务。\nuse test;\nstart transaction;\nselect * from test; -- 无记录\n\nA:切换到 mysql 终端 A，增加一条记录并提交。\ninsert into test(id,name) values(1,'a');\ncommit;\nselect * from test; --可以看到已经更改\n\nB:切换到 msyql 终端 B。\nselect * from test; --此时查询还是无记录\n\n通过这一步可以证明，在该隔离级别下已经读取不到别的已提交的事务，如果想看到 mysql 终端 1 提交的事务，在 mysql 终端 2 将当前事务提交后再次查询就可以读取到 mysql 终端 1 提交的事务。\n 可重复读隔离级别只允许读取已提交记录，而且在一个事务两次读取一个记录期间，其他事务部的更新该记录。 \n\nB：此时接着在 mysql 终端 B 插入一条数据。\ninsert into test(id,name) values(1,'b'); -- 此时报主键冲突的错误\n这就是该隔离级别下可能产生的问题，MySQL 称之为幻读。\ncommit;\n\n\n```\n\n\n\n1. **可串行读（SERIALIZABLE）**\n\n   1. 这是事务的最高隔离级别，通过强制事务排序，使之不可能相互冲突，就像在每个读的数据行加上共享锁来实现\n   2. 在该隔离级别下，可以解决前面出现的脏读、不可重复读和幻读问题，但也会导致大量的超时和锁竞争现象，一般不推荐使用\n\n### SERIALIZABLE（可串行读）\n\n 在该隔离级别下事务都是串行顺序执行的，MySQL 数据库的 InnoDB 引擎会给读操作隐式加一把读共享锁，从而避免了脏读、不可重读复读和幻读问题。 \n\n```\nA:准备两个终端，在此命名为 mysql 终端 A 和 mysql 终端 B，分别登入 mysql，准备一张测试表 test 并调整隔离级别为 SERIALIZABLE，任意一个终端执行即可。\nset session transaction isolation level serializable;\ncreate database test;\nuse test;\ncreate table test(id int primary key);\ninsert into test(id) values(1);\n\nA:登录 mysql 终端 A，开启一个事务，并写入一条数据。\nstart transaction;\nselect * from test;\n\nB:登录 mysql 终端 B，开启一个事务。\nstart transaction;\nselect * from test; \n delete from test;\n\nA:立马切换到 mysql 终端 A,提交事务。\ncommit;\n\n一旦事务提交，msyql 终端 B 会立马返回 ID 为 1 的记录，否则会一直卡住，直到超时，其中超时参数是由 innodb_lock_wait_timeout 控制\n\n```\n\n\n\n\n\n","tags":["数据库"]},{"title":"Mysql主从","url":"/2019/02/23/pout/数据库/mysql/Mysql主从/","content":"\n\n<!-- toc -->\n\n\n# 在阿里云Centos7.6上面配置Mysql主从数据库(master/slave)，实现读写分离\n\n\n\n​    在之前的一篇文章中，阐述了如何在高并发高负载的场景下使用nginx做后台服务的负载均衡:[在阿里云Centos上配置nginx+uwsgi+负载均衡配置](https://v3u.cn/a_id_77),但是不要以为这样做了就是一劳永逸的，到了数据业务层、数据访问层，如果还是传统的数据结构，或者只是单单靠一台服务器负载，如此多的数据库连接操作，数据库必然会崩溃，数据库如果宕机的话，后果更是不堪设想。这时候，我们会考虑如何减少数据库的连接，一方面采用优秀的代码框架，进行代码的优化，采用优秀的数据缓存技术如：redis,如果资金丰厚的话，必然会想到架设mysql服务集群，来分担主数据库的压力。今天总结一下利用MySQL主从配置，实现读写分离，减轻数据库压力。\n\n​    明确目的，部署mysql集群，采用一主一从的策略，写入操作使用主库，从库实时同步主库的数据，从库负责读取的业务，从而完成读写分离的目的。\n\n​    mysql主从同步的原理很简单，从库生成两个线程，一个I/O线程，一个SQL线程；i/o线程去请求主库 的binlog（二进制日志），并将得到的binlog日志写到relay log（中继日志） 文件中；主库会生成一个 log dump 线程，用来给从库 i/o线程传binlog；\n\n​    SQL 线程，会读取relay log文件中的日志，并解析成具体操作，来实现主从的操作一致，而最终数据一致。\n\n​    ![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190529/20190529071516_23557.png)\n\n​    首先准备两台阿里云服务器，一台作为主机(master)，一台作为从机(slave)，都安装好mysql5.7。\n\n​    进入master服务器\n\n​    修改mysql配置文件 vim /etc/my.cnf，加入如下配置\n\n​    \n\n```\nserver-id=1\ninnodb_flush_log_at_trx_commit=2\nsync_binlog=1\nlog-bin=mysql-bin-1\n```\n\n​    \n\n配置说明：\n\n保存后，重启mysql\n\n```\nsystemctl restart mysqld\n```\n\n进入mysql命令行 mysql -uroot -p你的密码\n\n输入授权命令\n\n```\nGRANT REPLICATION SLAVE ON *.* to 'repl'@'%' identified by 'Admin123!'; \n```\n\n意思是所有slave都可以通过账号repl和密码Admin123!来同步master的数据\n\n然后查看master的状态:\n\n```\nshow master status;\n```\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190529/20190529072527_35767.png)\n\n把file列和Position列记录下来，一会配置slave要用到\n\n此时Master的配置已经搞定，登录一下从机(slave)\n\n同理修改slave服务器的mysql配置 vim /etc/my.cnf 加入下面的配置，需要注意的是server-id不要和master一样\n\n```\nserver-id=201 \ninnodb_flush_log_at_trx_commit=2 \nsync_binlog=1 \nlog-bin=mysql-bin-201\n```\n\n保存后重启服务 systemctl restart mysqld\n\n进入mysql命令行 mysql -uroot -p你的密码\n\n输入命令：\n\n```\nchange master to master_host='39.106.228.179',master_user='repl' ,master_password='Admin123!', master_log_file='mysql-bin.000002' ,master_log_pos=154\n```\n\n命令说明：\n\nmaster_host: 主机的ip\nmaster_user : 主机授权的用户.\nmaster_password : 主机授权时候填写的密码\nmaster_log_file : 主机show master status;中的File\nmaster_log_pos: 主机show master status;中的Position.\n\n输入命令启动slave\n\n```\nstart slave;\n```\n\n```\nshow slave status G;\n```\n\n然后我们就可以测试一下对master进行写入，看看salve是否可以同步数据了\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190529/20190529073320_84778.png)\n\n当然了，mysql的读写分离主从配置并不是万能的，根据不同的应用场景选择不同的策略，MySQL的主从复制功能有一定的延迟性，如果对数据实时一致性的要求比较高的场景不推荐使用。\n","tags":["数据库"]},{"title":"MySql","url":"/2019/02/23/pout/数据库/mysql/mysql/","content":"\n\n<!-- toc -->\n\n# mysql集群主从库读写分离\n\n高负载高并发环境下，数据业务层、数据访问层，如果还是传统的数据结构，或者只是单单靠一台服务器负载，如此多的数据库连接操作，数据库必然会崩溃，数据库如果宕机的话，后果更是不堪设想。这时候，我们会考虑如何减少数据库的连接，一方面采用优秀的代码框架，进行代码的优化，采用优秀的数据缓存技术如：redis,如果资金丰厚的话，必然会想到架设mysql服务集群，来分担主数据库的压力。今天总结一下利用MySQL主从配置，实现读写分离，减轻数据库压力。\n\n具体搭建:<https://v3u.cn/a_id_85>\n\nmysql主从同步的原理很简单，从库生成两个线程，一个I/O线程，一个SQL线程；i/o线程去请求主库 的binlog（二进制日志），并将得到的binlog日志写到relay log（中继日志） 文件中；主库会生成一个 log dump 线程，用来给从库 i/o线程传binlog；\n\nSQL 线程，会读取relay log文件中的日志，并解析成具体操作，来实现主从的操作一致，而最终数据一致。\n\n### 关于binlog日志\n\nbinlog是二进制日志文件，用于记录mysql的数据更新或者潜在更新(比如DELETE语句执行删除而实际并没有符合条件的数据)\n\nbinlog索引文件mysql-bin.index。如官方文档中所写，binlog格式如下：\n\nbinlog文件以一个值为0Xfe62696e的魔数开头，这个魔数对应0xfe 'b''i''n'。\n\nbinlog由一系列的binlog event构成。每个binlog event包含header和data两部分。\n\nheader部分提供的是event的公共的类型信息，包括event的创建时间，服务器等等。\n\ndata部分提供的是针对该event的具体信息，如具体数据的修改。\n\n![img](img/mysql.png)\n\n### 主从同步延迟问题：\n\n架构方面\n\n1.业务的持久化层的实现采用分库架构，mysql服务可平行扩展，分散压力。\n\n2.单个库读写分离，一主多从，主写从读，分散压力。这样从库压力比主库高，保护主库。\n\n3.服务的基础架构在业务和mysql之间加入memcache或者Redis的cache层。降低mysql的读压力。\n\n4.不同业务的mysql物理上放在不同机器，分散压力。\n\n5.使用比主库更好的硬件设备作为slave\n\n总结，mysql压力小，延迟自然会变小。\n\n硬件方面\n\n1.采用好服务器，比如4u比2u性能明显好，2u比1u性能明显好。\n\n2.存储用ssd或者盘阵或者san，提升随机写的性能。\n\n3.主从间保证处在同一个交换机下面，并且是万兆环境。\n\n总结，硬件强劲，延迟自然会变小。一句话，缩小延迟的解决方案就是花钱和花时间。\n\nmysql主从同步加速\n\n1、sync_binlog在slave端设置为0\n\n2、–logs-slave-updates 从服务器从主服务器接收到的更新不记入它的二进制日志。\n\n3、直接禁用slave端的binlog\n\n4、slave端，如果使用的存储引擎是innodb，innodb_flush_log_at_trx_commit = 2\n\n# 悲观锁和乐观锁\n\n### 乐观锁\n\n乐观锁不是数据库自带的，需要我们自己去实现。乐观锁是指操作数据库时(更新操作)，想法很乐观，认为这次的操作不会导致冲突，在操作数据时，并不进行任何其他的特殊处理（也就是不加锁），而在进行更新后，再去判断是否有冲突了。\n\n### 悲观锁\n\n每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁\n\n### 更新丢失：最后的更新覆盖了其他事务之前的更新，而事务之间并不知道，发生更新丢失。更新丢失，可以完全避免，应用对访问的数据加锁即可。\n\n### 脏读：(针对未提交的数据)一个事务在更新一条记录，未提交前，第二个事务读到了第一个事务更新后的记录，那么第二个事务就读到了脏数据，会产生对第一个未提交,解决方案加锁，或者调整mysql事务隔离级别，数据库的事务隔离越严格，并发负作用越小，代价越高\n\n# 触发器\n\n触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象\n\n监听：记录的增加、修改、删除。\n\n-- 创建触发器\n\nCREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN...END复合语句结构\n\n-- 删除\n\nDROP TRIGGER [schema_name.]trigger_name\n\n可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new.\n\n-- 注意\n\n```\n1. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。\n```\n\n# 事务\n\n事务是指逻辑上的一组操作，组成这组操作的各个单元，要不全成功要不全失败。\n\n```\n- 支持连续SQL的集体成功或集体撤销。\n- 事务是数据库在数据晚自习方面的一个功能。\n- 需要利用 InnoDB 或 BDB 存储引擎，对自动提交的特性支持完成。\n- InnoDB被称为事务安全型引擎。\n```\n\n-- 事务开启 START TRANSACTION; 或者 BEGIN; 开启事务后，所有被执行的SQL语句均被认作当前事务内的SQL语句。 -- 事务提交 COMMIT; -- 事务回滚 ROLLBACK; 如果部分操作发生问题，映射到事务开启前。\n\n-- 事务的特性\n\n```\n1. 原子性（Atomicity）\n    事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。\n2. 一致性（Consistency）\n    事务前后数据的完整性必须保持一致。\n    - 事务开始和结束时，外部数据一致\n    - 在整个事务过程中，操作是连续的\n3. 隔离性（Isolation）\n    多个用户并发访问数据库时，一个用户的事务不能被其它用户的事物所干扰，多个并发事务之间的数据要相互隔离。\n4. 持久性（Durability）\n    一个事务一旦被提交，它对数据库中的数据改变就是永久性的。\n```\n\n-- 事务的实现\n\n```\n1. 要求是事务支持的表类型\n2. 执行一组相关的操作前开启事务\n3. 整组操作完成后，都成功，则提交；如果存在失败，选择回滚，则会回到事务开始的备份点。\n```\n\n-- 事务的原理 利用InnoDB的自动提交(autocommit)特性完成。 普通的MySQL执行语句后，当前的数据提交操作均可被其他客户端可见。 而事务是暂时关闭“自动提交”机制，需要commit提交持久化数据操作。\n\n-- 注意\n\n```\n1. 数据定义语言（DDL）语句不能被回滚，比如创建或取消数据库的语句，和创建、取消或更改表或存储的子程序的语句。\n2. 事务不能被嵌套\n```\n\n-- 保存点 SAVEPOINT 保存点名称 -- 设置一个事务保存点 ROLLBACK TO SAVEPOINT 保存点名称 -- 回滚到保存点 RELEASE SAVEPOINT 保存点名称 -- 删除保存点\n\n-- InnoDB自动提交特性设置 SET autocommit = 0|1; 0表示关闭自动提交，1表示开启自动提交。\n\n```\n- 如果关闭了，那普通操作的结果对其他客户端也不可见，需要commit提交后才能持久化数据操作。\n- 也可以关闭自动提交来开启事务。但与START TRANSACTION不同的是，\n    SET autocommit是永久改变服务器的设置，直到下次再次修改该设置。(针对当前连接)\n    而START TRANSACTION记录开启前的状态，而一旦事务提交或回滚后就需要再次开启事务。(针对当前事务)\n```\n\n## 并发事务带来的几个问题：更新丢失，脏读，不可重复读，幻读。\n\n事务隔离级别：未提交读(Read uncommitted)，已提交读(Read committed)，可重复读(Repeatable read)，可序列化(Serializable)\n\nRead Uncommitted（读取未提交内容）\n\n在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少。读取未提交的数据，也被称之为脏读（Dirty Read）。\n\nRead Committed（读取提交内容）\n\n这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这种隔离级别 也支持所谓的不可重复读（Nonrepeatable Read），因为同一事务的其他实例在该实例处理其间可能会有新的commit，所以同一select可能返回不同结果。\n\nRepeatable Read（可重读）\n\n这是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。InnoDB和Falcon存储引擎通过多版本并发控制（MVCC，Multiversion Concurrency Control）机制解决了该问题。\n\nSerializable（可串行化） 这是最高的隔离级别，它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之，它是在每个读的数据行上加上共享锁。在这个级别，可能导致大量的超时现象和锁竞争。\n\n```\n这四种隔离级别采取不同的锁类型来实现，若读取的是同一个数据的话，就容易发生问题。例如：\n\n脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。\n\n不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。\n\n幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。\n```\n\n1、脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据\n\n2、不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果 不一致。\n\n3、幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。\n\n小结：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表\n\n低级别的隔离级一般支持更高的并发处理，并拥有更低的系统开销。\n\n可重复读(Repeatable read)是MySQL的默认事务隔离级别，它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行。不过理论上，这会导致另一个棘手的问题：幻读 （Phantom Read）。简单的说，幻读指当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行。\n\n# 锁表\n\n表锁定只用于防止其它客户端进行不正当地读取和写入\n\nMyISAM 支持表锁，InnoDB 支持行锁\n\n-- 锁定 LOCK TABLES tbl_name [AS alias]\n\n-- 解锁 UNLOCK TABLES\n\n# 联合索引&最左原则\n\n具体实践和测试：<https://v3u.cn/a_id_91>\n\n联合索引的好处：\n\n覆盖索引，这一点是最重要的，重所周知非主键索引会先查到主键索引的值再从主键索引上拿到想要的值，这样多一次查询索引下推。但是覆盖索引可以直接在非主键索引上拿到相应的值，减少一次查询。\n\n在一张大表中如果有 (a,b,c)联合索引就等于同时加上了 (a) (ab) (abc) 三个索引减少了存储上的一部分的开销和操作开销\n\n梯度漏斗，比如 select *from t where a = 1 and b = 2 and c = 3; 就等于在满足 a = 1 的一部分数据中过滤掉b = 2 的 再从 a = 1 and b = 2 过滤掉 c = 3 的，越多查询越高效。\n\n最左原则：最左优先，在检索数据时从联合索引的最左边开始匹配,类似于给(a,b,c)这三个字段加上联合索引就等于同时加上了 (a) (ab) (abc) 这三种组合的查询优化\n\n底层数据结构：b+tree\n\nBTREE 每个节点都是一个二元数组: [key, data]，所有节点都可以存储数据。key为索引key,data为除key之外的数据。\n\n查找算法：首先从根节点进行折半查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或未找到节点返回空指针\n\nB+Tree有以下不同点：非叶子节点不存储data，只存储索引key；只有叶子节点才存储data，而Mysql中B+Tree：在经典B+Tree的基础上进行了优化，增加了顺序访问指针。在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针，就形成了带有顺序访问指针的B+Tree。这样就提高了区间访问性能：请见下图，如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可\n\n总结：B+Tree 在 B-Tree 的基础上有两点变化：\n\n数据是存在叶子节点中的；\n\n数据节点之间是有指针指向的。\n\n![img](img/btree.png)\n\n# 连接池问题\n\n连接池基本的思想是在系统初始化的时候，将数据库连接作为对象存储在内存中，当用户需要访问数据库时，并非建立一个新的连接，而是从连接池中取出一个已建立的空闲连接对象。使用完毕后，用户也并非将连接关闭，而是将连接放回连接池中，以供下一个请求访问使用。而连接的建立、断开都由连接池自身来管理。同时，还可以通过设置连接池的参数来控制连接池中的初始连接数、连接的上下限数以及每个连接的最大使用次数、最大空闲时间等等。也可以通过其自身的管理机制来监视数据库连接的数量、使用情况等。\n\n有点类似单例模式的概念\n\ndjango最新版本已经包含了连接池，通过修改配置控制，官方文档：<https://docs.djangoproject.com/en/1.9/ref/databases/>\n\n# 关于ip存储问题\n\n1、为什么要转换？\n\n```\n  IP转换成整型存储可以减少索引IP字符串类型消耗的资源量。\n```\n\n2、转换的原理是什么？\n\n```\n  IP转换为整形：形如123.125.0.236 可以看成用点分割的四个小段的整数，每个小段的范围为0~255，那么就可以将每个数字转换为二进制的数字。\n\n 第一段：123转换为二进制   01111011\n\n 第二段：125转换为二进制   01111101\n\n 第三段： 0   转换为二进制   00000000\n\n 第四段： 236转换为二进制  11101100\n\n 然后将四段拼接起来成了长的二进制数：01111011011111010000000011101100\n\n 并将这个厂二进制数转换为整形的数字：2071789804\n\n 整形转换为IP的思路刚好与上述相反，将十进制数转换为二进制之后，从后向前每8个数取一个段落，转换为十进制数字\n```\n\n```\ndef convert_int(a):\n    #  转换为4个段的列表\n    a_list = a.split('.',4)\n    # a_list.reverse()\n    a_str = ''\n    for i in a_list:\n        a_tem = bin(int(i))[2:] # 字符串\n        if len( a_tem) != 8:\n        # 在前面加 0\n            a_str += '0'*(8-len(a_tem))+a_tem\n        else:\n            a_str += a_tem\n    # print a_str\n    return int(a_str,2)\n\ndef convert_ip(b):\n    #先转换为二进制\n    b_tem = bin(int(b))[2:]\n    b_str = ''\n    # 将所有的 0 补齐\n    if len(b_tem) != 32:\n       b_str += '0' * (32 - len(b_tem)) + b_tem\n    #切片处理\n    b1 = b_str[0:8]\n    b2 = b_str[8:16]\n    b3 = b_str[16:24]\n    b4 = b_str[24:]\n    b_out= str(int(b1,2))+'.'+str(int(b2 ,2))+'.'+str(int(b3,2))+'.'+str(int(b4,2))\n    return b_out\n```\n\n# mysql 优化\n\n## 为什么要优化\n\n系统的吞吐量瓶颈往往出现在数据库的访问速度上 随着应用程序的运行，数据库的中的数据会越来越多，处理时间会相应变慢 数据是存放在磁盘上的，读写速度无法和内存相比\n\n## 如何优化\n\n设计数据库时：数据库表、字段的设计，存储引擎 利用好MySQL自身提供的功能，如索引等 横向扩展：MySQL集群、负载均衡、读写分离 SQL语句的优化（收效甚微）\n\n## 字段设计\n\n字段类型的选择，设计规范，范式，常见设计案例 原则：尽量使用整型表示字符串 存储IP INET_ATON(str)，address to number\n\nINET_NTOA(number)，number to address\n\nMySQL内部的枚举类型（单选）和集合（多选）类型 但是因为维护成本较高因此不常使用，使用关联表的方式来替代enum\n\n原则：定长和非定长数据类型的选择 decimal不会损失精度，存储空间会随数据的增大而增大。double占用固定空间，较大数的存储会损失精度。非定长的还有varchar、text 金额 对数据的精度要求较高，小数的运算和存储存在精度问题（不能将所有小数转换成二进制） 定点数decimal price decimal(8,2)有2位小数的定点数，定点数支持很大的数（甚至是超过int,bigint存储范围的数）\n\n小单位大数额避免出现小数 元->分\n\n字符串存储 定长char，非定长varchar、text（上限65535，其中varchar还会消耗1-3字节记录长度，而text使用额外空间记录长度）\n\n原则：尽可能选择小的数据类型和指定短的长度 原则：尽可能使用 not null 非null字段的处理要比null字段的处理高效些！且不需要判断是否为null。\n\nnull在MySQL中，不好处理，存储需要额外空间，运算也需要特殊的运算符。如select null = null和select null <> null（<>为不等号）有着同样的结果，只能通过is null和is not null来判断字段是否为null。\n\n如何存储？MySQL中每条记录都需要额外的存储空间，表示每个字段是否为null。因此通常使用特殊的数据进行占位，比如int not null default 0、string not null default ‘’\n\n原则：字段注释要完整，见名知意 原则：单表字段不宜过多 二三十个就极限了\n\n原则：可以预留字段 在使用以上原则之前首先要满足业务需求 关联表的设计 外键foreign key只能实现一对一或一对多的映射 一对多 使用外键\n\n多对多 单独新建一张表将多对多拆分成两个一对多\n\n一对一 如商品的基本信息（item）和商品的详细信息（item_intro），通常使用相同的主键或者增加一个外键字段（item_id）\n\n## 范式 Normal Format\n\n数据表的设计规范，一套越来越严格的规范体系（如果需要满足N范式，首先要满足N-1范式）。N 第一范式1NF：字段原子性 字段原子性，字段不可再分割。\n\n关系型数据库，默认满足第一范式 注意比较容易出错的一点，在一对多的设计中使用逗号分隔多个外键，这种方法虽然存储方便，但不利于维护和索引（比如查找带标签java的文章）\n\n第二范式：消除对主键的部分依赖 即在表中加上一个与业务逻辑无关的字段作为主键 主键：可以唯一标识记录的字段或者字段集合。\n\ncourse_namecourse_classweekday（周几）course_teacherMySQL教育大楼1525周一张三Java教育大楼1521周三李四MySQL教育大楼1521周五张三\n\n依赖：A字段可以确定B字段，则B字段依赖A字段。比如知道了下一节课是数学课，就能确定任课老师是谁。于是周几和下一节课和就能构成复合主键，能够确定去哪个教室上课，任课老师是谁等。但我们常常增加一个id作为主键，而消除对主键的部分依赖。\n\n对主键的部分依赖：某个字段依赖复合主键中的一部分。\n\n解决方案：新增一个独立字段作为主键。\n\n第三范式：消除对主键的传递依赖 传递依赖：B字段依赖于A，C字段又依赖于B。比如上例中，任课老师是谁取决于是什么课，是什么课又取决于主键id。因此需要将此表拆分为两张表日程表和课程表（独立数据独立建表）：\n\nidweekdaycourse_classcourse_id1001周一教育大楼15213546course_idcourse_namecourse_teacher3546Java张三\n\n这样就减少了数据的冗余（即使周一至周日每天都有Java课，也只是course_id:3546出现了7次）\n\n## 垂直分割(分表)\n\n是一种把数据库中的表按列变成几张表的方法，这样可以降低表的复杂度和字段的数目，从而达到优化的目的。（以前，在银行做过项目，见过一张表有100多个字段，很恐怖）\n\n示例一：在Users表中有一个字段是家庭地址，这个字段是可选字段，相比起，而且你在数据库操作的时候除了个人信息外，你并不需要经常读取或是改写这个字段。那么，为什么不把他放到另外一张表中呢？ 这样会让你的表有更好的性能，大家想想是不是，大量的时候，我对于用户表来说，只有用户ID，用户名，口令，用户角色等会被经常使用。小一点的表总是会有好的性能。\n\n示例二： 你有一个叫 “last_login” 的字段，它会在每次用户登录时被更新。但是，每次更新时会导致该表的查询缓存被清空。所以，你可以把这个字段放到另一个表中，这样就不会影响你对用户ID，用户名，用户角色的不停地读取了，因为查询缓存会帮你增加很多性能。\n\n另外，你需要注意的是，这些被分出去的字段所形成的表，你不会经常性地去Join他们，不然的话，这样的性能会比不分割时还要差，而且，会是极数级的下降。\n\n## 存储引擎选择\n\n早期问题：如何选择MyISAM和Innodb？ 现在不存在这个问题了，Innodb不断完善，从各个方面赶超MyISAM，也是MySQL默认使用的。 存储引擎Storage engine：MySQL中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现。\n\n功能差异 show engines\n\nEngineSupportCommentInnoDBDEFAULTSupports transactions, row-level locking, and foreign keysMyISAMYESMyISAM storage engine\n\n存储差异 MyISAMInnodb文件格式数据和索引是分别存储的，数据.MYD，索引.MYI数据和索引是集中存储的，.ibd文件能否移动能，一张表就对应.frm、MYD、MYI3个文件否，因为关联的还有data下的其它文件记录存储顺序按记录插入顺序保存按主键大小有序插入空间碎片（删除记录并flush table 表名之后，表文件大小不变）产生。定时整理：使用命令optimize table 表名实现不产生事务不支持支持外键不支持支持锁支持（锁是避免资源争用的一个机制，MySQL锁对用户几乎是透明的）表级锁定行级锁定、表级锁定，锁定力度小并发能力高\n\n锁扩展 表级锁（table-level lock）：lock tables ,... read/write，unlock tables ,...。其中read是共享锁，一旦锁定任何客户端都不可读；write是独占/写锁，只有加锁的客户端可读可写，其他客户端既不可读也不可写。锁定的是一张表或几张表。 行级锁（row-level lock）：锁定的是一行或几行记录。共享锁：select *from where <条件> LOCK IN SHARE MODE;，对查询的记录增加共享锁；select* from where <条件> FOR UPDATE;，对查询的记录增加排他锁。这里值得注意的是：innodb的行锁，其实是一个子范围锁，依据条件锁定部分范围，而不是就映射到具体的行上，因此还有一个学名：间隙锁。比如select * from stu where id < 20 LOCK IN SHARE MODE会锁定id在20左右以下的范围，你可能无法插入id为18或22的一条新纪录。 选择依据 如果没有特别的需求，使用默认的Innodb即可。\n\nMyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。\n\nInnodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键保证数据完整性。比如OA自动化办公系统。\n\n## 索引\n\n关键字与数据的映射关系称为索引（==包含关键字和对应的记录在磁盘中的地址==）。关键字是从数据当中提取的用于标识、检索数据的特定内容。 索引检索为什么快？ 关键字相对于数据本身，==数据量小== 关键字是==有序==的，二分查找可快速确定位置 图书馆为每本书都加了索引号（类别-楼层-书架）、字典为词语解释按字母顺序编写目录等都用到了索引。\n\nMySQL中索引类型 普通索引（key），唯一索引（unique key），主键索引（primary key），全文索引（fulltext key） 三种索引的索引方式是一样的，只不过对索引的关键字有不同的限制：\n\n普通索引：对关键字没有限制 唯一索引：要求记录提供的关键字不能重复 主键索引：要求关键字唯一且不为null 索引管理语法 查看索引 show create table 表名：\n\n## 索引的缺点\n\n1.创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加\n\n2.需占用额外的物理空间\n\n3.当对表中数据进行增加、删除和修改的时候，\n\n索引也要动态的维护，降低了数据的维护速度\n\n## 执行计划explain\n\n我们可以通过explain selelct来分析SQL语句执行前的执行计划： 执行计划是：当执行SQL语句时，首先会分析、优化，形成执行计划，在按照执行计划执行。\n\n## 索引使用场景（重点）\n\nwhere\n\norder by 当我们使用order by将查询结果按照某个字段排序时，如果该字段没有建立索引，那么执行计划会将查询出的所有数据使用外部排序（将数据从硬盘分批读取到内存使用内部排序，最后合并排序结果），这个操作是很影响性能的，因为需要将查询涉及到的所有数据从磁盘中读到内存（如果单条数据过大或者数据量过多都会降低效率），更无论读到内存之后的排序了。\n\n但是如果我们对该字段建立索引alter table 表名 add index(字段名)，那么由于索引本身是有序的，因此直接按照索引的顺序和映射关系逐条取出数据即可。而且如果分页的，那么只用取出索引表某个范围内的索引对应的数据，而不用像上述那取出所有数据进行排序再返回某个范围内的数据。（从磁盘取数据是最影响性能的）\n\njoin\n\n对join语句匹配关系（on）涉及的字段建立索引能够提高效率\n\nlike查询，不能以通配符开头 比如搜索标题包含mysql的文章：\n\nselect * from article where title like '%mysql%'; 这种SQL的执行计划用不了索引（like语句匹配表达式以通配符开头），因此只能做全表扫描，效率极低，在实际工程中几乎不被采用。而一般会使用第三方提供的支持中文的全文索引来做。\n\n但是 关键字查询 热搜提醒功能还是可以做的，比如键入mysql之后提醒mysql 教程、mysql 下载、mysql 安装步骤等。用到的语句是：\n\nselect * from article where title like 'mysql%'; 这种like是可以利用索引的（当然前提是title字段建立过索引）。\n\n## 索引的存储结构\n\nBTree btree（多路平衡查找树）是一种广泛应用于==磁盘上实现索引功能==的一种数据结构，也是大多数数据库索引表的实现。\n\n![img](img/btree.png)\n\nBTree的一个node可以存储多个关键字，node的大小取决于计算机的文件系统，因此我们可以通过减小索引字段的长度使结点存储更多的关键字。如果node中的关键字已满，那么可以通过每个关键字之间的子节点指针来拓展索引表，但是不能破坏结构的有序性，比如按照first_name第一有序、last_name第二有序的规则，新添加的韩香就可以插到韩康之后。白起 < 韩飞 < 韩康 < 李世民 < 赵奢 < 李寻欢 < 王语嫣 < 杨不悔。这与二叉搜索树的思想是一样的，只不过二叉搜索树的查找效率是log(2,N)（以2为底N的对数），而BTree的查找效率是log(x,N)（其中x为node的关键字数量，可以达到1000以上）。\n\n## 倒排索引\n\n在关系数据库系统里，索引是检索数据最有效率的方式,。但对于搜索引擎，它并不能满足其特殊要求： 1）海量数据：搜索引擎面对的是海量数据，像Google，百度这样大型的商业搜索引擎索引都是亿级甚至百亿级的网页数量 ，面对如此海量数据 ,使得数据库系统很难有效的管理。 2）数据操作简单：搜索引擎使用的数据操作简单 ,一般而言 ,只需要增、 删、 改、 查几个功能 ,而且数据都有特定的格式 ,可以针对这些应用设计出简单高效的应用程序。而一般的数据库系统则支持大而全的功能 ,同时损失了速度和空间。最后 ,搜索引擎面临大量的用户检索需求 ,这要求搜索引擎在检索程序的设计上要分秒必争 ,尽可能的将大运算量的工作在索引建立时完成 ,使检索运算尽量的少。一般的数据库系统很难承受如此大量的用户请求 ,而且在检索响应时间和检索并发度上都不及我们专门设计的索引系统。\n\n正向索引（正排索引）：正排表是以文档的ID为关键字，表中记录文档中每个字的位置信息，查找时扫描表中每个文档中字的信息直到找出所有包含查询关键字的文档。 正排表结构如图1所示，这种组织方法在建立索引的时候结构比较简单，建立比较方便且易于维护;因为索引是基于文档建立的，若是有新的文档加入，直接为该文档建立一个新的索引块，挂接在原来索引文件的后面。若是有文档删除，则直接找到该文档号文档对应的索引信息，将其直接删除。但是在查询的时候需对所有的文档进行扫描以确保没有遗漏，这样就使得检索时间大大延长，检索效率低下。 尽管正排表的工作原理非常的简单，但是由于其检索效率太低，除非在特定情况下，否则实用性价值不大。\n\n倒排索引（英语：Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。 　　\n\n倒排索引有两种不同的反向索引形式： 　　一条记录的水平反向索引（或者反向档案索引）包含每个引用单词的文档的列表。 　　一个单词的水平反向索引（或者完全反向索引）又包含每个单词在一个文档中的位置。 　　后者的形式提供了更多的兼容性（比如短语搜索），但是需要更多的时间和空间来创建。 　　现代搜索引擎的索引都是基于倒排索引。相比“签名文件”、“后缀树”等索引结构，“倒排索引”是实现单词到文档映射关系的最佳实现方式和最有效的索引结构。\n\n## select * 要少用\n\n即尽量选择自己需要的字段select，但这个影响不是很大，因为网络传输多了几十上百字节也没多少延时，并且现在流行的ORM框架都是用的select *，只是我们在设计表的时候注意将大数据量的字段分离，比如商品详情可以单独抽离出一张商品详情表，这样在查看商品简略页面时的加载速度就不会有影响了。\n\n## order by rand()不要用\n\n它的逻辑就是随机排序（为每条数据生成一个随机数，然后根据随机数大小进行排序）。如select * from student order by rand() limit 5的执行效率就很低，因为它为表中的每条数据都生成随机数并进行排序，而我们只要前5条。\n\n解决思路：在应用程序中，将随机的主键生成好，去数据库中利用主键检索。\n\n## 单表和多表查询\n\n多表查询：join、子查询都是涉及到多表的查询。如果你使用explain分析执行计划你会发现多表查询也是一个表一个表的处理，最后合并结果。因此可以说单表查询将计算压力放在了应用程序上，而多表查询将计算压力放在了数据库上。\n\n现在有ORM框架帮我们解决了单表查询带来的对象映射问题（查询单表时，如果发现有外键自动再去查询关联表，是一个表一个表查的）。\n\n## count(*)\n\n在MyISAM存储引擎中，会自动记录表的行数，因此使用count(*)能够快速返回。而Innodb内部没有这样一个计数器，需要我们手动统计记录数量，解决思路就是单独使用一张表：\n\nidtablecount1student100\n\n## limit 1\n\n如果可以确定仅仅检索一条，建议加上limit 1，其实ORM框架帮我们做到了这一点（查询单条的操作都会自动加上limit 1）。\n\n## 慢查询日志\n\n用于记录执行时间超过某个临界值的SQL日志，用于快速定位慢查询，为我们的优化做参考。 开启慢查询日志 配置项：slow_query_log\n\n可以使用show variables like ‘slov_query_log’查看是否开启，如果状态值为OFF，可以使用set GLOBAL slow_query_log = on来开启，它会在datadir下产生一个xxx-slow.log的文件。\n","tags":["数据库"]},{"title":"Mysql定时备份shell脚本","url":"/2019/02/23/pout/数据库/mysql/Mysql定时备份shell脚本/","content":"\n\n<!-- toc -->\n\n\n# MYSQL定时备份任务shell脚本\n\n## 备份的必要性##\n\n+ 每个公司的业务都是基于数据进行的，数据的存储基本都是数据库\n+ 保证了数据的安全，稳定，也就可以做到防范于未然。\n\n##冷备和热备\n\n+ 冷备份：\n  + 冷备份，off，快，时间点上恢复\n  + 冷备份发生在数据库已经正常关闭的情况下，当正常关闭时会提供给我们一个完整的数据库，冷备份是将关键性文件拷贝的另外位置的一种说法，对于备份数据库信息而言，冷备份是最快和最安全的方法。\n  + 冷备份的优点：\n    + 低度维护，安全\n    + 容易归档，拷贝文件\n    + 速度快\n    + 恢复简单\n  + 冷备份的缺点：\n    + 不能按表或按用户恢复\n    + 若磁盘空间有限，只能拷贝到磁带等其他外部存储设备上，速度会很慢。\n    + 恢复限制，在备份期间，数据库不能做读写写操作。\n+ 热备份：\n  + 在数据库运行时，直接进行备份，对运行的数据库没有影响。\n  + 热备份的优点：\n    + 可在表空间或数据文件级备份，备份时间段。\n    + 灵活，备份时不影响数据库的使用。\n    + 秒级恢复，能够恢复到某一时间点上。\n  + 热备份的缺点：\n    + 严谨性，尽量不要出错，否则后果很严重。\n    + 维护困难\n+ 温备：（只能读，不可写）\n    + 上锁（考虑持锁的长久）\n\n## MYSQL备份数据的几种方式##\n\n###into outfile###\n\n+ 利用`select into outfile +指定的路径` 实现指定表数据的备份 备份完的文件是文本文件\n\n + 在使用`select into outfile` 使用默认的用户是mysql在操作，使用`show variables like 'datadir'`会显示所属mysql用户的文件件具有一切权限\n\n + ```shell\n   MariaDB [exam]> show variables like 'datadir';\n   +---------------+-----------------+\n   | Variable_name | Value           |\n   +---------------+-----------------+\n   | datadir       | /var/lib/mysql/ |\n   +---------------+-----------------+\n   1 row in set (0.00 sec)\n   \n   ```\n\n + 指定路径可以直接放在mysql所属的文件夹下\n\n + 如果指定文件备份位置，会报错`ERROR 1 (HY000): Can't create/write to file '/data/test.txt' (Errcode: 13)` 这是代表mysql没有文件目录的操作权限。\n\n    + 解决办法 \n       + 直接更改文件目录的所属用户为mysql让mysql拥有一切权限，`clown mysql:mysql +指定文件夹的路径`\n       + 修改文件夹其它用户可以有写的权限。(不建议)\n       + 先备份至mysql所属文件夹内 进行mv\n\n + 可以使用`load data infile +备份完数据的路径 into table +表名` 进行恢复。\n\n### mysqldump##\n\n+ mysqldump 常用来做温备 （所以需要先对备份的数据施加锁，只能读不能写）本质上就是将指定数据库中的数据已sql语句的方式输出 使用重定向到指定的文件\n\n+ 施加读锁的方式：\n\n  + flush tables with read lock 施加锁，表示把位于内存上的表统统都同步到磁盘上，然后去施加读锁。\n  + unlock tables 释放锁。\n\n+ 语法：\n\n  + -A  --alldatabases 备份所有数据库，含create database\n  + -B  --databases db_name  指定备份的数据库，包含create database语句\n  + -E --events  备份相关的所有event scheduler（[事件调度器](https://blog.csdn.net/kelvin_yin/article/details/79128981 ))\n  + -R  --routines  备份所有存储过程和自定义函数\n  + --tirggers  备份表相关触发器，默认启用，用--skip-triggers，不备份触发器（[触发器](https://www.jb51.net/article/164675.htm )）默认开启\n  + --default-character-set=utf8(默认)\n  + --single-transaction 通过在一个事务中导出所有表从而创建一个一致性的快照，（只支持innodb，myisam不支持事务），从而有效的保证了dump文件，即正确的表内容和二进制日志位置。\n  + --master-data=2 选项开启式默认会打开lock-all-tables 一个是加锁，一个是获取log信息 =1和=2的在于log信息前者不注释，后者注释。\n  + lock-all-tables 在dump执行的时候会在表上加上读锁，保证数据一致性，innodb不需加上此参数，而使用 --single-tansaction\n  + --all-databases  导出所有的数据库 也可以指定表名\n  + -F --flush-logs  备份前滚动日志\n  + |gzip  对压缩的文件进行压缩（有效的节省磁盘空间）\n  + gzip -d 对压缩文件解压\n\n+ 全量备份\n\n  + 使用mysqldump工具进行\n\n    ```shell\n    [root@ax mysql]# mysqldump -uroot -p  --master-data=2 --databases 表名|gzip >/data/mysql/a1_`date +%F`.sql\n    ```\n\n  + 恢复\n\n    ```shell\n    [root@ax mysql]# mysql -uroot -p < 备份文件路径/备份文件名\n    ```\n\n+ 增量备份\n\n  + 使用mysqlbinlog工具进行，本质上就是指定备份文件的log开始偏移量和结束日志的偏移量\n\n    ```shell\n    [root@ax mysql]# mysqlbinlog --start-position=3210 /var/lib/mysql/mysql-bin.000009 > /data/mysql/a1_2019-12-19_17.sql \n    ```\n\n    + --start-position=开始的偏移量\n\n    + --stop-position=结束的偏移量\n\n    + 可以先使用`show master status` 来查看日志版本\n\n    + 通过find / -name 日志版本名来查找到路径\n\n    + 使用`mysqlbinlog --start-position=上次同步的结束位置 查询到的路径` 来查看需要同步的结束偏移量\n\n      ```shell\n      [root@ax mysql]# mysqlbinlog --start-position=0 /var/lib/mysql/mysql-bin.000010\n      SET TIMESTAMP=1576755138/*!*/;\n      insert into a1 values (4,'赵六'),(5,'冯七')\n      /*!*/;\n      # at 423\n      #191219 19:32:18 server id 1  end_log_pos 450 \tXid = 919\n      COMMIT/*!*/;\n      # at 450\n      #191219 19:32:32 server id 1  end_log_pos 531 \tQuery\tthread_id=334\texec_time=0\terror_code=0\n      SET TIMESTAMP=1576755152/*!*/;\n      drop database exam\n      /*!*/;\n      # at 531\n      #191219 19:50:53 server id 1  end_log_pos 574 \tRotate to mysql-bin.000011  pos: 4\n      DELIMITER ;\n      # End of log file\n      ROLLBACK /* added by mysqlbinlog */;\n      /*!50003 SET COMPLETION_TYPE=@OLD_COMPLETION_TYPE*/;\n      /*!50530 SET @@SESSION.PSEUDO_SLAVE_MODE=0*/;\n      \n      ```\n\n    + 其中end_log_pos就是结束的偏移量\n\n    + 或者直接使用`show master status` 中查询出来的偏移量，备份到操作该库的最后一次位置\n\n  + 恢复，和全量恢复是相同的  \n\n    + [root@ax mysql]# mysql -uroot -p < 备份文件路径/增量备份名备份文件名\n    + 恢复前先关闭二进制日志 `set sql_log_bin=0`\n    + 滚动日志 `flush logs`\n\n###利用lvm快照备份和恢复##\n\n+ LVM（Logical Volume Manager）是一个应用于Linux的内核的逻辑卷管理器，是Linux环境下对磁盘进行分区管理的一种机制。相关名词：\n  + PV（物理卷）可以是一个磁盘，一个分区。由PE（物理盘区）组成，多个PV可以组成一个VG（卷组）\n    + VG（卷组）多个物理卷组成的一个组，卷组不可以直接使用，需要在上面创建LV（逻辑卷）才可以使用。VG上可以创建多个LV。\n    + PE（物理盘区），默认是4MB,像磁盘的block块\n    + LV（逻辑卷）是建立在卷组之上的一个可用空间，有物理边界和逻辑边界两种边界。\n+ LVM扩展\n  +  [连接](https://www.jb51.net/LINUXjishu/105937.html )\n  +  LVM是一种将一个或多个硬盘的分区在逻辑上的集合，相当于一个大硬盘来使用，当空间不够使用时，可以继续将其它硬盘的分区加入其中，这样可以实现一种磁盘空间的动态管理，相对于普通的磁盘分区有很大的灵活性，使用普通的磁盘分区，当一个磁盘的分区空间不够使用的时候，可能就会带来很大的麻烦，使用LVM在一定程度就可以解决普通磁盘分区带来的问题。LVM通常用于装备大量磁盘的系统，但他它同样适于仅有一，两块硬盘的小系统。\n\n### Mysql定时备份shell脚本###\n\n+ 利用的都是mysqldump，利用Crontab设置定时任务，来自动执行备份命令。直接上代码。\n\n  ```shell\n  #!/bin/bash\n  \n  # 以下配置信息请自己修改\n  mysql_user=\"root\" #MySQL备份用户\n  mysql_password=\"password\" #MySQL备份用户的密码\n  mysql_host=\"localhost\"\n  mysql_port=\"3306\"\n  mysql_charset=\"utf8\" #MySQL编码\n  backup_db_arr=(\"db1\" \"db2\") #要备份的数据库名称，多个用空格分开隔开 如(\"db1\" \"db2\" \"db3\")\n  backup_location=/data/mysql #备份数据存放位置，末尾请不要带\"/\",此项可以保持默认，程序会自动创建文件夹\n  expire_backup_delete=\"ON\" #是否开启过期备份删除 ON为开启 OFF为关闭\n  expire_days=3 #过期时间天数 默认为三天，此项只有在expire_backup_delete开启时有效\n  \n  # 本行开始以下不需要修改\n  backup_time=`date +%F:%T` #定义备份详细时间\n  backup_Ymd=`date +%Y-%m-%d` #定义备份目录中的年月日时间\n  backup_3ago=`date -d '3 days ago' +%Y-%m-%d` #3天之前的日期\n  backup_dir=$backup_location/$backup_Ymd #备份文件夹全路径\n  welcome_msg=\"Welcome to use MySQL backup tools!\" #欢迎语\n  \n  # 判断MYSQL是否启动,mysql没有启动则备份退出\n  mysql_ps=`ps -ef |grep mysql |wc -l`\n  mysql_listen=`netstat -an |grep LISTEN |grep $mysql_port|wc -l`\n  if [ [$mysql_ps == 0] -o [$mysql_listen == 0] ]; then\n  echo \"`date +%F:%T` --ERROR:MySQL is not running! backup stop!\"\n  exit\n  else\n  echo \"备份时间-- `date +%F:%T`\"\n  echo `date +%F:%T` -- $welcome_msg\n  fi\n  \n  # 连接到mysql数据库，无法连接则备份退出\n  mysql -h$mysql_host -P$mysql_port -u$mysql_user -p$mysql_password <<end\n  use mysql;\n  select host,user from user where user='root' and host='localhost';\n  exit\n  end\n  \n  flag=`echo $?`\n  if [ $flag != \"0\" ]; then\n  echo \"`date +%F:%T` --ERROR:Can't connect mysql server! backup stop!\"\n  exit\n  else\n  echo \"`date +%F:%T` --MySQL connect ok! Please wait......\"\n  # 判断有没有定义备份的数据库，如果定义则开始备份，否则退出备份\n  if [ \"$backup_db_arr\" != \"\" ];then\n  #dbnames=$(cut -d ',' -f1-5 $backup_database)\n  #echo \"arr is (${backup_db_arr[@]})\"\n  for dbname in ${backup_db_arr[@]}\n  do\n  echo \"`date +%F:%T` --database $dbname backup start...\"\n  `mkdir -p $backup_dir`\n  `mysqldump -h$mysql_host -P$mysql_port -u$mysql_user -p$mysql_password $dbname --default-character-set=$mysql_charset | gzip > $backup_dir/$dbname-$backup_time.sql.gz`\n  flag=`echo $?`\n  if [ $flag == \"0\" ];then\n  echo \"`date +%F:%T` --database $dbname success backup to $backup_dir/$dbname-$backup_time.sql.gz\"\n  else\n  echo \"`date +%F:%T` --database $dbname backup fail!\"\n  fi\n  \n  done\n  else\n  echo \"`date +%F:%T` --ERROR:No database to backup! backup stop\"\n  exit\n  fi\n  # 如果开启了删除过期备份，则进行删除操作\n  if [ \"$expire_backup_delete\" == \"ON\" -a \"$backup_location\" != \"\" ];then\n  #`find $backup_location/ -type d -o -type f -ctime +$expire_days -exec rm -rf {} \\;`\n  `find $backup_location/ -type d -mtime +$expire_days | xargs rm -rf`\n  echo \"`date +%F:%T` --Expired backup data delete complete!\"\n  fi\n  echo \"`date +%F:%T` --All database backup success! Thank you!\"\n  echo \"\"\n  exit\n  fi\n  \n  ```\n\n  \n\n+ 在非linux界面下操作，sh脚本中每行都会多个\\r，导致linux无法执行脚本。解决办法，使用vim编辑shell脚本文件，执行::set ff=unix。文件即使unix文件了。可以使用::set ff? 查看文件的类型。dos就是在ATOM操作后保存的文件。\n\n  ```shell\n    fileformat=unix \n    fileformat=dos \n  ```\n\n+ 文件的创建默认UGO权限为 -rw-r--r-- 代表的这个文件没有执行权限，所以需要给文件增加权限。\n\n  + 文件创建者拥有所有权限，用户组和其他组只有执行权限\n\n    `chmod 711 备份文件`\n\n  + r=4 w=2 x=1 \n\n+ 拥有了执行权限后，该shell脚本就可以执行了，可以执行一下\n\n  ```shell\n  [root@ax auto_backup]# ./Backup.sh \n  备份时间-- 2019-12-20:17:09:40\n  2019-12-20:17:09:40 -- Welcome to use MySQL backup tools!\n  host\tuser\n  localhost\troot\n  2019-12-20:17:09:40 --MySQL connect ok! Please wait......\n  2019-12-20:17:09:40 --database exam backup start...\n  2019-12-20:17:09:40 --database exam success backup to /data/mysql/2019-12-20/exam-2019-12-20:17:09:40.sql.gz\n  2019-12-20:17:09:40 --database shiyouyun backup start...\n  mysqldump: Got error: 1049: \"Unknown database 'shiyouyun'\" when selecting the database\n  2019-12-20:17:09:40 --database shiyouyun success backup to /data/mysql/2019-12-20/shiyouyun-2019-12-20:17:09:40.sql.gz\n  2019-12-20:17:09:40 --Expired backup data delete complete!\n  2019-12-20:17:09:40 --All database backup success! Thank you!\n  \n  ```\n\n+ 执行成功说明shell没有问题，只剩下写入定时任务了\n\n#### Crontab####\n\n+ linux下使用crontab命令来提交和管理用户需要定时、周期性的执行任务。\n\n+ linux下默认会安装此服务工具，并且会自动启动crond进程，crond进程会每分钟定期检查是否有要执行的任务。如果有要执行的任务，则自动执行该任务。\n\n+ 命令语法：\n\n  + crontab（选项） （参数）\n  + 选项：\n    + -e  编辑该用户的计时设置\n    + -l   列出该用户的计时器设置\n    + -r   删除该用户的计时器设置\n    + -u   用户名称   指定要设定计时器的用户名称\n  + 参数：\n    + crontab文件   指定包含待执行任务的crontab文件\n\n+ 直接在/etc/crontab文件中添加shell脚本任务\n\n  ```shell\n  00 3 * * * root /data/auto_backup/Backup.sh >> /data/mysql/backup.log\n  ```\n\n  + 上面代表每天在凌晨3点运行Backup.sh脚本 并将输出重定向追加到backup.log文件中。\n  + 从左往右依次代表\n    + 分 可取0-59的整数 加/数字 代表每分钟执行多少次\n    + 时 可取0-23的整数  \n    + 天  可取1-31的整数 ，必须是指定月份的有效日期\n    + 月 可取1-12的整数，也可写英文简写\n    + 周几 可取1-7的整数，描述周几\n    + 执行的用户  指定用户\n    + shell脚本的路径\n    + log日志的路径\n\n+ 添加完后保存，重启crond服务。一切ok\n\n  ```\n  [root@ax mysql]# ls\n  2019-12-20  backup.log\n  [root@ax mysql]# cd 2019-12-20/\n  [root@ax 2019-12-20]# ls\n  exam-201912201600.sql.gz  exam-201912201613.sql.gz         exam-2019-12-20:16:31:01.sql.gz  shiyouyun-201912201608.sql.gz  shiyouyun-2019-12-20:16:21:01.sql.gz\n  exam-201912201601.sql.gz  exam-201912201614.sql.gz         exam-2019-12-20:16:32:01.sql.gz  shiyouyun-201912201609.sql.gz  shiyouyun-2019-12-20:16:22:01.sql.gz\n  [root@ax mysql]# cat backup.log \n  备份时间-- 2019-12-20:16:30:01\n  2019-12-20:16:30:01 -- Welcome to use MySQL backup tools!\n  host\tuser\n  localhost\troot\n  2019-12-20:16:30:01 --MySQL connect ok! Please wait......\n  2019-12-20:16:30:01 --database exam backup start...\n  2019-12-20:16:30:01 --database exam success backup to /data/mysql/2019-12-20/exam-2019-12-20:16:30:01.sql.gz\n  2019-12-20:16:30:01 --database shiyouyun backup start...\n  2019-12-20:16:30:01 --database shiyouyun success backup to /data/mysql/2019-12-20/shiyouyun-2019-12-20:16:30:01.sql.gz\n  2019-12-20:16:30:01 --Expired backup data delete complete!\n  2019-12-20:16:30:01 --All database backup success! Thank you!\n  \n  备份时间-- 2019-12-20:16:31:01\n  2019-12-20:16:31:01 -- Welcome to use MySQL backup tools!\n  host\tuser\n  localhost\troot\n  2019-12-20:16:31:01 --MySQL connect ok! Please wait......\n  2019-12-20:16:31:01 --database exam backup start...\n  2019-12-20:16:31:01 --database exam success backup to /data/mysql/2019-12-20/exam-2019-12-20:16:31:01.sql.gz\n  2019-12-20:16:31:01 --database shiyouyun backup start...\n  2019-12-20:16:31:01 --database shiyouyun success backup to /data/mysql/2019-12-20/shiyouyun-2019-12-20:16:31:01.sql.gz\n  2019-12-20:16:31:01 --Expired backup data delete complete!\n  2019-12-20:16:31:01 --All database backup success! Thank you!\n  \n  ```\n\n  \n\n","tags":["数据库"]},{"title":"qlc解决了什么问题","url":"/2019/02/16/pout/qlc解决了什么问题/","content":"\n### 目前遇到的问题\n\n作为一名前端工程师，陆陆续续负责了不少项目，这些项目中，有一些是正在迭代的，其他同事同时在负责的项目，但是也有不少项目，要么就是老旧项目维护的同事已经离职或转岗了的，要么就是新项目从 0 开始的，再加上前端代码积累速度和迭代速度都比较快，其中暴露了不少问题。\n\n我身边的大多数程序员都有一个特点，就是喜欢把具体的东西抽象化，我们通常会抽象出公共的函数或方法、公共的类或HOC，放在一起，集中在项目的某一个文件夹下，叫做 js 文件夹或 lib 文件夹（以下我们用 js 文件夹代表公共函数文件夹 ）。\n\n这样做的确带来了很多便利，但同时也有一些隐患：\n\n* js 文件夹下代码越来越多，而且大多数鹅厂小伙伴的作风是 0 文档 0 注释，这给新接手项目的同学熟悉项目带来了极大的麻烦。\n* 不同项目都有自己的 js 文件夹，在开发一个新项目时，我们通常的做法是：\n\t* 直接将原有项目的 js 文件夹拷贝到新项目中，这样在新项目中，我们也可以直接使用这些公共函数了。\n\t* 将原有项目的部分 js 文件拷贝到新项目中，并且随着新项目的开发，增量拷贝。\n\t* 以上两种做法，本质区别不大，前者会直接给新项目增加很多无用代码（原有项目中所谓的公共函数在新项目中并不一定会用到），而对这两种方式，如果我们要修复一个 bug，修改或升级公共代码中的一个文件，那么我们就要一个一个的，将修复好的文件拷贝到不同的项目中，如果项目多了并且已经交由不同的人维护了，这简直是一个灾难。\n* 由于公共 js 文件夹下内容比较多，并且有的开发同学习惯以 'urlUtils.js'、'strUtils.js' 这种方式来整合一些小的函数集，这样会造成函数重复的隐患（毕竟我们一个文件一行行的去分析目前的公共库已经有了哪些能力是不现实的），我观察过之前自己接手的一个不算复杂的项目（潘多拉），其仅仅是从 url 解析 query 这种功能函数，就有多达 3个(甚至更多)，分布在 js 文件夹以及 node_modules 里面，这显然是不同的维护人员由于信息不对称重复引入的。\n* 对于怎么样才能算作“公共”函数，目前是缺乏一个 review 过程的，任何项目开发人员，几乎都可以无限制地在公共 js 文件夹下增加内容，并在之后被携带着拷贝到其他项目中，这其中有些函数，也许并不适合在这里。\n\n### 问题归纳与解决\n\n实际上，总结下来，我们需要解决三个痛点：\n\n* 以低成本的方式增加高可读性的文档，方便新接手项目的同学熟悉。\n* 解决跨项目之间的公共函数复用和更新维护困难的问题。\n* 增加必要的 review 环节，对公共函数库的必要性和代码正确性进行 review。\n\n就第一个问题而言，其实现在的前端文档工具链已经极大降低了写文档的成本了，利用 [jsdoc](http://usejsdoc.org/) 或 [esdoc](https://esdoc.org/) 等文档生成工具，我们基本上已经不需要手动写文档，而是在写代码的同时写注释，就可以自动生成文档，并且配合相关的编辑器插件，一部分注释都可以自动生成。\n\n但是目前我经历的大多数项目还是没有文档，这里可能是由于以下四个原因：\n\n* 部分同学并不知道有 esdoc、jsdoc 这种比较好用的文档生成工具。\n* 开发组中没有人去推动，文档不属于 KPI 和考核的内容，加之时间紧迭代快，缺乏前人栽树的动力。\n* 虽然现在的文档生成工具比较简单了，但一般还是需要一定的配置，也有一点上手成本。\n* 生成的文档不能十分满足需求，例如 esdoc 默认只能生成 html 格式的文档，在编辑器里面没法直接看。\n\n针对第一个问题，qlc 做出了一些努力：\n\n* 0 配置，全自动化生成文档，甚至集成到了其他开发流程中，命令行也不用敲。\n* 基于 esdoc 以及开源插件二次开发，可以选择性生成 html 和 markdown 格式的文档，注重文档体验。\n* 基于 esdoc 注释写作成本更低，更能节省时间。\n\n>跟 jsdoc 相比，esdoc 使用方式比较简单，不需要严格使用标签，而且能够支持搜索，并且官方资料更为齐全。\n\n至此，使用 qlc 生成文档，已经非常简单了。\n\n第二、第三个问题实际上是公共函数库的维护问题，qlc 也设计了对应的流程，着力解决该问题：\n\n* 首先有一个远程公共库（基于 git）。\n* 对于某一个项目而言，可以从远程库中下拉所需要的公共函数/类，并自动生成文档。\n* 如果我们对某一个项目增加了一个公共函数并且认为可以为更多的项目所用，命令行上传到远程库自动触发 MR，维护人员 review 通过后即可供其他项目使用。\n* 修复或更新某一个公共函数之后，我们只需同步到远程库，其他项目维护人员在命令行工具的辅助下同步即可。\n\n### 更多\n\n到此，你是否认为 qlc 给你带来了一定的价值呢，可以到 qlc 的官方仓库查看更多的[文档细节](https://git.code.oa.com/qlc/qlc)\n\n\n\n\n\n","tags":["javascript"]},{"title":"入门WebAssembly以及使用其进行图像卷积处理","url":"/2019/02/16/pout/后端技术/入门WebAssembly以及使用其进行图像卷积处理/","content":"\n> WebAssembly 出现有很长时间了，但是由于日常工作并无直接接触，因此一直疏于尝试，最近终于利用一些业余时间简单入门了一下，因此在此总结。\n\n### 简介\n\n首先我们需要知道 WebAssembly 是一个什么东西，其实际是一个字节码编码方式，比较接近机器码（但是又无法直接执行），这样可以方便地做到跨平台同时省去像 JavaScript 等语言的解释时间，所以是有一定优势的，使用起来其实也比较灵活，凡是可以转化成字节码的，都是可以使用 WebAssembly。\n\n以下仅列举部分可以支持 WebAssembly 转化的语言：\n\n* [AssemblyScript](https://github.com/AssemblyScript/assemblyscript): 语法和 TypeScript 一致(事实上，其是 Typescript 的一个子集)，对前端来说学习成本低，为前端编写 WebAssembly 最佳选择；\n* c\\c++: 官方推荐的方式，详细使用见[文档](http://webassembly.org.cn/getting-started/developers-guide/);\n* [Rust](https://www.rust-lang.org/): 语法复杂、学习成本高，对前端来说可能会不适应。详细使用见[文档](https://github.com/rust-lang-nursery/rust-wasm);\n* [Kotlin](http://kotlinlang.org/): 语法和 Java、JS 相似，语言学习成本低，详细使用见[文档](https://kotlinlang.org/docs/reference/native-overview.html);\n* [Golang](https://golang.org/): 语法简单学习成本低。但对 WebAssembly 的支持还处于未正式发布阶段，详细使用见[文档](https://blog.gopheracademy.com/advent-2017/go-wasm/)。\n\n尝试使用 WebAssembly 官方推荐的方式，我们首先可以在[这里](http://webassembly.org.cn/getting-started/developers-guide/)来下载。\n\n如果用腾讯内网有的文件是下载不下来的，这个时候我们可以给命令行增加一个代理（如果我们用的 Fiddler 或 Charles，开启的时候默认命令行也可以走代理，如果是 Whistle，我们需要手动设置代理），有些文件我们还可以下载好之后使用文件代理。\n\n```\nexport https_proxy=\"http://127.0.0.1:8899\"\nexport http_proxy=\"http://127.0.0.1:8899\"\n// 文件代理：\nhttps://s3.amazonaws.com/mozilla-games/emscripten/packages/node-v8.9.1-darwin-x64.tar.gz file:///Users/niexiaotao/node-v8.9.1-darwin-x64.tar.gz\n```\n\n## 初体验\n\n这里考虑到前端同学的上手难度，我们先使用 AssemblyScript 写一个极小的例子，一个斐波那契函数：\n\n```\nexport function f(x: i32): i32 {\n    if (x === 1 || x === 2) {\n        return 1;\n    }\n    return f(x - 1) + f(x - 2)\n}\n```\n\n通过类似 `asc f.ts -o f.wasm` 这样的命令编译成 f.wasm 之后，我们可以分别在 Node 环境和浏览器环境来执行：\n\nNode：\n\n```\nconst fs = require(\"fs\");\nconst wasm = new WebAssembly.Module(\n    fs.readFileSync(__dirname + \"/f.wasm\"), {}\n);\nconst myModule = new WebAssembly.Instance(wasm).exports;\nconsole.log(myModule.f(12));\n```\n\n浏览器：\n\n```\nfetch('f.wasm') // 网络加载 f.wasm 文件\n        .then(res => res.arrayBuffer()) // 转成 ArrayBuffer\n        .then( buffer =>\n            WebAssembly.compile(buffer)\n        )\n        .then(module => { // 调用模块实例上的 f 函数计算\n            const instance = new WebAssembly.Instance(module);\n            const { f } = instance.exports;\n            console.log('instance:', instance.exports);\n            console.log('f(20):', f(20));\n        });\n```\n\n于是，我们完成了 WebAssembly 的初体验。\n\n当然，这个例子太简单了。\n\n## 使用 WebAssembly 进行图像卷积处理\n\n实际上，WebAssembly 的目的在于解决一些复杂的计算问题，优化 JavaScript 的执行效率。所以我们可以使用 WebAssembly 来处理一些图像或者矩阵的计算问题。\n\n接下来，我们通过 WebAssembly 来处理一些图像的卷积问题，用于图像的风格变换，我们最终的例子可以在[这里](http://assembly.niexiaotao.com/)体验。\n\n每次进行卷积处理，我们的整个流程是这样的：\n\n* 将原图像使用 canvas 绘制到屏幕上。\n* 使用 `getImageData` 获取图像像素内容，并转化成类型数组。\n* 将上述类型数组通过共享内存的方式传递给 WebAssembly 部分。\n* WebAssembly 部分接收到数据，进行计算，并且通过共享内存的方式返回。\n* 将最终结果通过 canvas 画布更新。\n\n上述各个步骤中，绘制的部分集中在 JavaScript 端，而计算的部分集中在 WebAssembly，这两部分相互比较独立，可以分开编写，而双端数据通信是一个比较值得注意的地方，事实上，我们可以通过 ArrayBuffer 来实现双端通信，简单的说，JavaScript 端和 WebAssembly 可以共享一部分内存，并且都拥有读写能力，当一端写入新数据之后，另一段也可以读到，这样我们就可以进行通信了。\n\n关于数据通信的问题，这里还有一个比较直白的[科普文章](https://segmentfault.com/a/1190000010434237)，可以参考。\n\n在这里没有必要对整个项目代码进行展示，因此可以参考（[代码地址](https://github.com/aircloud/assemConvolution)），我们这里仅仅对部分关键代码进行说明。\n\n### 共享内存\n\n首先，我们需要声明一块共享内存，这其实可以使用 WebAssembly 的 API 来完成：\n\n```\nlet memory = new WebAssembly.Memory({ initial: ((memSize + 0xffff) & ~0xffff) >>> 16 });\n```\n\n这里经过这样的比较复杂的计算是因为 initial 传入的是以 page 为单位，详细可以参考[这里](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/WebAssembly/Memory)，实际上 memSize 即我们共享内存的字节数。\n\n然后这里涉及到 memSize 的计算，我们主要需要存储三块数据：卷积前的数据、卷积后的数据（由于卷积算法的特殊性以及为了避免更多麻烦，这里我们不进行数据共用），还有卷积核作为参数需要传递。\n\n这里我们共享内存所传递的数据按照如下的规则进行设计：\n\n![](http://niexiaotao.cn/img/ker1.jpg)\n\n传递给 WebAssembly 端的方式并不复杂，直接在 `WebAssembly.instantiate` 中声明即可。 \n\n```\nfetch(wasmPath)\n     .then(response => response.arrayBuffer())\n     .then(buffer => WebAssembly.instantiate(buffer, {\n         env: {\n             memory,\n             abort: function() {}\n         },\n         Math\n     })).then(module => {})\n                \n```\n\n然后我们在 AssemblyScript 中就可以进行读写了：\n\n```\n//写：\nstore<u32>(position, v) // position 为位置\n\n//读：\nload<u32>(position) // position 为位置\n```\n\n而在 JavaScript 端，我们也可以通过 `memory.buffer` 拿到数据，并且转化成类型数组：\n\n```\nlet mem = new Uint32Array(memory.buffer)\n//通过 mem.set(data) 可以在 JavaScript 端进行写入操作\n```\n\n这样，我们在 JavaScript 端和 AssemblyScript 端的读写都明晰了。\n\n这里需要注意的是，**JS端采用的是大端数据格式，而 AssemblyScript 中采用的是小端，因此其颜色数据格式为 AGBR**\n\n### 卷积计算\n\n我们所采用的卷积计算本身算法比较简单，并且不是本次的重点，但是这里需要注意的是：\n\n* 我们无法直接在 AssemblyScript 中声明数组并使用，因此除了 Kernel 通过共享内存的方式传递过来以外，我们应当尽量避免声明数组使用（虽然也有使用非共享内存数组的相关操作，但是使用起来比较繁琐）\n* 卷积应当对 R、G、B 三层单独进行，我这里 A 层不参与卷积。\n\n以上都在代码中有所体现，参考相关代码便可明了。\n\n卷积完成后，我们通过共享内存的方法写入类型数组，然后在 JavaScript 端合成数据，调用 `putImageData` 上屏即可。\n\n### 其他\n\n当然，本次图像卷积程序仅仅是对 Webassembly 和 AssemblyScript 的初步尝试，笔者也在学习阶段，如果上述说法有问题或者你想和我交流，也欢迎留言或者提相关 issue。\n","tags":["WebAssembly"]},{"title":"在Centos下使用Siege对Django服务进行压力测试","url":"/2019/02/13/pout/运维/在Centos下使用Siege对Django服务进行压力测试实际操作/","content":"\n\n<!-- toc -->\n\n\n# 在Centos下使用Siege对Django服务进行压力测试\n\n[首页](https://v3u.cn/) - [Mac & Linux](https://v3u.cn/l_id_4) /2019-05-31\n\n​    Siege是linux下的一个web系统的压力测试工具，支持多链接，支持get和post请求，可以对web系统进行多并发下持续请求的压力测试。今天我们就使用Siege来对Django进行一次压力测试，看看单台Django服务到底能抗住多少的并发数。\n\n​    首先安装Siege\n\n​    \n\n```\nwget http://download.joedog.org/siege/siege-3.0.8.tar.gz\ntar zxvf siege-3.0.8.tar.gz\ncd siege-3.0.8\n./configure\nmake\nmake install\n\n验证安装结果：输入siege -V 如果输出了版本号就代表安装没问题\n```\n\nSiege命令常用参数\n\n-c 200 指定并发数200\n-r 5 指定测试的次数5\n-f urls.txt 制定url的文件\n-i internet系统，随机发送url\n-b 请求无需等待 delay=0\n-t 5 持续测试5分钟\n\n测试指标说明：\n\nTransactions: 4 hits 完成4次处理 \nAvailability: 100.00 % 成功率 \nElapsed time: 1.19 secs 总共用时\nData transferred: 0.03MB 共数据传输：0.03MB\nResponse time: 0.13 secs 相应用时0.13秒，显示网络连接的速度\nTransaction rate: 3.36 trans/sec平均每秒完成3.36次处理，表示服务器后台处理的速度\nThroughput: 0.03MB/sec 平均每秒传送数据：0.03MB\nConcurrency: 0.45 最高并发数 0.45\nSuccessful transactions: 4成功处理次数\nFailed transactions: 0 失败处理次数\nLongest  transaction：0.25请求最长响应时间/每次传输所花最长时间\nShortest  transaction：0.09请求最短响应时间/每次传输所花最短时间\n\n主要参考指标是 Transaction rate\n\n测试背景:  \n\n软件：python3.7.2 Django2.0.4 \n\n硬件 内存:1g cpu:1个1核  这个硬件配置有点惨，没办法了，因为没钱买好的\n\n业务场景：Django使用mysql进行普通的读操作，没有使用任何缓存\n\n压测命令：255个用户并发访问localhost:8000，持续时间为1分钟\n\n```\nsiege -c255 -t60S -v -b 127.0.0.1:8000\n```\n\n首先使用runserver的起服务方式进行压测：\n\n```\npython3 manage.py runserver 0.0.0.0:8000\n```\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190531/20190531073440_46036.png)\n\n可以看到，这个有点凄惨，每秒后台只能处理166的请求，失败次数也有点高，更加说明了，runserver最好就是本地调试开发的时候用用就可以了，在生产环境使用runserver无异于自杀，不过在一些测试服务器上，如果懒得搭建uwsgi或者gunicorn，可以使用nohup配合runserver临时用一下。\n\n使用uwsgi来起服务，uwsgi作为一款高性能的服务器，安装方式请见：<https://v3u.cn/a_id_72> 起8个worker\n\n```\nuwsgi --http :8000 --module mypro.wsgi --processes 8\n```\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190531/20190531094643_42366.png)\n\n可以看到使用了uwsgi的提升还是很可观的，失败次数也减少了一半左右\n\n​    最后，我们来试一试Gunicorn\n\n​    Gunicorn是使用Python实现的WSGI服务器, 直接提供了http服务, 并且在woker上提供了多种选择, gevent, eventlet这些都支持, 在多worker最大化里用CPU的同时, 还可以使用协程来提供并发支撑, 对于网络IO密集的服务比较有利\n\n安装 gunicorn\n\n```\npip3 install gunicorn\n```\n\n起4个worker,50个线程\n\n```\ngunicorn --env DJANGO_SETTINGS_MODULE=mypro.settings mypro.wsgi:application -w 4 -b 0.0.0.0:8000 -k gthread --threads 50\n```\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190531/20190531100545_47571.png)\n\n可以看到性能上和uwsgi差不太多，但是失败数比较多。以1g1核的服务器，并发阈值也就在200左右了。\n\n​    综上，单以性能论，Django的表现并非很好，但是你不能忽略它的学习成本低，简单并且容易上手的优势，鱼与熊掌不能兼得，如果要求高性能，可以试试tornado, 如果tornado依然无法满足，可以尝试使用golang，毕竟golang是以高并发著称的编译语言，而且基于它的web框架也很容易上手，性能很可观，例如Iris。\n\n","tags":["运维"]},{"title":"Redis主从","url":"/2019/02/03/pout/数据库/Redis/redis主从复制/","content":"\n\n<!-- toc -->\n\n## Redis主从\n**参考链接：**\n\n[Redis主从复制和哨兵 参考1](https://www.cnblogs.com/leeSmall/p/8398401.html)\n\n[Redis主从复制和哨兵 参考2](https://www.cnblogs.com/chenhuabin/p/10048854.html)\n\n[Redis主从架构和主从从架构集群搭建详细步骤](https://www.cnblogs.com/lxx666/articles/10693844.html)\n\n[Redis主从复制原理](https://www.cnblogs.com/wade-luffy/p/9639986.html)\n\n[Redis复制官方文档翻译](http://doc.redisfans.com/topic/replication.html)\n\n> ​\t主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master/leader)，后者称为从节点(slave/follower)；数据的复制是单向的，只能由主节点到从节点。\n>\n> ​\t默认情况下，每台Redis服务器都是主节点；且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。\n\n### 主从复制的作用\n\n1.  **数据冗余：**主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。\n\n2.  **故障恢复：**当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。\n\n3.  **负载均衡：**在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。\n\n4.  **高可用基石：**主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。\n\n   \n\n### 主从拓扑结构\n\n​\t**一主一从：**\n\n![1539768-20181201112713274-1680086476](../../img/6996ce2ff55342fb040221d6bf914bf.png)\n\n　　这一结构主要用于主节点故障从节点，当主节点的“写”命令并发高且需要持久化，可以只在从节点开启AOF（主节点不需要），这样即保证了数据的安全性，也避免持久化对主节点的影响。\n　　\n​\t**一主多从：**\n\n![1539768-20181201112740108-1961344396](../../img/c05fa5ff18646dde563e6f8bf5e98be.png)\n\n\n\n　　这一结构主要针对“读”较多的场景，“读”由多个从节点来分担，但节点越多，主节点同步到多节点的次数也越多，影响带宽，也加重主节点的稳定。\n　　\n​\t**树状主从:**\n\n![1539768-20181201112824130-1051891659](../../img/917d24ce5cee2ad4663d5bff1b5ada0.png)\n\n　　这一结构是对一主多从的补充，主节点只推送一次数据到slave1和slave2，再由从slave2推送到slave3和 slave4，减轻主节点推送的压力。\n\n\n\n\n\n## 主从复制的实现原理\n\n主从复制过程大体可以分为3个阶段：**连接建立阶段（即准备阶段）**、**数据同步阶段**、**命令传播阶段**；\n\n\n\n### 连接建立阶段\n\n#### step1：保存主节点信息\n\n​\t从节点服务器内部维护了两个字段，即**masterhost**和**masterport**字段，用于存储主节点的**ip**和**port**信息。\n\n​\t**slaveof是异步命令，从节点完成主节点ip和port的保存后，向发送slaveof命令的客户端直接返回OK，实际的复制操作在这之后才开始进行。**\n\n#### step2：建立socket连接\n\n​\t**从节点每秒1次调用复制定时函数replicationCron()**，如果发现了有主节点可以连接，便会根据主节点的ip和port，创建socket连接。\n\n**如果连接成功：**\n\n​\t**从节点：**为该socket建立一个专门处理复制工作的文件事件处理器，负责后续的复制工作，如接收RDB文件、接收命令传播等。\n\n​\t**主节点：**接收到从节点的socket连接后（即accept之后），为该socket创建相应的客户端状态，并将从节点看做是连接到主节点的一个客户端，后面的步骤会以从节点向主节点发送命令请求的形式来进行。\n\n#### step3：发送ping命令\n\n​\t从节点成为主节点的客户端之后，发送ping命令进行首次请求，**目的是：检查socket连接是否可用，以及主节点当前是否能够处理请求。**\n\n**从节点发送ping命令后，可能出现3种情况：**\n\n1. 返回pong：说明socket连接正常，且主节点当前可以处理请求，复制过程继续。\n\n2. 超时：一定时间后从节点仍未收到主节点的回复，说明socket连接不可用，则从节点断开socket连接，并重连。\n\n3. 返回pong以外的结果：如果主节点返回其他结果，如正在处理超时运行的脚本，说明主节点当前无法处理命令，则从节点断开socket连接，并重连。\n\n#### step4：身份验证\n\n如果从节点中设置了**masterauth**选项，则从节点需要向主节点进行身份验证；没有设置该选项，则不需要验证。\n\n从节点进行身份验证是通过向主节点发送auth命令进行的，auth命令的参数即为配置文件中的masterauth的值。\n\n如果主节点设置密码的状态，与从节点masterauth的状态一致（一致是指都存在，且密码相同，或者都不存在），则身份验证通过，复制过程继续；如果不一致，则从节点断开socket连接，并重连。\n\n#### step5：发送从节点端口信息\n\n身份验证之后，从节点会向主节点发送其监听的端口号，主节点将该信息保存到该从节点对应的客户端的slave_listening_port字段中；**该端口信息除了在主节点中执行info Replication时显示以外，没有其他作用。**\n\n\n\n### 数据同步阶段\n\n主从节点之间的连接建立以后，便可以开始进行数据同步，该阶段可以理解为从节点数据的初始化。\n\n具体执行的方式是：从节点向主节点发送**psync命令**，开始同步。\n\n数据同步阶段是主从复制最核心的阶段，根据主从节点当前状态的不同，可以分为**全量复制和部分复制**。\n\n> 在数据同步阶段之前，从节点是主节点的客户端，主节点不是从节点的客户端；而到了这一阶段及以后，主从节点互为客户端。原因在于：在此之前，主节点只需要响应从节点的请求即可，不需要主动发请求，而在数据同步阶段和后面的命令传播阶段，主节点需要主动向从节点发送请求（如推送缓冲区中的写命令），才能完成复制。\n\n\n\n###  命令传播阶段\n\n​\t数据同步阶段完成后，主从节点进入命令传播阶段；在这个阶段主节点将自己执行的写命令发送给从节点，从节点接收命令并执行，从而保证主从节点数据的一致性。\n\n​\t在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：PING和REPLCONF ACK。\n\n**PS：**\n\n​\t**延迟与不一致：**命令传播是异步的过程，即主节点发送写命令后并不会等待从节点的回复；因此实际上主从节点之间很难保持实时的一致性，延迟在所难免。数据不一致的程度，与主从节点之间的网络状况、主节点写命令的执行频率、以及主节点中的repl-disable-tcp-nodelay配置等有关。\n\n​\t**repl-disable-tcp-nodelay no：**该配置作用于命令传播阶段，控制主节点是否禁止与从节点的TCP_NODELAY；默认no，即不禁止TCP_NODELAY。当设置为yes时，TCP会对包进行合并从而减少带宽，但是发送的频率会降低，从节点数据延迟增加，一致性变差；具体发送频率与Linux内核的配置有关，默认配置为40ms。当设置为no时，TCP会立马将主节点的数据发送给从节点，带宽增加但延迟变小。一般来说，只有当应用对Redis数据不一致的容忍度较高，且主从节点之间网络状况不好时，才会设置为yes；多数情况使用默认值no。\n\n\n\n### 【数据同步阶段】全量复制和部分复制\n\n在Redis2.8以前，从节点向主节点发送sync命令请求同步数据，此时的同步方式是全量复制；\n\n在Redis2.8以后，从节点可以发送psync命令请求同步数据，此时根据主从节点当前状态的不同，同步方式可能是全量复制或部分复制。\n\n1. 全量复制：用于初次复制或其他无法进行部分复制的情况，将主节点中的所有数据都发送给从节点，是一个非常重型的操作。\n2. 部分复制：用于网络中断等情况后的复制，只将中断期间主节点执行的写命令发送给从节点，与全量复制相比更加高效。需要注意的是，如果网络中断时间过长，导致主节点没有能够完整地保存中断期间执行的写命令，则无法进行部分复制，仍使用全量复制。\n\n\n\n#### 全量复制\n\n**Redis通过psync命令进行全量复制的过程如下：**\n\n1. 从节点判断无法进行部分复制，向主节点发送全量复制的请求；或从节点发送部分复制的请求，但主节点判断无法进行全量复制；\n\n2. 主节点收到全量复制的命令后，执行bgsave，在后台生成RDB文件，并使用一个缓冲区（称为复制缓冲区）记录从现在开始执行的所有写命令。\n\n3. 主节点的bgsave执行完成后，将RDB文件发送给从节点；从节点首先清除自己的旧数据，然后载入接收的RDB文件，将数据库状态更新至主节点执行bgsave时的数据库状态。\n\n4. 主节点将前述复制缓冲区中的所有写命令发送给从节点，从节点执行这些写命令，将数据库状态更新至主节点的最新状态。\n\n5. 如果从节点开启了AOF，则会触发bgrewriteaof的执行，从而保证AOF文件更新至主节点的最新状态。\n\n**通过全量复制的过程可以看出，全量复制是非常重型的操作：**\n\n1. 主节点通过**bgsave**命令**fork**子进程进行**RDB**持久化，该过程是非常消耗CPU、内存(页表复制)、硬盘IO的；\n\n2. 主节点通过网络将RDB文件发送给从节点，对主从节点的带宽都会带来很大的消耗。\n\n3. 从节点清空老数据、载入新RDB文件的过程是阻塞的，无法响应客户端的命令；如果从节点执行bgrewriteaof，也会带来额外的消耗。\n\n\n\n#### 部分复制\n\n​\t由于全量复制在主节点数据量较大时效率太低，因此Redis2.8开始提供部分复制，用于处理网络中断时的数据同步。\n\n​\t部分复制的实现，依赖于三个重要的概念：复制偏移量，复制积压缓冲区，服务器运行ID\n\n\n\n\n##### offset 复制偏移量\n\n​\t\t在主从复制的Master(主节点)和Slave(从节点)双方都会各自维持一个offset，代表的是**主节点向从节点传递的字节数**；Master成功发送N个字节的命令后会将Master的offset加上N，Slave在接收到N个字节命令后同样会将Slave的offset增加N。Master和Slave如果状态是一致的那么它的的offset也应该是一致的。\n\n​\t\toffset用于判断主从节点的数据库状态是否一致：如果二者offset相同，则一致；如果offset不同，则不一致，此时可以根据两个offset找出从节点缺少的那部分数据。例如，如果主节点的offset是1000，而从节点的offset是500，那么部分复制就需要将offset为501-1000的数据传递给从节点。而offset为501-1000的数据存储的位置，就是下面要介绍的复制积压缓冲区。\n\n\n\n##### 复制积压缓冲区\n\n  复制积压缓冲区是由**Master(主节点)维护的一个固定长度的FIFO队列(先进先出)**，默认大小1MB；当主节点开始有从节点时创建，它的作用是缓存已经传播出去的命令。当Master进行命令传播时，不仅将命令发送给所有Slave，还会将命令写入到复制积压缓冲区里面。注意，无论主节点有一个还是多个从节点，都只需要一个复制积压缓冲区。\n\n​\t\t除了存储写命令，复制积压缓冲区中还存储了其中的每个字节对应的复制偏移量（offset）。由于复制积压缓冲区定长且是先进先出，所以它保存的是主节点最近执行的写命令；时间较早的写命令会被挤出缓冲区。\n\n​\t\t由于该缓冲区长度固定且有限，因此可以备份的写命令也有限，当主从节点offset的差距过大超过缓冲区长度时，将无法执行部分复制，只能执行全量复制。反过来说，为了提高网络中断时部分复制执行的概率，可以根据需要增大复制积压缓冲区的大小(通过配置repl-backlog-size)；例如如果网络中断的平均时间是60s，而主节点平均每秒产生的写命令(特定协议格式)所占的字节数为100KB，则复制积压缓冲区的平均需求为6MB，保险起见，可以设置为12MB，来保证绝大多数断线情况都可以使用部分复制。\n\n**从节点将offset发送给主节点后，主节点根据offset和缓冲区大小决定能否执行部分复制：**\n\n- **如果offset偏移量之后的数据，仍然都在复制积压缓冲区里，则执行部分复制；**\n- **如果offset偏移量之后的数据已不在复制积压缓冲区中（数据已被挤出），则执行全量复制。**\n\n\n\n##### runid 服务器运行ID\n\n​\t\t每个Redis服务器(无论主从)在启动时都会自动生成一个表明自己身份的随机ID(每次启动都不一样)，由40个随机的十六进制字符组成。在PSYNC中发送的这个ID是指之前连接的Master的ID，如果没保存这个ID，PSYNC命令会使用**”PSYNC ? -1”** 这种形式发送给Master，表示需要全量复制。\n\n​\t\t每个Redis节点，在启动时都会自动生成一个随机ID，由40个随机的十六进制字符组成；\n\nrunid用来唯一识别一个Redis节点。**通过info Server命令，可以查看节点的runid。**\n\n​\t\t主从节点初次复制时，主节点将自己的runid发送给从节点，从节点将这个runid保存起来；当断线重连时，从节点会将这个runid发送给主节点；\n\n**主节点根据runid判断能否进行部分复制：**\n\n- 如果从节点保存的runid与主节点现在的runid相同，说明主从节点之前同步过，主节点会继续尝试使用部分复制(到底能不能部分复制还要看offset和复制积压缓冲区的情况)；\n\n- 如果从节点保存的runid与主节点现在的runid不同，说明从节点在断线前同步的Redis节点并不是当前的主节点，只能进行全量复制。\n\n  \n\n### PSYNC命令\n\n  Redis在2.8版本提供了PSYNC命令来带代替SYNC命令，为Redis主从复制提供了部分复制的能力。\n\n#### PSYNC命令格式\n\n```shell\nPSYNC <runid> <offset>\n# runid:主服务器ID\n# offset:从服务器最后接收命令的偏移量\n```\n\n  **PSYNC执行过程中比较重要的概念有3个：runid、offset（复制偏移量）以及复制积压缓冲区。**\n\n#### psync命令的执行\n\n![990532-20180913134017449-1623896661](../../img/e9335bf6a03bc5b9ca5dce5a345eff0.png)\n\n1. 首先从节点根据当前状态，决定如何调用psync命令：\n\n   - 如果从节点之前未执行过**slaveof**或最近执行了**slaveof no one**，则从节点发送命令为**psync ? -1**，向主节点请求全量复制；\n   - 如果从节点之前执行了**slaveof**，则发送命令为 **psync <runid> <offset> **，其中**runid**为上次复制的主节点的**runid**，**offset**为上次复制截止时从节点保存的复制偏移量。\n\n2. 主节点根据收到的psync命令，及当前服务器状态，决定执行全量复制还是部分复制：\n\n   - 如果主节点版本低于Redis2.8，则返回-ERR回复，此时从节点重新发送sync命令执行全量复制；\n   - 如果主节点版本够新，且runid与从节点发送的runid相同，且从节点发送的offset之后的数据在复制积压缓冲区中都存在，则回复+CONTINUE，表示将进行部分复制，从节点等待主节点发送其缺少的数据即可；\n   - 如果主节点版本够新，但是runid与从节点发送的runid不同，或从节点发送的offset之后的数据已不在复制积压缓冲区中(在队列中被挤出了)，则回复**+FULLRESYNC <runid> <offset>**，表示要进行全量复制，其中runid表示主节点当前的runid，offset表示主节点当前的offset，从节点保存这两个值，以备使用。\n\n   \n\n### 【命令传播阶段】心跳机制\n\n在命令传播阶段，除了发送写命令，主从节点还维持着心跳机制：PING和REPLCONF ACK。心跳机制对于主从复制的超时判断、数据安全等有作用。\n\n\n\n#### 主->从：PING\n\n每隔指定的时间，**主节点会向从节点发送PING命令**，这个PING命令的作用，主要是为了让从节点进行超时判断。\n\nPING发送的频率由 repl-ping-slave-period 参数控制，单位是秒，默认值是10s。\n\n\n\n#### 从->主：REPLCONF ACK\n\n在命令传播阶段，**从节点会向主节点发送REPLCONF ACK命令，**频率是每秒1次；\n\n**命令格式为：**\n\n```shell\nREPLCONF ACK {offset}\t# offset指从节点保存的复制偏移量。\n```\n\n**REPLCONF ACK命令的作用包括：**\n\n1. **实时监测主从节点网络状态：**该命令会被主节点用于复制超时的判断。此外，在主节点中使用info Replication，可以看到其从节点的状态中的lag值，代表的是主节点上次收到该REPLCONF ACK命令的时间间隔，在正常情况下，该值应该是0或1。\n\n2. **检测命令丢失：**从节点发送了自身的offset，主节点会与自己的offset对比，如果从节点数据缺失（如网络丢包），主节点会推送缺失的数据（这里也会利用复制积压缓冲区）。\n- **注意：offset和复制积压缓冲区，不仅可以用于部分复制，也可以用于处理命令丢失等情形；区别在于前者是在断线重连后进行的，而后者是在主从节点没有断线的情况下进行的。**\n\t\n3. **辅助保证从节点的数量和延迟：**Redis主节点中使用min-slaves-to-write和min-slaves-max-lag参数，来保证主节点在不安全的情况下不会执行写命令；所谓不安全，是指从节点数量太少，或延迟过高。例如min-slaves-to-write和min-slaves-max-lag分别是3和10，含义是如果从节点数量小于3个，或所有从节点的延迟值都大于10s，则主节点拒绝执行写命令。而这里从节点延迟值的获取，就是通过主节点接收到REPLCONF ACK命令的时间来判断的，即前面所说的info Replication中的lag值。\n\n\n\n　　\n\n## 开启主从复制\n\n从节点开启主从复制，有3种方式：\n\n- 配置文件：在从服务器的配置文件中加入：**slaveof <masterip> <masterport>**\n- 启动命令：redis-server启动命令后加入： **--slaveof <masterip> <masterport>**\n- 客户端命令：Redis服务器启动后，直接通过客户端执行命令：**slaveof <masterip> <masterport>**，则该Redis实例成为从节点。\n\n\n\n\n### 修改配置文件方法：\n\n#### 1. 配置从服务配置文件redis.conf\n\n```shell\nslaveof 192.168.1.9 6379    #添加属于某台主机的从 服务\nmasterauth 123456       #从服务连接主服的密码（访问主服务器的密码）\nslave-read-only yes     #从服务只读，不可在命令行写入数据\n\n5.0.4以后：\nreplicaof <masterip> <masterport>\nreplica-read-only yes\n```\n\n#### 2. 重新启动从服务即实现主从连接\n\n```shell\n1. ./bin/redis-cli\t# 启动redis客户端\n2. 输入 info replication # 查看与复制相关的状态，了解主从节点的当前状态\n```\n\n**输入info replication 后显示的内容：**\n\n```shell\n# Replication\nrole:slave      # 表示此台服务器是主是从\nmaster_host:39.107.38.62     # 主服务器ip\nmaster_port:6379        # 主服务器端口号\nmaster_link_status:up       # 与主服务器是否连接成功 up为成功 down失败\nmaster_last_io_seconds_ago:9\nmaster_sync_in_progress:0\nslave_repl_offset:808\nslave_priority:100\nslave_read_only:1\nconnected_slaves:0\nmaster_replid:ea5230cc485f9c6f372b2c89a65613fb075aff8b\nmaster_replid2:0000000000000000000000000000000000000000\nmaster_repl_offset:808\nsecond_repl_offset:-1\nrepl_backlog_active:1\nrepl_backlog_size:1048576\nrepl_backlog_first_byte_offset:15\nrepl_backlog_histlen:794\n```\n\n#### 遇到的报错：\n\n##### 1. Error condition on socket for SYNC: Connection refused\n\n  **出现原因**：\n\n  ​\tredis主服务器绑定了127.0.0.1，跨服务器IP的访问就会失败，只能本机才能访问，外部请求会被过滤。\n```shell\n解决方法：\n1. 主服务器绑定ip: bind 39.107.38.62\n3. bind 0.0.0.0\n2. 注释bind  # 会报下面的错↓\n```\n\n\n\n##### 2. '-DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connec\n\n   **出现原因**：\n\n   ​\t处于保护模式，只能本地链接。没有绑定ip 没有设置验证密码。\n\n```shell\n解决方法：\n1. 主服务器绑定ip： bind 39.107.38.62\n2. 设置主服务器访问密码：requirepass 12345\n```\n\n\n\n##### 3. (error) READONLY You can't write against a read only replica.\n\n​\t**出现原因**：\n\n​\t\t从库只可读不可写\n\n```shell\n解决方法：\n1. 设置slave-read-only no # 代表不限于只读\n```\n\n\n\n## 断开主从复制\n\n​\t通过**slaveof <masterip> <masterport>**命令建立主从复制关系以后，可以通过slaveof no one断开。\n\n从节点断开复制后，不会删除已有的数据，只是不再接受主节点新的数据变化。\n","tags":["数据库"]},{"title":"单元测试","url":"/2019/02/03/pout/python中高级面试题/单元测试/","content":"\n\n<!-- toc -->\n\n# 定义\n\n\n\n传统测试无非就是自己运行一下程序查看结果，或者前后端服务进行联调，这里要说的是走正规流程的单元测试，那到底什么是单元测试呢？顾名思义，只测试当前单元的程序或者代码，也可以理解当前模块的代码块，单元测试假设所有的内部或外部的依赖应该是稳定的, 已经在别处进行测试过的.使用mock 就可以对外部依赖组件实现进行模拟并且替换掉, 从而使得单元测试将焦点只放在当前的单元功能。\n\n简单地说，mock就是帮我们解决测试依赖的一个模块，在Python3中，mock已经被集成到了unittest单元测试框架中，所以不需要单独安装，可以直接使用。\n\n\n\n\n\n# 具体操作\n\n\n\n# python3的单元测试模块mock与性能测试模块cProfile\n\n​    我们知道写完了代码需要自己跑一跑进行测试，一个写好的程序如果连测试都没有就上到生产环境是不敢想象的，这么做的人不是太自信就是太无知。\n\n​    传统测试无非就是自己运行一下程序查看结果，或者前后端服务进行联调，这里要说的是走正规流程的单元测试，那到底什么是单元测试呢？顾名思义，只测试当前单元的程序或者代码，也可以理解当前模块的代码块，单元测试假设所有的内部或外部的依赖应该是稳定的, 已经在别处进行测试过的.使用mock 就可以对外部依赖组件实现进行模拟并且替换掉, 从而使得单元测试将焦点只放在当前的单元功能。\n\n​    简单地说，mock就是帮我们解决测试依赖的一个模块，在Python3中，mock已经被集成到了unittest单元测试框架中，所以不需要单独安装，可以直接使用。\n\n​    什么情况下使用mock\n    在项目的单元测试过程中，会遇到：\n    1、接口的依赖\n    2、外部接口调用\n    3、测试环境非常复杂\n\n​    代码示例：\n\n​    \n\n```\ndef add_and_multiply(x, y):\n    addition = x + y\n    multiple = multiply(x, y)\n    return (addition, multiple)\n\ndef multiply(x, y):\n    return x * y\n\nclass MyTestCase(unittest.TestCase):\n\n    def test_add_and_multiply(self):\n        x = 3\n        y = 5\n        addition, multiple = add_and_multiply(x, y)\n        self.assertEqual(8, addition)\n        self.assertEqual(15, multiple)\n\nif __name__ == \"__main__\":\n    unittest.main()\n```\n\n \n\n这个时候就是mock发挥作用的时候了。通过mock模拟掉影响A模块的部分（B模块）。至于mock掉的部分（B模块）应该由其它用例来测试。\n\n​    总有人吐槽 Python 的性能低下，但是 Python 本质其实也不是用来做计算任务的，Python 是一门胶水语言，是用来写业务逻辑的，而不是用来写CPU密集的算法的。事实上复杂的解析一般都会用 C++ 这种硬核语言来写了，比如 numpy TensorFlow lxml。大多数程序员一天 90% 的工作除了和PM撕逼以外，也就是在写 CRUD，也就是调用这些包。所以瓶颈一般在 IO 上而不在 CPU 上，而解决 IO 的瓶颈手段就多了，Python 中至少有 多进程、多线程、AsyncIO、Gevent 等多种方法。不过方法多其实也是一个弊端，这几种方法可以说是基本互不兼容，对各种第三方库的支持也参差不齐。\n\n​    \n\n​    而测试python程序的cpu瓶颈，就需要cProfile模块了，cProfile是一种确定性分析器，只测量CPU时间，并不关心内存消耗和其他与内存相关联的信息。\n\n​    cprofile在python3.7.2里是内置模块，不需要单独安装。\n\n​    cProfile 有多种调用方法，可以直接从命令行调用：\n\n​    \n\n```\npython -m cProfile -s tottime 你的脚本.py\n```\n\n \n\n \n\n \n\n \n\n要获得对程序性能的全面理解，经常需要两个指标都看一下。\n\n​    至此，使用cprofile就可以很简单的看出你写的程序是否性能堪忧了，不过性能这个问题其实是典型的木桶理论的场景，系统的整体性能是由最差的一块决定的。所以也是一个不断迭代的过程。\n","tags":["python中高级面试题"]},{"title":"递归算法","url":"/2019/02/02/pout/算法/递归算法/","content":"\n\n<!-- toc -->\n\n# 递归\n\n在调用一个函数的过程中，直接或间接地调用了函数本身这个就叫递归。但为了避免出现死循环，必须要有一个结束条件\n\n在函数中调用函数本身时，相当于你让程序回到函数的第一行重新走一遍而已。\n\n![img](img/dd1.jpg)\n\n```\ndef foo(S, T):\n    S = T * T - S\n    if S >= 10:\n        W = S + T * T\n        return W\n    else:\n        foo(S, T * 2)\n```\n","tags":["算法"]},{"title":"web跨端融合方案浅析","url":"/2018/12/04/pout/后端技术/web跨端融合方案浅析/","content":"\n本文会对目前流行的基于 JavaScript 的 web 跨端融合方案进行总结和分析，目标人群为 web 方向的从业者但是对跨端融合方案了解不多的人。\n\n### web 跨端融合简介\n\n在 2015 年 React Native 发布之前，web 在移动端 APP 上主要通过 WebView 进行承载，其有许多优点，可以快速迭代发布，不特别受 APP 版本的影响，因此，一些快速发展的业务（包括前期的手机QQ、手机淘宝）大量采用了 WebView 内嵌 H5 页面的形式来推动业务。\n\n但是这种方式缺点也比较明显，主要体现在以下两点：\n\n* 加载时间较长，包括 WebView 初始化的时间、网络请求的时间。\n* HTML 页面在性能上天然不如 Native 页面，无论怎么进行性能优化。\n\n在 2015 年，Facebook 推出了 React Native，从而打开了 web 跨端融合的大门，后续在此架构基础上又出现了阿里巴巴的 Weex（2016）、腾讯的小程序（小程序实际上更偏 web 一点，和其他几类稍有不同，本文不作介绍）、 Hippy（2018）、Taro（Taro 其实更偏向解释翻译，和其他几类定位不同）等跨端融合解决方案，并且渐渐被用到越来越多的项目中，目前，跨端融合开发已经是一种比较主流的 web 开发模式，在阿里系应用、腾讯的微信、QQ浏览器、手机QQ均已经进行了大规模应用。\n\n### 基本架构\n\n虽然 web 跨端融合方案众多，除了上述提到的三种，还有各个公司的更多方案，但是一般来说跨端融合的技术架构都比较相近，我们可以通过下面这一个图来简单概括：\n\n![](/img/1.jpg)\n\n接下来，我们逐个进行简析：\n\n* 业务代码：即我们写的 React Native 代码、Weex 代码，一般来说，我们的业务代码需要经过框架工具或者打包工具（例如 webpack 配合 loader）进行打包，从而兼容一些 ES Next 的写法以及一些框架本身不支持的 Web 写法。\n* Javascript FrameWork：这部分主要是针对 Weex、Hippy 来讲的，Weex 声称支持 Vue、Rax 语法，而 Hippy 声称支持 React、Vue 写法，实际上，对于这些库而言，并不是直接将 React、Vue 引入到项目中，而是会对其源代码进行修改（Vue 有针对 Weex 平台的[版本](https://github.com/vuejs/vue/tree/dev/src/platforms)），而 Hippy 也是对 React 源代码进行了修改，例如，你写的一个` createElement `的操作，在 Web 平台中实际调用的是 `document.createElement(tagName) `这个接口；而在 Weex 平台中实际执行的是` new renderer.Element(tagName)`（renderer 由 Javascript Runtime 提供，并且最终和 Native 通信渲染上屏）。\n* Javascript Runtime：Runtime 的部分，主要是对外暴露了一些统一的接口，比如说节点的增删改查、网络请求的接口等，而这些借口，实际上是其“代理”的客户端的能力，通过客户端 JSAPI 的方式进行调用。另外，把 Runtime 和 FrameWork 进行抽离，也可以便于一个跨端方案适配多个框架，只需要将不同的 FrameWork 和浏览器交互的部分代码转换成 Runtime 提供的标准接口，就可以实现对不同框架的支持。\n* Core：这部分主要是对 Javascript 的解释执行，在 iOS 上一般是 JSCore（系统自带，给客户端提供了执行 JavaScript 程序的能力），而安卓上则可以采用 V8、X5 等。\n* 最下层则是分 Android 和 iOS 端去进行渲染。\n\n### 发展现状\n\n实际上，React Native 最初提出这种解决方案的时候，市面上并没有同类的产品，但是由于 React Native 的一些问题和其他原因，各个大公司基本都在实现自己的跨端融合方案，这里 React Native 的问题主要体现在：\n\n* 最主要的是协议风险。\n* React Native 打包出来的 JSBundle 较大，并且默认没有灵活的分包机制，需要自行解决相关问题。\n* 在部分组件比如 List 组件中，性能较差（据非官方说法，性能并不是 React Native 团队首要考察因素，但是国内团队一般都比较重视性能）。\n* 部分事件发送频繁导致性能损失、例如列表滚动事件、手势事件等。\n* 双端 API 大量没有对齐（这也和其 slogan 是‘learn once, write everywhere’ 而不是 ‘write once, run everywhere’ 相对应）。\n\n而对于国内的 Weex 和 Hippy 框架，其都做了大量的性能优化解决了上述问题，并且规避了协议风险（Weex 采用了 Apache 2.0 协议，而 Hippy 即将开源）。\n\n另外值得一提的是，Weex 和 Hippy 都可以在 web 端进行运行，一般可以作为降级方案使用，从而真正做到了“一份代码”，三端运行。\n\n### 性能优化\n\n实际上，采用目前的跨端融合方案的体验已经比采用 WebView 的方案强太多了，但是性能优化是没有止境的，随着页面复杂度的提高以及用户体验的要求，实际上目前这类跨端融合方案采用了以下几个方向的性能和用户体验优化：\n\n#### 减少网络请求\n\n在我们上述提供的架构图中，一般而言对于一个这类页面，业务代码是通过网络请求加载的，这个时候在加载上主要省去的是 WebView 的初始化时间，这其实是不够的，所以我们也可以采用将业务代码提前下发并存在用户本地，打开的时候只需要从本地拉取并执行代码，这样可以减少相关的网络请求阻塞，优化加载时间。\n\n另外，减少网络请求还体现在对资源的缓存上，对一个页面中所采用的图片等资源文件进行 LRU 策略的缓存，从而防止重复的请求（在传统的 WebView 的方案上，也可以采用对 WebView 增加 Hook 的方式实现）。\n\n当然，以上两点在 WebView 的方案上也可以采用。\n\n#### 降低通信成本\n\n我们从上文的架构图中可以看出，这里的层级实际上比较多，如果不同层级的通信数据较多，并且有比较频繁甚至重复的编解码操作，肯定会有很大的开销，从而影响性能，所以，在不同层级之间做好数据的传递，并且防止重复的编解码操作是比较重要的。\n\n这里可以优化的细节其实比较多，我们举一个 Hippy 的例子：\n\n在 Hippy 架构中，jsRuntime 会生成一个 jsObject 对象树（即需要渲染的 DOM 信息），其在经过 JSBridge 时需要通过`JSON.stringify` 进行序列化，而在 Java（andriod) 接收端，则需要先将其变成一个 JsonObject，最终转化成 HippyMap，这里实际上是有重复的编解码操作的，我们看看 Hippy 的优化策略：\n\n![](/img/3.jpg)\n\n>图片来自 IMWeb 2018\n\n通过 hippybuffer 的方式减少通信的数据量，并且防止重复的编解码操作，可以有效提高性能。\n\n#### 减少通信次数\n\n为了减少在通信方面的消耗，我们除了降低通信的成本，还可以做的就是减少通信次数，当然，前提是不影响用户体验。\n\n这方面可以减少的通信消耗，其中一个方面是频繁的事件通信，我们知道，事件的触发是在 native 端的，但是事件处理的逻辑代码实际上是在 js 层来完成的，在这方面的通信，React Native 就因为频繁的通信从而影响了性能。\n\n我们可以优化的地方在于，首先减少没有绑定回调函数的事件通信，一般而言这部分通信是不必要的，其次是多次通信可以进行合并，比如说 list 滚动回调函数、以及动画通信，我们可以通过配置驱动代替数据驱动的方式（即一次向客户端传递整个配置，后续相同事件可以直接在客户端进行处理），来减少通信次数。\n\n这方面 Hippy 和 Weex 都有大量细碎的实践，在此便不具体介绍了。\n\n#### 降低首屏时间\n\n在原来的 WebView 页面中，我们为了增强用户体验，防止用户进来之后看到白屏，可以采用服务端渲染的方式，将渲染好的页面返回给客户端，同时优化了首屏请求，也防止了客户端设备较差造成JS执行时间较长的情况。\n\n在跨端融合方案中我们仍然有类似的解决方案，在不考虑离线包的情况下（即只考虑业务代码从远程加载的情况），我们也可以由服务端渲染好再返回，Weex 便采用了类似的方案，不过其做的更加彻底，在服务端将代码结果编译成 AST 树并转化成字节码（OPcode），在客户端解析后直接生成虚拟 DOM：\n\n![](/img/2.jpg)\n\n>图片来自 IMWeb 2018\n\n#### 客户端级别的其他优化\n\n客户端的优化有一部分是本来客户端开发就会面临的内容，也有一部分是和混合方案有关的优化，比如 Flex Render 的优化，不过这方面的内容一般而言和前端关系不是非常密切，笔者作为初级前端工程师，对这方面的内容还并不熟悉。\n\n### 框架选型\n\n本文的最后一部分，介绍框架选型。\n\n对于各类跨端融合的方案，其相对于 WebView 都有非常大的性能提升，因此在前期，无论选择什么框架都能够看到成效，这里也并不进行特定的框架选型推荐，但是一般认为，如果是从 Vue 的项目切换，Weex 会更合适一点，而如果从 React 项目切换，在确保没有证书风险的情况下可以采用 React Native，否则可以尝试原生支持 React 的 Hippy。\n\n以上。\n\n\n","tags":["跨端融合"]},{"title":"Node.js 的 TCP 链接管理","url":"/2018/11/25/pout/前端技术/Node-js的TCP链接管理/","content":"\n在 Node.js 的微服务中，一般不同的服务模块我们会采用 TCP 进行通信，本文来简单谈一谈如何设计 TCP 服务的基础管理。\n\n>在具体设计上，本文参考了微服务框架 [Seneca](https://github.com/senecajs/seneca) 所采用的通信方案 [Seneca-transport](https://github.com/senecajs/seneca-transport)，已经被实践所证明其可行性。\n\n一提到 TCP 通信，我们肯定离不开 `net` 模块，事实上，借助 `net` 模块，我们也可以比较快速地完成一般的 TCP 通信的任务。\n\n为了避免对基础的遗忘，我们还是先附上一个基本的 TCP 链接代码：\n\n```javascript\n//server.js:\nconst net = require('net');\n\nconst server = net.createServer((socket) => {\n    socket.write('goodbye\\n');\n    socket.on('data', (data) => {\n        console.log('data:', data.toString());\n        socket.write('goodbye\\n');\n    })\n}).on('error', (err) => {\n    throw err;\n});\n\n// grab an arbitrary unused port.\nserver.listen(8024, () => {\n    console.log('opened server on', server.address());\n});\n\n//client.js:\nconst net = require('net');\n\nconst client = net.createConnection({ port: 8024 }, () => {\n    //'connect' listener\n    console.log('connected to server!');\n    client.write('world!\\r\\n');\n    setInterval(() => {\n        client.write('world!\\r\\n');\n    }, 1000)\n});\nclient.on('data', (data) => {\n    console.log(data.toString());\n    // client.end();\n});\nclient.on('end', () => {\n    console.log('disconnected from server');\n});\n```\n\n其实，上述已经是一个几乎最简单的客户端和服务端通信 Demo，但是并不能在实际项目中使用，首先我们需要审视，其离生产环境还差哪些内容：\n\n1. 以上要求 Server 端要在 Client 端之前启动，并且一旦因为一些错误导致 Server 端重启了并且这个时候 Client 端正好和 Server 端进行通信，那么肯定会 crash，所以，我们需要一个更为平滑兼容的方案。\n2. 以上 TCP 链接的 Server 部分，并没有对 connection 进行管理的能力，并且在在以上的例子中，双方都没有主动释放链接，也就是说，建立的是一个 TCP 长连接。\n3. 以上链接的处理数据能力有限，只能处理纯文本的内容，并且还有一定的风险性（你也许会说可以用 JSON 的序列化反序列化的方法来处理 JSON 数据，但是你别忘了 `socket.on('data'...` 很可能接收到的不是一个完整的 JSON，如果 JSON 较长，其可能只接收到一般的内容，这个时候如果直接 `JSON.parse())` 很可能就会报错）。\n\n以上三个问题，便是我们要解决的主要问题，如果你看过之后立刻知道该如何解决了，那么这篇文章可能你不需要看了，否则，我们可以一起继续探索解决方案。\n\n### 使用 reconnect-core\n\n[reconnect-core](https://www.npmjs.com/package/reconnect-core) 是一个和协议无关的链接重试算法，其工作方式也比较简单，当你需要在 Client 端建立链接的时候，其流程是这样的：\n\n* 调用事先传入的链接建立函数，如果这个时候返回成功了，即成功建立链接。\n* 如果第一次建立链接失败了，那么再隔一段时间建立第二次，如果第二次还是失败，那么再隔一段更长的时间建立第三次，如果还是失败，那么再隔更长的一段时间……直到到达最大的尝试次数。\n\n实际上关于尝试的时间间隔，也会有不同的策略，比较常用的是 Fibonacci 策略和 exponential 策略。\n\n当然，关于策略的具体实现，reconnect-core 采用了一个 [backoff](https://www.npmjs.com/package/backoff) 的库来管理，其可以支持  Fibonacci 策略和 exponential 策略以及更多的自定义策略。\n\n对于上面提到的 DEMO 代码。我们给出 Client 端使用 reconnect-core 的一个实现：\n\n```javascript\n//client.js:\nconst Reconnect = require('reconnect-core');\nconst net = require('net');\nconst Ndjson = require('ndjson');\n\nconst Connect = Reconnect(function() {\n    var args = [].slice.call(arguments);\n    return net.connect.apply(null, args)\n});\n\nlet connection = Connect(function(socket) {\n    socket.write('world!\\r\\n');\n    socket.on('data', (msg) => {\n        console.log('data', msg.toString());\n    });\n    socket.on('close', (msg) => {\n        console.log('close', msg).toString();\n        connection.disconnect();\n    });\n    socket.on('end', () => {\n        console.log('end');\n    });\n});\n\nconnection.connect({\n    port: 8024\n});\nconnection.on('reconnect', function () {\n    console.log('on reconnect...')\n});\nconnection.on('error', function (err) {\n   console.log('error:', err);\n});\nconnection.on('disconnect', function (err) {\n   console.log('disconnect:', err);\n});\n```\n>采用 Reconnect 实际上相比之前是多了一层内容，我们在这里需要区分 connection 实例和 socket 句柄，并且附加正确的时间监听。\n\n现在，我们就不用担心到底是先启动服务端还是先启动客户端了，另外，就算我们的服务端在启动之后由于某些错误关闭了一会，只要没超过最大时间（而这个也是可配置的），仍然不用担心客户端与其建立连接。\n\n\n### 给 Server 端增加管理能力\n\n给 Server 端增加管理能力是一个比较必要的并且可以做成不同程度的，一般来说，最重要的功能则是及时清理链接，常用的做法是收到某条指令之后进行清理，或者到达一定时间之后定时清理。\n\n这里我们可以增加一个功能，达到一定时间之后，自动清理所有链接：\n\n```javascript\n//server.js\nconst net = require('net');\n\nvar connections = [];\n\nconst server = net.createServer((socket) => {\n    connections.push(socket);\n    socket.write('goodbye\\n');\n    socket.on('data', (data) => {\n        console.log('data:', data.toString());\n        socket.write('goodbye\\n');\n    })\n}).on('error', (err) => {\n    throw err;\n});\n\nsetTimeout(() => {\n    console.log('clear connections');\n    connections.forEach((connection) => {\n        connection.end('end')\n        // connection.destory()\n    })\n}, 10000);\n\n// grab an arbitrary unused port.\nserver.listen(8024, () => {\n    console.log('opened server on', server.address());\n});\n```\n\n我们可以通过`connection.end('end')` 和 `connection.destory()` 来清理，一般来说，前者是正常情况下的关闭指令，需要 Client 端进行确认，而后者则是强制关闭，一般在出错的时候会这样调用。\n\n### 使用 ndjson 来格式化数据\n\n[ndjson](https://www.npmjs.com/package/ndjson) 是一个比较方便的 JSON 序列化/反序列化库，相比于我们直接用 JSON，其好处主要体现在：\n\n* 可以同时解析多个 JSON 对象，如果是一个文件流，即其可以包含多个 `{}`，但是要求则是每一个占据一行，其按行分割并且解析。\n* 内部使用了 [split2](https://www.npmjs.com/package/split2)，好处就是其返回时可以保证该行的所有内容已经接受完毕，从而防止 ndjson 在序列化的时候出错。\n\n关于 ndjson 的基本使用，可以根据上述链接查找文档，这里一般情况下，我们的使用方式如下（以下是一个 demo）：\n\n```javascript\n//server.js:\nconst net = require('net');\n\nvar connections = [];\n\nconst server = net.createServer((socket) => {\n    connections.push(socket);\n    socket.on('data', (data) => {\n        console.log('data:', data.toString());\n        socket.write('{\"good\": 1234}\\r\\n');\n        socket.write('{\"good\": 4567}\\n\\n');\n    })\n}).on('error', (err) => {\n    throw err;\n});\n\n// grab an arbitrary unused port.\nserver.listen(8024, () => {\n    console.log('opened server on', server.address());\n});\n\n//client.js:\nconst Reconnect = require('reconnect-core');\nconst net = require('net');\nconst Ndjson = require('ndjson');\nvar Stream = require('stream');\n\nconst Connect = Reconnect(function() {\n    var args = [].slice.call(arguments);\n    return net.connect.apply(null, args)\n});\n\nlet connection = Connect(function(socket) {\n    socket.write('world!\\r\\n');\n    var parser = Ndjson.parse();\n    var stringifier = Ndjson.stringify();\n\n    function yourhandler(){\n        var messager = new Stream.Duplex({ objectMode: true });\n        messager._read = function () {\n            // console.log('data:', data);\n        };\n        messager._write = function (data, enc, callback) {\n            console.log(typeof data, data);\n            // your handler\n            return callback()\n        };\n        return messager\n    }\n    socket // 链接句柄\n        .pipe(parser)\n        .pipe(yourhandler())\n        .pipe(stringifier)\n        .pipe(socket);\n\n    socket.on('close', (msg) => {\n        console.log('close', msg).toString();\n        connection.disconnect();\n    });\n    socket.on('end', (msg) => {\n        console.log('end', msg);\n    });\n});\nconnection.connect({\n    port: 8024\n});\nconnection.on('reconnect', function () {\n    console.log('on reconnect...')\n});\nconnection.on('error', function (err) {\n   console.log('error:', err);\n});\nconnection.on('disconnect', function (err) {\n   console.log('disconnect:', err);\n});\n```\n\n其中，用户具体的逻辑代码，可以是 `yourhandler` 函数 `_write` 里面的一部分，其接收的是一个一个处理好的对象。\n\n","tags":["TCP"]},{"title":"多组件单页列表应用的代码组织实践","url":"/2018/11/10/pout/多组件单页列表应用的代码组织实践/","content":"\n本文主要对多组件单页面列表应用的代码组织实践进行总结，从而给相关应用的 Web 开发提供参考。\n\n### 什么是多组件单页面列表应用？\n\n目前，其实多组件单页面列表应用非常常见，也是我们日常生活中使用非常高频的一个类别的应用，最典型的比如新闻信息流产品腾讯新闻、今日头条等这类新闻应用，在这类新闻应用中，往往图片、图文、视频、问答、投票等多种模块混杂排列。再简单一点的话，知乎、豆瓣甚至一些论坛以及一些购物软件，也可以归为此类应用。\n\n由于笔者在负责QQ看点搜索模块的相关内容， 因此，这里给出一个QQ看点搜索的展示图：\n\n![](/img/kd.jpg)\n\n这类应用其实有如下特点：\n\n* 属于长列表滚动，内容随着滚动不断加载，一般在用户返回之前可能积累了大量的内容，因此可能会造成一定的性能问题。\n* 模块众多，并且模块的种类和样式更新迭代快，这给我们在复用组件的选择上带来了挑战，如果我们盲目复用组件，则会造成胶水代码越来越多，如果不复用组件，那么代码量会随着业务发展线性增长，这都给我们后续的维护带来了挑战。\n\n当然，一般的基于 Web 的应用（实际上，QQ看点搜索并不完全是纯粹的 WebView 应用）所面临的问题这里也都会遇到。不过，上述两类问题应该算是这类应用的比较重要的问题，其实归根到底，前者是性能问题（面向用户），后者是维护问题（面向开发者）。\n\n如何解决这里的性能问题，其实已经有很多常规的方案可以借鉴了，这并不是本篇文章的重点，除了传统 Web 用到的性能优化方法，这里仅仅列举一些常规的做法：\n\n* 图片等资源的懒加载。\n* 列表虚拟滚动，即使用有限的元素，优化CSS写法等。\n* 使用跨端融合方案渲染，例如 Weex、ReactNative、Hippy 等。\n\n### 多组件单页面应用的维护困境\n\n对于这类多组件单页面应用，一般都是增量发展的，即最开始只有很个别的几个模块，随着业务越来越复杂，模块越来越多，逻辑也越来越复杂。\n\n我们一开始，肯定可以想到一个模块（即上文中灰色分割线分割的一块）是一个组件，不同组件之间抽离出公共的函数，或者采用 mixin 将公共的部分抽离，至于数据端，由于这类应用通常在深度上不复杂，直接采用 React 或者 Vue 提供的父子组件通信的方式一般就够用了。这样设计既满足组件化的思路，也能够方便的维护项目，比较适合项目的初期。\n\n但是随着项目发展，我们会发现，问题慢慢地产生了：\n\n* 单元组件非常不好界定，比如一个左图右文的图文混排组件（例如刘亦菲的热点），之后又会增加左视频右文字，和图文展示的区别不大但是加了视频的播放时长，之后又加了左视频集合右文字（例如双世宠妃第一部分），如果我们把这多类内容当作一个组件，我们的组件中就会有非常多的判断代码，那么就会有大量的代码冗余，或者设计复杂的 mixin 和工具函数。\n* 除了我们自身的问题，往往随着内容增多，后端返回的数据内容也会非常的不一致，在相似甚至相同的组件中，数据格式也不尽相同，我们需要在我们的单元组件中，来解析判断多种数据格式。\n* 第三点就是样式更新隐患，当我们的组件多了之后，如果我们对我们的组件进行更新，那么很可能需要同时更新多处（嗯，全局替换也许是个不错的主意），这也是相当有风险的，也许会无意间改动我们并不想改动的 UI。\n\n如果我们等项目复杂后面对这个问题，我们会发现改动前期的代码工作量比较巨大，但是这又是我们不得不做的事情，这类问题的产生，实际上主要原因是我们的组件设计规划的不合理，我们完全可以在最初的项目中，通过一定的设计规划，来规避这些问题。\n\n### 多组件单页面应用的组件规划\n\n既然，我们现在希望设计一套比较好的组件规划，我们就需要重新审视我们的项目，对于我们的项目而言，一个业务模块一个组件的方式，的确简单方便，但是这样粗放的组件划分原则，实际上并不能完全满足我们复杂的维护需求，反而会给我们带来困扰。\n\n经过一系列的重构和整理，目前QQ看点搜索的组件规划逻辑是这样的：\n\n![](/img/kds.jpg)\n\n这里为了方便理解，我们采取上面样例图片中比较常见的一类业务：图文混排条目（左图右文和右图左文）来进行举例，如何设计组件来让提高我们项目的可维护性。\n\n这里首先是零件层，零件层应该有如下内容：\n\n* 图片零件，定宽，定高，自带懒加载，正常情况下只需传入一个 URL 即可使用。\n* 标题组件，一行标题和两行标题可以设计成两个组件，但进行 CSS 层面的复用。\n* 描述内容组件，例如双世宠妃的两行剧集描述。\n* 元信息内容组件，例如普通图文的来源和发表时间。\n* 时长组件，视频图文中用到。\n* 带有描述性的图片组件，视频图文中用到。\n* 图标组件：可以承载图标。\n\n以上各个组件的内容，几乎都足够简单，只需传入一个 props 作为内容，一般情况下，组件中不能出现 if 或 switch 等逻辑。\n\n接下来是组合器部分，组合器也是零件，只不过是零件的组合，其实也可以设计的比较薄弱，从而将更多的功能在布局器中完成，但是个别的时候，有这一层会给我们带来一定的方便，这里比如：\n\n* 图标+文字的组合器标题。\n\n对于零件层和组合层，一般情况下都不需要有影响外部的 margin 和 padding，即如果不增加任何多余样式罗列零件层和组合层，其上下左右四边应该是互相贴合的。\n\n接下来是布局层，这里的布局层，其实可以进行多种方式的设计，根据设计不同其数目也不同，这里给出一种设计方式：\n\n* 第一种是左图右文形式，右边可以选择普通图片、普通图片+时长组件、普通图片+描述。右边可以在一行标题、两行标题、描述零件、元信息零件中任意选择和组合。\n* 第二种是右图左文形式，左边的可配置内容和上文右边相同。\n\n当然，这两种整合成一种也无妨。\n\n在布局层，是拥有事件能力的，但是其主要应该是绑定响应时间并且调用通过 props 传入的回调函数，其不应该自己执行事件的响应逻辑。\n\n最后是控制器层，**在控制器层，除了包裹标签之外，不应该出现任何 html 标签，其也不应当引用除了布局层组件以外的更深层次的组件。控制层的主要作用是进行数据处理。**\n\n控制层的分类方式和上述几层稍有不同，这里，我们就不是按照 UI 来分不同的控制器了，而是按照数据或者业务来分类，因为这里我们主要是进行数据逻辑的处理，和 UI 的关系不是那么重要了（已经将 UI 的压力进行了下沉）。\n\n通过上述的做法，之后如果有新的需求增加进来，我们根据需要，在不同层级的组件增加内容就好了。\n\n### 总结\n\n通过以上的逻辑，我们把组件划分的更加清晰明确，将 UI 展示和数据逻辑分离，并且方便我们对样式进行迭代升级。\n\n当然，这个时候你也许还会问，如果我对部分组件样式进行升级改造，怎么样防止对原有的样式无影响呢？暂时还没有好的办法，不过，我们正在做的 UI 自动化测试套件——mangosteen，可以完美解决这个问题，敬请期待。","tags":["组件化"]},{"title":"生成器和迭代器","url":"/2018/11/09/pout/python中高级面试题/生成器和迭代器/","content":"\n\n<!-- toc -->\n\n## 1.迭代的概念\n\n上一次输出的结果为下一次输入的初始值，重复的过程称为迭代,每次重复即一次迭代，并且每次迭代的结果是下一次迭代的初始值\n\n## 2.可迭代的对象\n\n内置**iter**方法的，都是可迭代的对象。 list是可迭代对象，dict是可迭代对象，set也是可迭代对象。\n\n## 3.迭代器\n\n### 1.为什么要有迭代器？\n\n对于没有索引的数据类型，必须提供一种不依赖索引的迭代方式。\n\n### 2.迭代器定义：\n\n迭代器：可迭代对象执行**iter**方法，得到的结果就是迭代器，迭代器对象有**next**方法\n\n它是一个带状态的对象，他能在你调用next()方法的时候返回容器中的下一个值，任何实现了**iter**和**next**()方法的对象都是迭代器，**iter**返回迭代器自身，**next**返回容器中的下一个值，如果容器中没有更多元素了，则抛出StopIteration异常\n\n## 二、生成器\n\n### 1.定义\n\n生成器(generator)是一个特殊的迭代器，它的实现更简单优雅，yield是生成器实现**next**()方法的关键。它作为生成器执行的暂停恢复点，可以对yield表达式进行赋值，也可以将yield表达式的值返回。\n\n也就是说，yield是一个语法糖，内部实现支持了迭代器协议，同时yield内部是一个状态机，维护着挂起和继续的状态。\n\n### yield的功能：\n\n1.相当于为函数封装好**iter**和**next** 2.return只能返回一次值，函数就终止了，而yield能返回多次值，每次返回都会将函数暂停，下一次next会从上一次暂停的位置继续执行\n\n## 为什么说生成器是一种迭代器？\n\nPython 判断一个对象是否是迭代器的标准是看这个对象是否遵守迭代器协议 ，判断一个对象是否遵守迭代器协议主要看两个方面：\n\n1对象首先得实现 **iter** 和 **next** 方法\n\n2其次**iter** 方法必须返回它自己\n\n而生成器恰好满足了这两个条件（可以自己写个生成器，然后调用生成器的这两个方法试试）。我们平常还会经常碰到另外一个概念：可迭代对象。可迭代对象就是可迭代的对象，可迭代的对象就是说我们可以从这个对象拿到一个迭代器。在 Python 中，iter 方法可以帮我们完成这个事情，也就是说，可迭代对象和迭代器满足这样一个关系：iter(iterable) -> iterator。\n\n在 Python 中，list 是个可迭代对象，所以我们经常会写这样的代码：\n\n```\n>>> l = [1, 2, 3]\n>>> for element in l:\n...     print(element)\n```\n\n但你想过为什么我们可以这么写吗？为啥在 c 语言里面，我们访问数组元素的时候，必须要通过 index?\n\n因为：list 是个可迭代对象，我们在 Python 中使用 for ... in 时，Python 会给我们生成一个迭代器对象，而如上所说：迭代器是个数据流，它可以产生数据，我们一直从里面取数据就好了，而不需要我们在代码中维护 index，Python 已经通过迭代器给我们完成了这个事情。\n","tags":["python中高级面试题"]},{"title":"使用 Node.js 打造多用户实时监控系统","url":"/2018/10/21/pout/前端技术/使用 Node.js 打造多用户实时监控系统/","content":"\n### 背景概述\n\n首先描述一下笔者遇到的问题，我们可以设定这样一个场景：现在有一个实时监控系统的开发需求，要求同时支持多个用户（这里我们为了简化，暂时不涉及登陆态，假定一个设备即为一个用户），对于不同的用户来讲，他们需要监控的一部分内容是完全相同的，比如设备的 CPU 信息、内存信息等，而另外一部分内容是部分用户重叠的，比如对某一区域的用户来说某些监控信息是相同的，而还有一些信息，则是用户之间完全不同的。\n\n对于每个用户来讲，当其进入页面之后即表明其开始监控，需要持续地进行数据更新，而当其退出界面或者手动点击停止监控，则停止监控。\n\n### 问题描述\n\n实际上，对于以上情况，我们很容易想到通过 WebSocket，对不同的用户进行隔离处理，当一个用户开始监控的时候，通过函数来逐个启动其所有的监控项目，当其停止监控的时候，取消相关监控，并且清除无关变量等。我们可以将所有内容写到 WebSocket 的连接回调中，由于作用域隔离，不同用户之间的监控（读操作）不会产生互相影响。\n\n这种方式可以说是最为快捷方便的方式了，并且几乎无需进行设计，但是这样有一个非常明显的效率问题：\n\n由于不同用户的部分监控项目是有重叠的，对于这些重叠的项目，我们如果对于每一个用户都单独监控，那么就会产生非常多的浪费，如果这些监控中还涉及到数据库交互或者较为复杂的计算，那么成倍之后的性能损失是非常难以承受的。\n\n所以，我们需要将不同用户重叠的那些监控项目，进行合并，合并成一个之后，如果有新的消息，我们就推到所有相关用户的回调函数中去处理。\n\n也就是说，我们需要管理一个一对多的订阅发布模式。\n\n到这里，我们发现我们想要实现这样一个监控系统，并不是非常简单，主要有下列问题：\n\n* [1]对于可能有用户重叠的监控项目，我们需要抽离到用户作用域之外，并且通过统计计数等方式来\"记住\"当前所有的监控用户，当有新内容时推到各个用户的处理函数中，并且当最后一个用户取消监控的时候要及时清理相关对象。\n* [2]不同用户的重叠监控项目的监控方式也各不相同，有的是通过 `setInterval` 等方式的定时任务，有的是事件监听器等等。\n* [3]判断不同用户的项目是否重叠也有一定的争议，比如假设不同用户端监控的是同一个项目，调用的也是相同的函数，但是由于用户 ID 不同，这个时候我们如何判断是否算\"同一个监控\"？\n\n以上的这些问题，如果我们不借助现有的库和工具，自己顺着思路一点点去写，则很容易陷入修修补补的循环，无法专注监控本身，并且最后甚至在效率上适得其反。\n\n### 解决方案\n\n以下解决方案基于 Rx.js，需要对 [Observable](https://cn.rx.js.org/class/es6/Observable.js~Observable.html) 有一定了解。\n\n#### 多个用户的监控以及取消\n\n[Monitor-RX](https://github.com/aircloud/monitor-rx) 是对以上场景问题的一个解决方案封装，其利用了 Rx.js 对订阅发布的管理能力，可以让整个流程变的清晰。\n\n在 Rx.js 中，我们可以通过以下方式建立一个多播对象 `multicasted`：\n\n```\nvar source = Rx.from([1, 2, 3]);\nvar subject = new Rx.Subject();\nvar multicasted = source.pipe(multicast(subject)).refCount();\n// 其属于 monitor-rx 的实现细节，无需理解亦可使用 monitor-rx\n\nsubscription1 = refCounted.subscribe({\n    next: (v) => console.log('observerA: ' + JSON.stringify(v))\n});\n\nsetTimeout(() => {\n    subscription2 = refCounted.subscribe({\n        next: (v) => console.log('observerB: ' + JSON.stringify(v))\n    });\n}, 1200);\n\nsubscription1.unsubscribe();\nsetTimeout(() => {\n    subscription2.unsubscribe();\n    // 这里 refCounted 的 unsubscribe 相关清理逻辑会自动被调用\n}, 3200);\n```\n\n在这里采用多播，有如下几个好处：\n\n* 可以随时增加新的订阅者，并且新的订阅者只会收到其加入订阅之后的数据。\n* 可以随时对任意一个订阅者取消订阅。\n* 当所有订阅者取消订阅之后，Observable 会自动触发 Observable 函数，从而可以对其事件循环等进行清理。\n\n以上能力其实可以帮助我们解决上文提到的问题 [1]。\n\n#### 监控格式的统一\n\n实际上，在我们的监控系统中，从数据依赖的角度，我们的监控函数会有这样几类：\n\n* [a]纯粹的定时任务，无数据依赖，这方面比如当前内存快照数据等。\n* [b]带有记忆依赖的定时任务：定时任务依赖前一次的数据（甚至更多次），需要两次数据做差等，这方面的数据比如一段时间的消耗数据，cpu 使用率的计算。\n* [c]带有用户依赖的定时任务：依赖用户 id 等信息，不同用户无法共用。\n\n而从任务触发的角度，我们仍待可以对其分类：\n\n* [i]简单的 `setInterval` 定时任务。\n* [ii]基于事件机制的不定时任务。\n* [iii]基于其他触发机制的任务。\n\n实际上，我们如果采用 Rx.js 的模式进行编写，无需考虑任务的数据依赖和触发的方式，只需写成一个一个 Observable 实例即可。另外，对于比较简单的 [a]&[i] 或 [c]&[i]  类型，我们还可以通过 monitor-rx 提供的 `convertToRx` 或 `convertToSimpleRx` 转换成 Observable 实例生成函数，例如：\n\n```\nvar os = require('os');\nvar process = require('process');\nconst monitorRx = require('monitor-rx');\n\nfunction getMemoryInfo() {\n    return process.memoryUsage();\n}\n\nconst memory = monitorRx.Utils.convertToSimpleRx(getMemoryInfo)\n\n// 或者\n//const memory = monitorRx.Utils.convertToRx({\n//    getMemoryInfo\n//});\n\nmodule.exports = memory;\n```\n\nconvertToRx 相比于 convertToSimpleRx，可以支持函数配置注入（即下文中 opts 的 func 属性和 args 属性）,可以在具体生成 Observable 实例的时候具体指定使用哪些函数以及其参数。\n\n如果是比较复杂的 Observable 类型，那么我们就无法直接通过普通函数进行转化了，这个时候我们遵循 Observable 的标准返回 Observable 生成函数即可（不是直接返回 Observable 实例） \n\n这实际上也对问题 [2] 进行了解决。\n\n#### 监控唯一性：\n\n我们知道，如果两个用户都监控同一个信息，我们可以共用一个 Observable，这里的问题，就是如何定义两个用户的监控是\"相同\"的。\n\n这里我们采用一个可选项 opts 的概念，其一共有如下属性：\n\n```\n{\n    module: 'ModuleName',\n    func: ['FuncName'],\n    args: [['arg1','arg2']],\n    opts: {interval:1000}, \n}\n```\n\nmodule 即用户是对哪一个模块进行监控（实际上是 Observable），func 和 args 则是监控过程中需要调用的函数，我们也可以通过 agrs 传入用户个人信息。于没有内部子函数调用的监控，二者为空即可，opts 是一些其他可选项，比如定义请求间隔等。\n\n之后，我们通过 `JSON.stringify(opts)` 来序列化这个可选项配置，如果两个用户序列化后的可选项配置相同，那么我们就认为这两个用户可以共用一个监控，即共用一个 Observable。\n\n### 更多内容\n\n实际上，借助 Monitor-RX，我们可以很方便的解决上述提出的问题，Monitor-RX 也在积极的更新中，大家可以在[这里](https://github.com/aircloud/monitor-rx)了解到更多的信息。","tags":["Rx.js"]},{"title":"mysql常规操作详细","url":"/2018/09/23/pout/数据库/mysql/mysql常规操作详细/","content":"\n\n<!-- toc -->\n\nMYSQL常用命令  \n\n1. 1.导出整个数据库  \n2. mysqldump -u 用户名 -p –default-character-set=latin1 数据库名 > 导出的文件名(数据库默认编码是latin1)  \n3. mysqldump -u wcnc -p smgp_apps_wcnc > wcnc.sql  \n4. 2.导出一个表  \n5. mysqldump -u 用户名 -p 数据库名 表名> 导出的文件名  \n6. mysqldump -u wcnc -p smgp_apps_wcnc users> wcnc_users.sql  \n7. 3.导出一个数据库结构  \n8. mysqldump -u wcnc -p -d –add-drop-table smgp_apps_wcnc >d:wcnc_db.sql  \n9. -d 没有数据 –add-drop-table 在每个create语句之前增加一个drop table  \n10. 4.导入数据库  \n11. A:常用source 命令  \n12. 进入mysql数据库控制台，  \n13. 如mysql -u root -p  \n14. mysql>use 数据库  \n15. 然后使用source命令，后面参数为脚本文件(如这里用到的.sql)  \n16. mysql>source wcnc_db.sql  \n17. B:使用mysqldump命令  \n18. mysqldump -u username -p dbname < filename.sql  \n19. C:使用mysql命令  \n20. mysql -u username -p -D dbname < filename.sql  \n21. 一、启动与退出  \n22. 1、进入MySQL：启动MySQL Command Line Client（MySQL的DOS界面），直接输入安装时的密码即可。此时的提示符是：mysql>  \n23. 2、退出MySQL：quit或exit  \n24. 二、库操作  \n25. 1、、创建数据库  \n26. 命令：create database <数据库名>  \n27. 例如：建立一个名为xhkdb的数据库  \n28. mysql> create database xhkdb;  \n29. 2、显示所有的数据库  \n30. 命令：show databases （注意：最后有个s）  \n31. mysql> show databases;  \n32. 3、删除数据库  \n33. 命令：drop database <数据库名>  \n34. 例如：删除名为 xhkdb的数据库  \n35. mysql> drop database xhkdb;  \n36. 4、连接数据库  \n37. 命令：use <数据库名>  \n38. 例如：如果xhkdb数据库存在，尝试存取它：  \n39. mysql> use xhkdb;  \n40. 屏幕提示：Database changed  \n41. 5、查看当前使用的数据库  \n42. mysql> select database();  \n43. 6、当前数据库包含的表信息：  \n44. mysql> show tables; （注意：最后有个s）  \n45. 三、表操作，操作之前应连接某个数据库  \n46. 1、建表  \n47. 命令：create table <表名> ( <字段名> <类型> [,..<字段名n> <类型n>]);  \n48. mysql> create table MyClass(  \n49. \\> id int(4) not null primary key auto_increment,  \n50. \\> name char(20) not null,  \n51. \\> sex int(4) not null default ’′,  \n52. \\> degree double(16,2));  \n53. 2、获取表结构  \n54. 命令：desc 表名，或者show columns from 表名  \n55. mysql>DESCRIBE MyClass  \n56. mysql> desc MyClass;  \n57. mysql> show columns from MyClass;  \n58. 3、删除表  \n59. 命令：drop table <表名>  \n60. 例如：删除表名为 MyClass 的表  \n61. mysql> drop table MyClass;  \n62. 4、插入数据  \n63. 命令：insert into <表名> [( <字段名>[,..<字段名n > ])] values ( 值 )[, ( 值n )]  \n64. 例如，往表 MyClass中插入二条记录, 这二条记录表示：编号为的名为Tom的成绩为.45, 编号为 的名为Joan 的成绩为.99，编号为 的名为Wang 的成绩为.5.  \n65. mysql> insert into MyClass values(1,’Tom’,96.45),(2,’Joan’,82.99), (2,’Wang’, 96.59);  \n66. 5、查询表中的数据  \n67. 1)、查询所有行  \n68. 命令：select <字段，字段，…> from < 表名 > where < 表达式 >  \n69. 例如：查看表 MyClass 中所有数据  \n70. mysql> select  from MyClass;  \n71. 2）、查询前几行数据  \n72. 例如：查看表 MyClass 中前行数据  \n73. mysql> select  from MyClass order by id limit 0,2;  \n74. 或者：  \n75. mysql> select  from MyClass limit 0,2;  \n76. 6、删除表中数据  \n77. 命令：delete from 表名 where 表达式  \n78. 例如：删除表 MyClass中编号为 的记录  \n79. mysql> delete from MyClass where id=1;  \n80. 7、修改表中数据：update 表名 set 字段=新值,…where 条件  \n81. mysql> update MyClass set name=’Mary’where id=1;  \n82. 7、在表中增加字段：  \n83. 命令：alter table 表名 add字段 类型 其他;  \n84. 例如：在表MyClass中添加了一个字段passtest，类型为int(4)，默认值为  \n85. mysql> alter table MyClass add passtest int(4) default ’′  \n86. 8、更改表名：  \n87. 命令：rename table 原表名 to 新表名;  \n88. 例如：在表MyClass名字更改为YouClass  \n89. mysql> rename table MyClass to YouClass;  \n90. 更新字段内容  \n91. update 表名 set 字段名 = 新内容  \n92. update 表名 set 字段名 = replace(字段名,’旧内容’,’新内容’)  \n93.   \n94.   \n95.   \n96.   \n97.   \n98.   \n99.   \n100. 文章前面加入个空格  \n101. update article set content=concat(‘　　’,content);  \n102. 字段类型  \n103. 1．INT[(M)] 型：正常大小整数类型  \n104. 2．DOUBLE[(M,D)] [ZEROFILL] 型：正常大小(双精密)浮点数字类型  \n105. 3．DATE 日期类型：支持的范围是-01-01到-12-31。MySQL以YYYY-MM-DD格式来显示DATE值，但是允许你使用字符串或数字把值赋给DATE列  \n106. 4．CHAR(M) 型：定长字符串类型，当存储时，总是是用空格填满右边到指定的长度  \n107. 5．BLOB TEXT类型，最大长度为(2^16-1)个字符。  \n108. 6．VARCHAR型：变长字符串类型  \n109. 5.导入数据库表  \n110. 　　（）创建.sql文件  \n111. 　　（）先产生一个库如auction.c:mysqlbin>mysqladmin -u root -p creat auction，会提示输入密码，然后成功创建。  \n112. 　　（）导入auction.sql文件  \n113. 　　c:mysqlbin>mysql -u root -p auction < auction.sql。  \n114. 　　通过以上操作，就可以创建了一个数据库auction以及其中的一个表auction。  \n115. 　　6．修改数据库  \n116. 　　（）在mysql的表中增加字段：  \n117. 　　alter table dbname add column userid int(11) not null primary key auto_increment;  \n118. 　　这样，就在表dbname中添加了一个字段userid，类型为int(11)。  \n119. 　　7．mysql数据库的授权  \n120. 　　mysql>grant select,insert,delete,create,drop  \n121. 　　on . *(或test.*/user.*/..)*  \n122. 　　to 用户名@localhost  \n123. 　　identified by ‘密码’；  \n124. 　　如：新建一个用户帐号以便可以访问数据库，需要进行如下操作：  \n125. 　　mysql> grant usage  \n126. 　　-> ON test.  \n127. 　　-> TO testuser@localhost;  \n128. 　　Query OK, 0 rows affected (0.15 sec)  \n129. 　　此后就创建了一个新用户叫：testuser，这个用户只能从localhost连接到数据库并可以连接到test 数据库。下一步，我们必须指定testuser这个用户可以执行哪些操作：  \n130. 　　mysql> GRANT select, insert, delete,update  \n131. 　　-> ON test.  \n132. 　　-> TO testuser@localhost;  \n133. 　　Query OK, 0 rows affected (0.00 sec)  \n134. 　　此操作使testuser能够在每一个test数据库中的表执行SELECT，INSERT和DELETE以及UPDATE查询操作。现在我们结束操作并退出MySQL客户程序：  \n135. 　　mysql> exit  \n136. 　　Bye9!  \n137. 1:使用SHOW语句找出在服务器上当前存在什么数据库：  \n138. mysql> SHOW DATABASES;  \n139. 2:2、创建一个数据库MYSQLDATA  \n140. mysql> Create DATABASE MYSQLDATA;  \n141. 3:选择你所创建的数据库  \n142. mysql> USE MYSQLDATA; (按回车键出现Database changed 时说明操作成功！)  \n143. 4:查看现在的数据库中存在什么表  \n144. mysql> SHOW TABLES;  \n145. 5:创建一个数据库表  \n146. mysql> Create TABLE MYTABLE (name VARCHAR(20), sex CHAR(1));  \n147. 6:显示表的结构：  \n148. mysql> DESCRIBE MYTABLE;  \n149. 7:往表中加入记录  \n150. mysql> insert into MYTABLE values (“hyq”,”M”);  \n151. 8:用文本方式将数据装入数据库表中（例如D:/mysql.txt）  \n152. mysql> LOAD DATA LOCAL INFILE “D:/mysql.txt”INTO TABLE MYTABLE;  \n153. 9:导入.sql文件命令（例如D:/mysql.sql）  \n154. mysql>use database;  \n155. mysql>source d:/mysql.sql;  \n156. 10:删除表  \n157. mysql>drop TABLE MYTABLE;  \n158. 11:清空表  \n159. mysql>delete from MYTABLE;  \n160. 12:更新表中数据  \n161. mysql>update MYTABLE set sex=”f”where name=’hyq’;  \n162. 以下是无意中在网络看到的使用MySql的管理心得,  \n163.   \n164.   \n165. 在windows中MySql以服务形式存在，在使用前应确保此服务已经启动，未启动可用net start mysql命令启动。而Linux中启动时可用“/etc/rc.d/init.d/mysqld start”命令，注意启动者应具有管理员权限。  \n166. 刚安装好的MySql包含一个含空密码的root帐户和一个匿名帐户，这是很大的安全隐患，对于一些重要的应用我们应将安全性尽可能提高，在这里应把匿名帐户删除、root帐户设置密码，可用如下命令进行：  \n167. use mysql;  \n168. delete from User where User=””;  \n169. update User set Password=PASSWORD(‘newpassword’) where User=’root’;  \n170. 如果要对用户所用的登录终端进行限制，可以更新User表中相应用户的Host字段，在进行了以上更改后应重新启动数据库服务，此时登录时可用如下类似命令：  \n171. mysql -uroot -p;  \n172. mysql -uroot -pnewpassword;  \n173. mysql mydb -uroot -p;  \n174. mysql mydb -uroot -pnewpassword;  \n175. 上面命令参数是常用参数的一部分，详细情况可参考文档。此处的mydb是要登录的数据库的名称。  \n176. 在进行开发和实际应用中，用户不应该只用root用户进行连接数据库，虽然使用root用户进行测试时很方便，但会给系统带来重大安全隐患，也不利于管理技术的提高。我们给一个应用中使用的用户赋予最恰当的数据库权限。如一个只进行数据插入的用户不应赋予其删除数据的权限。MySql的用户管理是通过User表来实现的，添加新用户常用的方法有两个，一是在User表插入相应的数据行，同时设置相应的权限；二是通过GRANT命令创建具有某种权限的用户。其中GRANT的常用用法如下：  \n177. grant all on mydb. to NewUserName@HostName identified by “password”;  \n178. grant usage on *.* to NewUserName@HostName identified by “password”;  \n179. grant select,insert,update on mydb. to NewUserName@HostName identified by “password”;  \n180. grant update,delete on mydb.TestTable to NewUserName@HostName identified by “password”;  \n181. 若要给此用户赋予他在相应对象上的权限的管理能力，可在GRANT后面添加WITH GRANT OPTION选项。而对于用插入User表添加的用户，Password字段应用PASSWORD 函数进行更新加密，以防不轨之人窃看密码。对于那些已经不用的用户应给予清除，权限过界的用户应及时回收权限，回收权限可以通过更新User表相应字段，也可以使用REVOKE操作。    \n182. 全局管理权限：  \n183. FILE: 在MySQL服务器上读写文件。  \n184. PROCESS: 显示或杀死属于其它用户的服务线程。  \n185. RELOAD: 重载访问控制表，刷新日志等。  \n186. SHUTDOWN: 关闭MySQL服务。  \n187. 数据库/数据表/数据列权限：  \n188. Alter: 修改已存在的数据表(例如增加/删除列)和索引。  \n189. Create: 建立新的数据库或数据表。  \n190. Delete: 删除表的记录。  \n191. Drop: 删除数据表或数据库。  \n192. INDEX: 建立或删除索引。  \n193. Insert: 增加表的记录。  \n194. Select: 显示/搜索表的记录。  \n195. Update: 修改表中已存在的记录。  \n196. 特别的权限：  \n197. ALL: 允许做任何事(和root一样)。  \n198. USAGE: 只允许登录–其它什么也不允许做。  \n199. ———————  \n200. MYSQL常用命令  \n201. 有很多朋友虽然安装好了mysql但却不知如何使用它。在这篇文章中我们就从连接MYSQL、修改密码、增加用户等方面来学习一些MYSQL的常用命令。  \n202. 　　有很多朋友虽然安装好了mysql但却不知如何使用它。在这篇文章中我们就从连接MYSQL、修改密码、增加用户等方面来学习一些MYSQL的常用命令。　  \n203. 　　一、连接MYSQL　  \n204. 　　格式：mysql -h主机地址-u用户名－p用户密码　　  \n205. 　　、例：连接到本机上的MYSQL  \n206. 　　首先在打开DOS窗口，然后进入目录mysqlbin，再键入命令mysql -uroot -p，回车后提示你输密码，如果刚安装好MYSQL，超级用户root是没有密码的，故直接回车即可进入到MYSQL中了，MYSQL的提示符是：mysql> 　　  \n207. 　　、例：连接到远程主机上的MYSQL  \n208. 　　假设远程主机的IP为：.110.110.110，用户名为root,密码为abcd123。则键入以下命令：　　　  \n209. 　　mysql -h110.110.110.110 -uroot -pabcd123 　　  \n210. 　　（注:u与root可以不用加空格，其它也一样）　　  \n211. 　　、退出MYSQL命令：exit （回车）  \n212. 　　二、修改密码　　  \n213. 　　格式：mysqladmin -u用户名-p旧密码password 新密码　  \n214. 　　、例：给root加个密码ab12。首先在DOS下进入目录mysqlbin，然后键入以下命令　　  \n215. 　　mysqladmin -uroot -password ab12 　　  \n216. 　　注：因为开始时root没有密码，所以-p旧密码一项就可以省略了。　　  \n217. 　　、例：再将root的密码改为djg345  \n218. 　　mysqladmin -uroot -pab12 password djg345  \n219. MYSQL常用命令（下）  \n220. 　　一、操作技巧  \n221. 　　、如果你打命令时，回车后发现忘记加分号，你无须重打一遍命令，只要打个分号回车就可以了。也就是说你可以把一个完整的命令分成几行来打，完后用分号作结束标志就OK。  \n222. 　　、你可以使用光标上下键调出以前的命令。但以前我用过的一个MYSQL旧版本不支持。我现在用的是mysql-3.23.27-beta-win。  \n223. 　　二、显示命令  \n224. 　　、显示数据库列表。  \n225. 　　show databases;  \n226. 　　刚开始时才两个数据库：mysql和test。mysql库很重要它里面有MYSQL的系统信息，我们改密码和新增用户，实际上就是用这个库进行操作。  \n227. 　　、显示库中的数据表：  \n228. 　　use mysql；／／打开库，学过FOXBASE的一定不会陌生吧  \n229. 　　show tables;  \n230. 　　、显示数据表的结构：  \n231. 　　describe 表名;  \n232. 　　、建库：  \n233. 　　create database 库名;  \n234. 　　、建表：  \n235. 　　use 库名；  \n236. 　　create table 表名(字段设定列表)；  \n237. 　　、删库和删表:  \n238. 　　drop database 库名;  \n239. 　　drop table 表名；  \n240. 　　、将表中记录清空：  \n241. 　　delete from 表名;  \n242. 　　、显示表中的记录：  \n243. 　　select  from 表名;  \n244. 三、一个建库和建表以及插入数据的实例  \n245. 　　drop database if exists school; //如果存在SCHOOL则删除  \n246. 　　create database school; //建立库SCHOOL  \n247. 　　use school; //打开库SCHOOL  \n248. 　　create table teacher //建立表TEACHER  \n249. 　　(  \n250. 　　id int(3) auto_increment not null primary key,  \n251. 　　name char(10) not null,  \n252. 　　address varchar(50) default ‘深圳’,  \n253. 　　year date  \n254. 　　); //建表结束  \n255. 　　//以下为插入字段  \n256. 　　insert into teacher values(”,’glchengang’,’深圳一中’,’-10-10′);  \n257. 　　insert into teacher values(”,’jack’,’深圳一中’,’-12-23′);  \n258. 　　注：在建表中（）将ID设为长度为的数字字段:int(3)并让它每个记录自动加一:auto_increment并不能为空:not null而且让他成为主字段primary key  \n259. 　　（）将NAME设为长度为的字符字段  \n260. 　　（）将ADDRESS设为长度的字符字段，而且缺省值为深圳。varchar和char有什么区别呢，只有等以后的文章再说了。  \n261. 　　（）将YEAR设为日期字段。  \n262. 　　如果你在mysql提示符键入上面的命令也可以，但不方便调试。你可以将以上命令原样写入一个文本文件中假设为school.sql，然后复制到c:\\下，并在DOS状态进入目录\\mysql\\bin，然后键入以下命令：  \n263. 　　mysql -uroot -p密码< c:\\school.sql  \n264. 　　如果成功，空出一行无任何显示；如有错误，会有提示。（以上命令已经调试，你只要将//的注释去掉即可使用）。  \n265. 四、将文本数据转到数据库中  \n266. 　　、文本数据应符合的格式：字段数据之间用tab键隔开，null值用\\n来代替.  \n267. 　　例：  \n268. 　　rose 深圳二中1976-10-10  \n269. 　　mike 深圳一中1975-12-23  \n270. 　　、数据传入命令load data local infile “文件名” into table 表名;  \n271. 　　注意：你最好将文件复制到\\mysql\\bin目录下，并且要先用use命令打表所在的库。  \n272. 五、备份数据库：（命令在DOS的\\mysql\\bin目录下执行）  \n273. 　　mysqldump –opt school>school.bbb  \n274. 　　注释:将数据库school备份到school.bbb文件，school.bbb是一个文本文件，文件名任取，打开看看你会有新发现。  \n275. 一.SELECT语句的完整语法为：  \n276. SELECT[ALL|DISTINCT|DISTINCTROW|TOP]  \n277. {*|talbe.*|[table.]field1[AS alias1][,[table.]field2[AS alias2][,…]]}  \n278. FROM tableexpression[,…][IN externaldatabase]  \n279. [WHERE…]  \n280. [GROUP BY…]  \n281. [HAVING…]  \n282. [ORDER BY…]  \n283. [WITH OWNERACCESS OPTION]  \n284. 说明：  \n285. 用中括号([])括起来的部分表示是可选的，用大括号({})括起来的部分是表示必须从中选择其中的一个。  \n286. 1 FROM子句  \n287. FROM 子句指定了SELECT语句中字段的来源。FROM子句后面是包含一个或多个的表达式(由逗号分开)，其中的表达式可为单一表名称、已保存的查询或由INNER JOIN、LEFT JOIN 或RIGHT JOIN 得到的复合结果。如果表或查询存储在外部数据库，在IN 子句之后指明其完整路径。  \n288. 例：下列SQL语句返回所有有定单的客户：  \n289. SELECT OrderID,Customer.customerID  \n290. FROM Orders Customers  \n291. WHERE Orders.CustomerID=Customers.CustomeersID  \n292. 2 ALL、DISTINCT、DISTINCTROW、TOP谓词  \n293. (1) ALL 返回满足SQL语句条件的所有记录。如果没有指明这个谓词，默认为ALL。  \n294. 例：SELECT ALL FirstName,LastName  \n295. FROM Employees  \n296. (2) DISTINCT 如果有多个记录的选择字段的数据相同，只返回一个。  \n297. (3) DISTINCTROW 如果有重复的记录，只返回一个  \n298. (4) TOP显示查询头尾若干记录。也可返回记录的百分比，这是要用TOP N PERCENT子句（其中N 表示百分比）  \n299. 例：返回%定货额最大的定单  \n300. SELECT TOP 5 PERCENT*  \n301. FROM [ Order Details]  \n302. ORDER BY UnitPrice*Quantity*(1-Discount) DESC  \n303. 3 用AS 子句为字段取别名  \n304. 如果想为返回的列取一个新的标题，或者，经过对字段的计算或总结之后，产生了一个新的值，希望把它放到一个新的列里显示，则用AS保留。  \n305. 例：返回FirstName字段取别名为NickName  \n306. SELECT FirstName AS NickName ,LastName ,City  \n307. FROM Employees  \n308. 例：返回新的一列显示库存价值  \n309. SELECT ProductName ,UnitPrice ,UnitsInStock ,UnitPrice*UnitsInStock AS valueInStock  \n310. FROM Products  \n311. 二.WHERE 子句指定查询条件  \n312. 1 比较运算符  \n313. 比较运算符含义  \n314. = 等于  \n315. \\> 大于  \n316. < 小于  \n317. \\>= 大于等于  \n318. <= 小于等于  \n319. <> 不等于  \n320. !> 不大于  \n321. !< 不小于  \n322. 例：返回年月的定单  \n323. SELECT OrderID, CustomerID, OrderDate  \n324. FROM Orders  \n325. WHERE OrderDate>#1/1/96# AND OrderDate<#1/30/96#  \n326. 注意：  \n327. Mcirosoft JET SQL 中，日期用‘#’定界。日期也可以用Datevalue()函数来代替。在比较字符型的数据时，要加上单引号’’，尾空格在比较中被忽略。  \n328. 例：  \n329. WHERE OrderDate>#96-1-1#  \n330. 也可以表示为：  \n331. WHERE OrderDate>Datevalue(‘/1/96’)  \n332. 使用NOT 表达式求反。  \n333. 例：查看年月日以后的定单  \n334. WHERE Not OrderDate<=#1/1/96#  \n335. 2 范围（BETWEEN 和NOT BETWEEN）  \n336. BETWEEN …AND…运算符指定了要搜索的一个闭区间。  \n337. 例：返回年月到年月的定单。  \n338. WHERE OrderDate Between #1/1/96# And #2/1/96#  \n339. 3 列表（IN ，NOT IN）  \n340. IN 运算符用来匹配列表中的任何一个值。IN子句可以代替用OR子句连接的一连串的条件。  \n341. 例：要找出住在London、Paris或Berlin的所有客户  \n342. SELECT CustomerID, CompanyName, ContactName, City  \n343. FROM Customers  \n344. WHERE City In(‘London’,’Paris’,’Berlin’)  \n345. 4 模式匹配(LIKE)  \n346. LIKE运算符检验一个包含字符串数据的字段值是否匹配一指定模式。  \n347. LIKE运算符里使用的通配符  \n348. 通配符含义  \n349. ？任何一个单一的字符  \n350.  *任意长度的字符*  \n351. \\# 0~9之间的单一数字  \n352. [字符列表] 在字符列表里的任一值  \n353. [！字符列表] 不在字符列表里的任一值  \n354. - 指定字符范围，两边的值分别为其上下限  \n355. 例：返回邮政编码在（）-0000到（）-9999之间的客户  \n356. SELECT CustomerID ,CompanyName,City,Phone  \n357. FROM Customers  \n358. WHERE Phone Like ‘(171)555-####’  \n359. LIKE运算符的一些样式及含义  \n360. 样式含义不符合  \n361. LIKE ‘A’A后跟任意长度的字符Bc,c255  \n362. LIKE’[*]’5\\*5 555*  \n363. LIKE’?5’5与之间有任意一个字符55,5wer5  \n364. LIKE’##5’5235，5kd5,5346  \n365. LIKE’[a-z]’a-z间的任意一个字符5,%  \n366. LIKE’[!0-9]’非-9间的任意一个字符0,1  \n367. LIKE’[[]’1,  \n368. 三.用ORDER BY子句排序结果  \n369. ORDER子句按一个或多个（最多个）字段排序查询结果，可以是升序（ASC）也可以是降序（DESC），缺省是升序。ORDER子句通常放在SQL语句的最后。  \n370. ORDER子句中定义了多个字段，则按照字段的先后顺序排序。  \n371. 例：  \n372. SELECT ProductName,UnitPrice, UnitInStock  \n373. FROM Products  \n374. ORDER BY UnitInStock DESC , UnitPrice DESC, ProductName  \n375. ORDER BY 子句中可以用字段在选择列表中的位置号代替字段名，可以混合字段名和位置号。  \n376. 例：下面的语句产生与上列相同的效果。  \n377. SELECT ProductName,UnitPrice, UnitInStock  \n378. FROM Products  \n379. ORDER BY 1 DESC , 2 DESC,3  \n380. 四.运用连接关系实现多表查询  \n381. 例：找出同一个城市中供应商和客户的名字  \n382. SELECT Customers.CompanyName, Suppliers.ComPany.Name  \n383. FROM Customers, Suppliers  \n384. WHERE Customers.City=Suppliers.City  \n385. 例：找出产品库存量大于同一种产品的定单的数量的产品和定单  \n386. SELECT ProductName,OrderID, UnitInStock, Quantity  \n387. FROM Products, [Order Deails]  \n388. WHERE Product.productID=[Order Details].ProductID  \n389. AND UnitsInStock>Quantity  \n390. 另一种方法是用Microsof JET SQL 独有的JNNER JOIN  \n391. 语法：  \n392. FROM table1 INNER JOIN table2  \n393. ON table1.field1 comparision table2.field2  \n394. 其中comparision 就是前面WHERE子句用到的比较运算符。  \n395. SELECT FirstName,lastName,OrderID,CustomerID,OrderDate  \n396. FROM Employees  \n397. INNER JOIN Orders ON Employees.EmployeeID=Orders.EmployeeID  \n398. 注意：  \n399. INNER JOIN不能连接Memo OLE Object Single Double 数据类型字段。  \n400. 在一个JOIN语句中连接多个ON子句  \n401. 语法：  \n402. SELECT fields  \n403. FROM table1 INNER JOIN table2  \n404. ON table1.field1 compopr table2.field1 AND  \n405. ON table1.field2 compopr table2.field2 OR  \n406. ON table1.field3 compopr table2.field3  \n407. 也可以  \n408. SELECT fields  \n409. FROM table1 INNER JOIN  \n410. （table2 INNER JOIN [( ]table3  \n411. [INNER JOER] [( ]tablex[INNER JOIN]  \n412. ON table1.field1 compopr table2.field1  \n413. ON table1.field2 compopr table2.field2  \n414. ON table1.field3 compopr table2.field3  \n415. 外部连接返回更多记录，在结果中保留不匹配的记录，不管存不存在满足条件的记录都要返回另一侧的所有记录。  \n416. FROM table [LEFT|RIGHT]JOIN table2  \n417. ON table1.field1comparision table.field2  \n418. 用左连接来建立外部连接，在表达式的左边的表会显示其所有的数据  \n419. 例：不管有没有定货量，返回所有商品  \n420. SELECT ProductName ,OrderID  \n421. FROM Products  \n422. LEFT JOIN Orders ON Products.PrductsID=Orders.ProductID  \n423. 右连接与左连接的差别在于：不管左侧表里有没有匹配的记录，它都从左侧表中返回所有记录。  \n424. 例：如果想了解客户的信息，并统计各个地区的客户分布，这时可以用一个右连接，即使某个地区没有客户，也要返回客户信息。  \n425. 空值不会相互匹配，可以通过外连接才能测试被连接的某个表的字段是否有空值。  \n426. SELECT   \n427. FROM talbe1  \n428. LEFT JOIN table2 ON table1.a=table2.c  \n429. 1 连接查询中使用Iif函数实现以值显示空值  \n430. Iif表达式：Iif(IsNull(Amount,0,Amout)  \n431. 例：无论定货大于或小于￥，都要返回一个标志。  \n432. Iif([Amount]>50,?Big order?,?Small order?)  \n433. 五. 分组和总结查询结果  \n434. 在SQL的语法里，GROUP BY和HAVING子句用来对数据进行汇总。GROUP BY子句指明了按照哪几个字段来分组，而将记录分组后，用HAVING子句过滤这些记录。  \n435. GROUP BY 子句的语法  \n436. SELECT fidldlist  \n437. FROM table  \n438. WHERE criteria  \n439. [GROUP BY groupfieldlist [HAVING groupcriteria]]  \n440. 注：Microsoft Jet数据库Jet 不能对备注或OLE对象字段分组。  \n441. GROUP BY字段中的Null值以备分组但是不能被省略。  \n442. 在任何SQL合计函数中不计算Null值。  \n443. GROUP BY子句后最多可以带有十个字段，排序优先级按从左到右的顺序排列。  \n444. 例：在‘WA’地区的雇员表中按头衔分组后，找出具有同等头衔的雇员数目大于人的所有头衔。  \n445. SELECT Title ,Count(Title) as Total  \n446. FROM Employees  \n447. WHERE Region = ‘WA’  \n448. GROUP BY Title  \n449. HAVING Count(Title)>1  \n450. JET SQL 中的聚积函数  \n451. 聚集函数意义  \n452. SUM ( ) 求和  \n453. AVG ( ) 平均值  \n454. COUNT ( ) 表达式中记录的数目  \n455. COUNT ( ) 计算记录的数目  \n456. MAX 最大值  \n457. MIN 最小值  \n458. VAR 方差  \n459. STDEV 标准误差  \n460. FIRST 第一个值  \n461. LAST 最后一个值  \n462. 六. 用Parameters声明创建参数查询  \n463. Parameters声明的语法:  \n464. PARAMETERS name datatype[,name datatype[, …]]  \n465. 其中name 是参数的标志符,可以通过标志符引用参数.  \n466. Datatype说明参数的数据类型.  \n467. 使用时要把PARAMETERS 声明置于任何其他语句之前.  \n468. 例:  \n469. PARAMETERS[Low price] Currency,[Beginning date]datatime  \n470. SELECT OrderID ,OrderAmount  \n471. FROM Orders  \n472. WHERE OrderAMount>[low price]  \n473. AND OrderDate>=[Beginning date]  \n474. 七. 功能查询  \n475. 所谓功能查询,实际上是一种操作查询,它可以对数据库进行快速高效的操作.它以选择查询为目的,挑选出符合条件的数据,再对数据进行批处理.功能查询包括更新查询,删除查询,添加查询,和生成表查询.  \n476. 1 更新查询  \n477. UPDATE子句可以同时更改一个或多个表中的数据.它也可以同时更改多个字段的值.  \n478. 更新查询语法:  \n479. UPDATE 表名  \n480. SET 新值  \n481. WHERE 准则  \n482. 例:英国客户的定货量增加%,货运量增加%  \n483. UPDATE OEDERS  \n484. SET OrderAmount = OrderAmount *1.1*  \n485. Freight = Freight*1.03  \n486. WHERE ShipCountry = ‘UK’  \n487. 2 删除查询  \n488. DELETE子句可以使用户删除大量的过时的或冗于的数据.  \n489. 注:删除查询的对象是整个记录.  \n490. DELETE子句的语法:  \n491. DELETE [表名.]  \n492. FROM 来源表  \n493. WHERE 准则  \n494. 例: 要删除所有年前的定单  \n495. DELETE   \n496. FROM Orders  \n497. WHERE OrderData<#94-1-1#  \n498. 3 追加查询  \n499. INSERT子句可以将一个或一组记录追加到一个或多个表的尾部.  \n500. INTO 子句指定接受新记录的表  \n501. valueS 关键字指定新记录所包含的数据值.  \n502. INSERT 子句的语法:  \n503. INSETR INTO 目的表或查询(字段,字段,…)  \n504. valueS(数值,数值,…)  \n505. 例:增加一个客户  \n506. INSERT INTO Employees(FirstName,LastName,title)  \n507. valueS(‘Harry’,’Washington’,’Trainee’)  \n508. 4 生成表查询  \n509. 可以一次性地把所有满足条件的记录拷贝到一张新表中.通常制作记录的备份或副本或作为报表的基础.  \n510. SELECT INTO子句用来创建生成表查询语法:  \n511. SELECT 字段,字段,…  \n512. INTO 新表[IN 外部数据库]  \n513. FROM 来源数据库  \n514. WHERE 准则  \n515. 例:为定单制作一个存档备份  \n516. SELECT   \n517. INTO OrdersArchive  \n518. FROM Orders  \n519. 八. 联合查询  \n520. UNION运算可以把多个查询的结果合并到一个结果集里显示.  \n521. UNION运算的一般语法:  \n522. [表]查询UNION [ALL]查询UNION …  \n523. 例:返回巴西所有供给商和客户的名字和城市  \n524. SELECT CompanyName,City  \n525. FROM Suppliers  \n526. WHERE Country = ‘Brazil’  \n527. UNION  \n528. SELECT CompanyName,City  \n529. FROM Customers  \n530. WHERE Country = ‘Brazil’  \n531. 注:  \n532. 缺省的情况下,UNION子句不返回重复的记录.如果想显示所有记录,可以加ALL选项  \n533. UNION运算要求查询具有相同数目的字段.但是,字段数据类型不必相同.  \n534. 每一个查询参数中可以使用GROUP BY 子句或HAVING 子句进行分组.要想以指定的顺序来显示返回的数据,可以在最后一个查询的尾部使用OREER BY子句.  \n535. 九. 交叉查询  \n536. 交叉查询可以对数据进行总和,平均,计数或其他总和计算法的计算,这些数据通过两种信息进行分组:一个显示在表的左部,另一个显示在表的顶部.  \n537. Microsoft Jet SQL 用TRANSFROM语句创建交叉表查询语法:  \n538. TRANSFORM aggfunction  \n539. SELECT 语句  \n540. GROUP BY 子句  \n541. PIVOT pivotfield[IN(value1 [,value2[,…]]) ]  \n542. Aggfounction指SQL聚积函数,  \n543. SELECT语句选择作为标题的的字段,  \n544. GROUP BY 分组  \n545. 说明：  \n546. Pivotfield 在查询结果集中创建列标题时用的字段或表达式,用可选的IN子句限制它的取值.  \n547. value代表创建列标题的固定值.  \n548. 例:显示在年里每一季度每一位员工所接的定单的数目:  \n549. TRANSFORM Count(OrderID)  \n550. SELECT FirstName&’’&LastName AS FullName  \n551. FROM Employees INNER JOIN Orders  \n552. ON Employees.EmployeeID = Orders.EmployeeID  \n553. WHERE DatePart(“yyyy”,OrderDate)= ‘’  \n554. GROUP BY FirstName&’’&LastName  \n555. ORDER BY FirstName&’’&LastName  \n556. POVOT DatePart(“q”,OrderDate)&’季度’  \n557. 十.子查询  \n558. 子查询可以理解为套查询.子查询是一个SELECT语句.  \n559. 1 表达式的值与子查询返回的单一值做比较  \n560. 语法:  \n561. 表达式comparision [ANY|ALL|SOME](https://blog.csdn.net/qq_35409127/article/details/%E5%AD%90%E6%9F%A5%E8%AF%A2)  \n562. 说明：  \n563. ANY 和SOME谓词是同义词,与比较运算符(=,<,>,<>,<=,>=)一起使用.返回一个布尔值True或False.ANY的意思是,表达式与子查询返回的一系列的值逐一比较,只要其中的一次比较产生True结果,ANY测试的返回True值(既WHERE子句的结果),对应于该表达式的当前记录将进入主查询的结果中.ALL测试则要求表达式与子查询返回的一系列的值的比较都产生True结果,才回返回True值.  \n564. 例:主查询返回单价比任何一个折扣大于等于%的产品的单价要高的所有产品  \n565. SELECT  FROM Products  \n566. WHERE UnitPrice>ANY  \n567. (SELECT UnitPrice FROM[Order Details] WHERE Discount>0.25)  \n568. 2 检查表达式的值是否匹配子查询返回的一组值的某个值  \n569. 语法:  \n570. [NOT]IN(子查询)  \n571. 例:返回库存价值大于等于的产品.  \n572. SELECT ProductName FROM Products  \n573. WHERE ProductID IN  \n574. (SELECT PrdoctID FROM [Order DEtails]  \n575. WHERE UnitPrice*Quantity>= 1000)  \n576. 3检测子查询是否返回任何记录  \n577. 语法:  \n578. [NOT]EXISTS (子查询)  \n579. 例:用EXISTS检索英国的客户  \n580. SELECT ComPanyName,ContactName  \n581. FROM Orders  \n582. WHERE EXISTS  \n583. (SELECT   \n584. FROM Customers  \n585. WHERE Country = ‘UK’AND  \n586. Customers.CustomerID= Orders.CustomerID)  \n\n","tags":["数据库"]},{"title":"设计模式","url":"/2018/09/23/pout/python中高级面试题/设计模式/","content":"\n\n<!-- toc -->\n\n## 设计模式\n\nPython设计模式 设计模式的定义:为了解决面向对象系统中重要和重复的设计封装在一起的一种代码实现框架,可以使得代码更加易于扩展和调用\n\n四个基本要素:模式名称,问题,解决方案,效果\n\n六大原则:\n\n　　1.开闭原则:一个软件实体,如类,模块和函数应该对扩展开发,对修改关闭.既软件实体应尽量在不修改原有代码的情况下进行扩展.\n\n```\n   2.里氏替换原则:所有引用父类的方法必须能透明的使用其子类的对象\n\n   3.依赖倒置原则:高层模块不应该依赖底层模块,二者都应该依赖其抽象,抽象不应该依赖于细节,细节应该依赖抽象,换而言之,要针对接口编程而不是针对实现编程\n\n   4.接口隔离原则:使用多个专门的接口,而不是使用单一的总接口,即客户端不应该依赖那些并不需要的接口\n\n   5.迪米特法则:一个软件实体应该尽可能的少与其他实体相互作用\n\n   6.单一直责原则:不要存在多个导致类变更的原因.即一个类只负责一项职责\n```\n\n### 接口\n\n定义:一种特殊的类,声明了若干方法,要求继承该接口的类必须实现这种方法\n\n作用:限制继承接口的类的方法的名称及调用方式,隐藏了类的内部实现\n\n```\nfrom abc import ABCMeta,abstractmethod\n\nclass Payment(metaclass=ABCMeta):\n    @abstractmethod#定义抽象方法的关键字\n    def pay(self,money):\n        pass\n\n    # @abstractmethod\n    # def pay(self,money):\n    #     raise NotImplementedError\n\nclass AiliPay(Payment):\n    #子类继承接口,必须实现接口中定义的抽象方法,否则不能实例化对象\n    def pay(self,money):\n        print('使用支付宝支付%s元'%money)\n\nclass ApplePay(Payment):\n    def pay(self,money):\n        print('使用苹果支付支付%s元'%money)\n```\n\n### 单例模式\n\n定义:保证一个类只有一个实例,并提供一个访问它的全局访问点\n\n适用场景:当一个类只能有一个实例而客户可以从一个众所周知的访问点访问它时\n\n优点:对唯一实例的受控访问,相当于全局变量,但是又可以防止此变量被篡改\n\n```\nclass Singleton(object):\n    #如果该类已经有了一个实例则直接返回,否则创建一个全局唯一的实例\n    def __new__(cls, *args, **kwargs):\n        if not hasattr(cls,'_instance'):\n            cls._instance = super(Singleton,cls).__new__(cls)\n        return cls._instance\n\nclass MyClass(Singleton):\n    def __init__(self,name):\n        if name:\n            self.name = name\n\na = MyClass('a')\nprint(a)\nprint(a.name)\n\nb = MyClass('b')\nprint(b)\nprint(b.name)\n\nprint(a)\nprint(a.name)\n```\n\n### 工厂模式（celery）\n\n定义:不直接向客户暴露对象创建的实现细节,而是通过一个工厂类来负责创建产品类的实例\n\n角色:工厂角色,抽象产品角色,具体产品角色\n\n优点:隐藏了对象创建代码的细节,客户端不需要修改代码\n\n缺点:违反了单一职责原则,将创建逻辑集中到一个工厂里面,当要添加新产品时,违背了开闭原则\n\n```\nfrom abc import ABCMeta,abstractmethod\n\nclass Payment(metaclass=ABCMeta):\n    #抽象产品角色\n    @abstractmethod\n    def pay(self,money):\n        pass\n\n\n\nclass AiliPay(Payment):\n    #具体产品角色\n    def __init__(self,enable_yuebao=False):\n        self.enable_yuebao = enable_yuebao\n\n    def pay(self,money):\n        if self.enable_yuebao:\n            print('使用余额宝支付%s元'%money)\n        else:\n            print('使用支付宝支付%s元'%money)\n\nclass ApplePay(Payment):\n    # 具体产品角色\n    def pay(self,money):\n        print('使用苹果支付支付%s元'%money)\n\nclass PaymentFactory:\n    #工厂角色\n    def create_payment(self,method):\n        if method == 'alipay':\n            return AiliPay()\n        elif method == 'yuebao':\n            return AiliPay(True)\n        elif method == 'applepay':\n            return ApplePay()\n        else:\n            return NameError\n\np = PaymentFactory()\nf = p.create_payment('yuebao')\nf.pay(100)\n```\n\n### 观察者模式\n\n定义:定义对象间的一种一对多的依赖关系,当一个对象的状态发生改变时,所有依赖它的对象都会得到通知并被自动更新.观察者模式又称为'发布订阅'模式\n\n角色:抽象主题,具体主题(发布者),抽象观察者,具体观察者(订阅者)\n\n适用场景:当一个抽象模型有两个方面,其中一个方面依赖于另一个方面.将两者封装在独立的对象中以使它们各自独立的改变和复用\n\n当一个对象的改变需要同时改变其他对象,而且不知道具体有多少对象以待改变\n\n当一个对象必须通知其他对象,而又不知道其他对象是谁,即这些对象之间是解耦的\n\n优点:目标和观察者之间的耦合最小,支持广播通信\n\n缺点:多个观察者之间互不知道对方的存在,因此一个观察者对主题的修改可能造成错误的更新\n\n```\nfrom abc import ABCMeta, abstractmethod\n\n#抽象主题\nclass Oberserver(metaclass=ABCMeta):\n    @abstractmethod\n    def update(self):\n        pass\n\n#具体主题\nclass Notice:\n    def __init__(self):\n        self.observers = []\n\n    def attach(self,obs):\n        self.observers.append(obs)\n\n    def detach(self,obs):\n        self.observers.remove(obs)\n\n    def notify(self):\n        for obj in self.observers:\n            obj.update(self)\n\n#抽象观察者\nclass ManagerNotice(Notice):\n    def __init__(self,company_info=None):\n        super().__init__()\n        self.__company_info = company_info\n\n    @property\n    def company_info(self):\n        return self.__company_info\n\n    @company_info.setter\n    def company_info(self,info):\n        self.__company_info = info\n        self.notify()\n\n#具体观察者\nclass Manager(Oberserver):\n    def __init__(self):\n        self.company_info = None\n    def update(self,noti):\n        self.company_info = noti.company_info\n\n#消息订阅-发送\nnotice = ManagerNotice()\n\nalex=Manager()\ntony=Manager()\n\nnotice.attach(alex)\nnotice.attach(tony)\nnotice.company_info=\"公司运行良好\"\nprint(alex.company_info)\nprint(tony.company_info)\n\nnotice.company_info=\"公司将要上市\"\nprint(alex.company_info)\nprint(tony.company_info)\n\nnotice.detach(tony)\nnotice.company_info=\"公司要破产了，赶快跑路\"\nprint(alex.company_info)\nprint(tony.company_info)\n```\n","tags":["python中高级面试题"]},{"title":"从源码分析sentry的错误信息收集","url":"/2018/08/18/pout/从源码分析sentry的错误信息收集/","content":"\nraven.js 是 sentry 为 JavaScript 错误上报提供的 JS-SDK，本篇我们基于其源代码对其原理进行分析，本篇文章只分析前端部分，对应的文件目录是`https://github.com/getsentry/sentry-javascript/tree/master/packages/raven-js`。\n\n首先抛出几个问题：\n\n* **raven.js 是如何收集浏览器错误信息的？**\n* **raven.js 上报的错误信息格式是什么样的？又是如何把这些信息传给后端？支不支持合并上报？**\n* **面包屑（breadcrumbs）是什么？raven.js 如何来收集面包屑信息？**\n* **raven.js 如何和框架配合使用（比如 vue、react）？**\n\n在回答以上这几个问题之前，我们首先来对 raven.js 做一个宏观的分析，主要涉及其文件目录、所引用的第三方框架等。\n\nraven.js 的核心文件内容并不多，其中使用了三个第三方库，放在了 vendor 文件夹下：\n\n* [json-stringify-safe](https://github.com/moll/json-stringify-safe) ：一个对 `JSON.stringify` 的封装，安全的 json 序列化操作函数，不会抛出循环引用的错误。\n\t* 这里面有一个注意点要单独说一下，我们熟知的 `JSON.stringify` , 可以接受三个参数：第一个参数是我们要序列化的对象；第二个参数是对其中键值对的处理函数；第三个参数是控制缩进空格。reven.js 的 `json-stringify-safe` 就是充分利用了这三个参数。\n* [md5](https://github.com/blueimp/JavaScript-MD5)：js 的 md5 函数。\n* [TraceKit](https://github.com/csnover/TraceKit)：TraceKit 是一个已经比较完善的错误收集、堆栈格式化的库，reven.js 的功能在很大程度上对它有所依赖。\n\n除此之外，raven.js 支持插件，官方提供的一些知名库的 sentry 插件主要放在了 plugin 文件夹下面，raven.js 的一些核心文件，则放在了 src 文件夹下面。\n\n### raven.js 是如何收集错误信息的？\n\n我们知道，在前端收集错误，肯定离不开 `window.onerror` 这个函数，那么我们就从这个函数说起。\n\n实际上，这部分工作是 raven.js 引用的第三方库 TraceKit 完成的：\n\n```\nfunction installGlobalHandler() {\n  if (_onErrorHandlerInstalled) { // 一个起到标志作用的全局变量\n    return;\n  }\n  _oldOnerrorHandler = _window.onerror; \n  // _oldOnerrorHandler 是防止对用户其他地方定义的回调函数进行覆盖\n  // 该 _window 经过兼容，实际上就是 window\n  _window.onerror = traceKitWindowOnError;\n  _onErrorHandlerInstalled = true;\n}\n```\n\n相关错误回调函数交给 traceKitWindowOnError 处理，下面我们来看一下 traceKitWindowOnError 函数，为了避免太多冗余代码，我们仅分析一种主要情况：\n\n```\nfunction traceKitWindowOnError(msg, url, lineNo, colNo, ex) {\n\t\n\tvar exception = utils.isErrorEvent(ex) ? ex.error : ex;\n\t//...\n    stack = TraceKit.computeStackTrace(exception);\n    notifyHandlers(stack, true);\n    //...\n   \n    //...\n    if (_oldOnerrorHandler) {\n       return _oldOnerrorHandler.apply(this, arguments);\n    }\n    return false;\n}\n```\n\n其中调用的最重要的一个函数，就是 computeStackTrace，而这个函数也是 TraceKit 的核心函数，简单来讲，它做的事情就是统一格式化报错信息调用栈，因为对于各个浏览器来说，返回的 Error 调用栈信息格式不尽相同，另外甚至还有的浏览器并不返回调用栈，computeStackTrace 函数对这些情况都做了兼容性处理，并且对于一些不返回调用栈的情况，还使用了 caller 来向上回溯函数的调用栈，最终把报错信息转化成一个键相同的对象数组，做到了报错信息格式的统一。\n\nnotifyHandlers 函数则是通知相关的回调函数。 实际上，raven.js 在 install 函数中会调用 TraceKit.report.subscribe 函数，并把对错误的处理逻辑写入回调：\n\n```\nfunction subscribe(handler) {\n    installGlobalHandler();\n    handlers.push(handler);\n}\n```\n\n以上过程完成了错误处理过程中的负责角色转换，并且借助 TraceKit，可以使 raven.js 得到一个结构比较清晰的带有格式化好的调用栈信息的错误内容对象，之后，raven.js 对错误内容进一步处理并最终上报。\n\n下面我们对错误处理 raven.js 控制的部分做了一些梳理：\n\n```\n _handleOnErrorStackInfo: function(stackInfo, options) {\n    options.mechanism = options.mechanism || {\n      type: 'onerror',\n      handled: false\n    };\n    // mechanism 和错误统计来源有关\n\n    if (!this._ignoreOnError) {\n      this._handleStackInfo(stackInfo, options);\n    }\n},\n\n_handleStackInfo: function(stackInfo, options) {\n    var frames = this._prepareFrames(stackInfo, options);\n\n    this._triggerEvent('handle', {\n      stackInfo: stackInfo,\n      options: options\n    });\n\n    this._processException(\n      stackInfo.name,\n      stackInfo.message,\n      stackInfo.url,\n      stackInfo.lineno,\n      frames,\n      options\n    );\n},\n\n_processException: function(type, message, fileurl, lineno, frames, options) {\n    // 首先根据 message 信息判断是否是需要忽略的错误类型\n    // 然后判断出错的文件是否在黑名单中或者白名单中\n    // 接下来对错误内容进行必要的整合与转换，构造出 data 对象\n    // 最后调用上报函数\n    this._send(data);\n}\n\n_send: function(data) {\n\t\n\t// 对 data 进一步处理，增加必要的信息，包括后续会提到的面包屑信息\n\n\t// 交由 _sendProcessedPayload 进行进一步处理\n\tthis._sendProcessedPayload(data);\n}\n\n_sendProcessedPayload: function(data, callback) {\n\n\t// 对 data 增加一些必要的元信息\n\t// 可以通过自定义 globalOptions.transport 的方式来自定义上报函数 \n\t(globalOptions.transport || this._makeRequest).call(this, {\n\t     url: url,\n\t     auth: auth,\n\t     data: data,\n\t     options: globalOptions,\n\t     onSuccess: function success() {\n\t       \n\t     },\n\t     onError: function failure(error) {\n\t       \n\t     }\n\t});\n}    \n\n// 真正发起请求的函数\n_makeRequest: function(opts) {\n\t// 对于支持 fetch 的浏览器，直接使用 fetch 的方式发送 POST 请求\n\t// 如果浏览器不支持 fetch，则使用 XHR 的传统方式发送 POST 请求\n}\n``` \n\n实际上我们可以发现，从拿到已经初步格式化的报错信息，到最终真正执行数据上报，raven.js 的过程非常漫长，这其中我分析有如下几个原因：\n\n* 每个函数只处理一件或者一些事情，保持函数的短小整洁。\n* 部分函数可以做到复用（因为除了自动捕获错误的方式， raven.js 还提供通过 captureException，即 `try {\n    doSomething(a[0])\n} catch(e) {\n    Raven.captureException(e)\n}` 的方式来上报错误，两个过程中有一些函数的调用是有重叠的）。\n\n但是笔者认为，raven.js 的代码设计还有很多值得优化的地方，比如：\n\n* 对最终上报数据（data）的属性处理和增加分散在多个函数，并且有较多可选项目，很难梳理出一个完整的 data 格式，并且不便于维护。\n* 部分函数的拆分必要性不足，并且会增加链路的复杂性，比如 `_processException `、`_sendProcessedPayload `、`_makeRequest `等都只在一个链路中被调用一次。\n* 部分属性重命名会造成资源浪费，由于 TraceKit 部分最终返回的数据格式并不完全满足 raven.js 的需要，所以 raven.js 之后又在较后阶段进行了重命名等处理，实际上这些内容完全可以通过一些其他的方式避免。\n\n最后，非常遗憾，sentry 目前完全不支持合并上报，就算是在同一个事件循环（甚至事件循环的同一个阶段，关于事件循环，可以参考我之前绘制的[一张图](https://www.processon.com/view/link/5b6ec8cbe4b053a09c2fb977)）的两个错误，sentry 都是分开来上报的，这里有一个简单例子：\n\n```javascript\nRaven.config('http://8ec3f1a9f652463bb58191bd0b35f20c@localhost:9000/2').install()\nlet s = window.ss;\n\ntry{\n    let b = s.b\n} catch (e) {\n    Raven.captureException(e)\n    // sentry should report error now\n}\n\ns.nomethod();\n// sentry should report error now\n```\n\n以上例子中，sentry 会发送两个 POST 请求。\n\n### raven.js 最终上报数据的格式\n\n\n这一部分，我们并不会详细地分析 raven.js 上报的数据的每一项内容，仅会给读者展示一个比较典型的情况。\n\n我们看一下对于一个一般的 js 错误，raven.js 上报的 json 中包含哪些内容，下面是一个已经删掉一些冗余内容的典型上报信息：\n\n```\n{\n  \"project\": \"2\",\n  \"logger\": \"javascript\",\n  \"platform\": \"javascript\",\n  \"request\": {\n    \"headers\": {\n      \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1\"\n    },\n    \"url\": \"http://localhost:63342/sentry-test1/test1.html?_ijt=j54dmgn136gom08n8v8v9fdddu\"\n  },\n  \"exception\": {\n    \"values\": [\n      {\n        \"type\": \"TypeError\",\n        \"value\": \"Cannot read property 'b' of undefined\",\n        \"stacktrace\": {\n          \"frames\": [\n            {\n              \"filename\": \"http://localhost:63342/sentry-test1/test1.html?_ijt=j54dmgn136gom08n8v8v9fdddu\",\n              \"lineno\": 19,\n              \"colno\": 19,\n              \"function\": \"?\",\n              \"in_app\": true\n            }\n          ]\n        }\n      }\n    ],\n    \"mechanism\": {\n      \"type\": \"generic\",\n      \"handled\": true\n    }\n  },\n  \"transaction\": \"http://localhost:63342/sentry-test1/test1.html?_ijt=j54dmgn136gom08n8v8v9fdddu\",\n  \"extra\": {\n    \"session:duration\": 6\n  },\n  \"breadcrumbs\": {\n    \"values\": [\n      {\n        \"timestamp\": 1534257309.996,\n        \"message\": \"_prepareFrames stackInfo: [object Object]\",\n        \"level\": \"log\",\n        \"category\": \"console\"\n      },\n      // ...\n   ]\n  },\n  \"event_id\": \"ea0334adaf9d43b78e72da2b10e084a9\",\n  \"trimHeadFrames\": 0\n}\n```\n\n其中支持的信息类型重点分为以下几种：\n\n* sentry 基本配置信息，包括库本身的配置和使用者的配置信息，以及用户的一些自定义信息\n* 错误信息，主要包括错误调用栈信息\n* request 信息，主要包括浏览器的 User-Agent、当前请求地址等\n* 面包屑信息，关于面包屑具体指的是什么，我们会在下一环节进行介绍\n\n### raven.js 面包屑收集\n\n面包屑信息，也就是错误在发生之前，一些用户、浏览器的行为信息，raven.js 实现了一个简单的队列（有一个最大条目长度，默认为 100），这个队列在时刻记录着这些信息，一旦错误发生并且需要上报，raven.js 就把这个队列的信息内容，作为面包屑 breadcrumbs，发回客户端。\n\n面包屑信息主要包括这几类：\n\n* 用户对某个元素的点击或者用户对某个可输入元素的输入\n* 发送的 http 请求\n* console 打印的信息（支持配置 'debug', 'info', 'warn', 'error', 'log' 等不同级别）\n* window.location 变化信息\n\n接下来，我们对这几类面包屑信息 sentry 的记录实现进行简单的分析。\n\n实际上，sentry 对这些信息记录的方式比较一致，都是通过对原声的函数进行包装，并且在包装好的函数中增加自己的钩子函数，来实现触发时候的事件记录，实际上，sentry 总共包装的函数有：\n\n* window.setTimeout\n* window.setInterval\n* window.requestAnimationFrame\n* EventTarget.addEventListener\n* EventTarget.removeEventListener\n* XMLHTTPRequest.open\n* XMLHTTPRequest.send\n* window.fetch\n* History.pushState\n* History.replaceState\n\n>备注：这里包装的所有函数，其中有一部分只是使 raven.js 具有捕获回调函数中错误的能力（对回调函数进行包装）\n\n接下来我们看一段典型的代码，来分析 raven.js 是如何记录用户的点击和输入信息的（通过对 EventTarget.addEventListener 进行封装）：\n\n```javascript\nfunction wrapEventTarget(global) {\n      var proto = _window[global] && _window[global].prototype;\n      if (proto && proto.hasOwnProperty && proto.hasOwnProperty('addEventListener')) {\n        fill(\n          proto,\n          'addEventListener',\n          function(orig) {\n            return function(evtName, fn, capture, secure) {\n              try {\n                if (fn && fn.handleEvent) { //兼容通过 handleEvent 的方式进行绑定事件\n                  fn.handleEvent = self.wrap(\n                    {\n                      mechanism: {\n                        type: 'instrument',\n                        data: {\n                          target: global,\n                          function: 'handleEvent',\n                          handler: (fn && fn.name) || '<anonymous>'\n                        }\n                      }\n                    },\n                    fn.handleEvent\n                  );\n                }\n              } catch (err) {\n              }\n\n              var before, clickHandler, keypressHandler;\n\n              if (\n                autoBreadcrumbs &&\n                autoBreadcrumbs.dom &&\n                (global === 'EventTarget' || global === 'Node')\n              ) {\n                // NOTE: generating multiple handlers per addEventListener invocation, should\n                //       revisit and verify we can just use one (almost certainly)\n                clickHandler = self._breadcrumbEventHandler('click');\n                keypressHandler = self._keypressEventHandler();\n                before = function(evt) { // 钩子函数，用于在回调函数调用的时候记录信息\n                  if (!evt) return;\n\n                  var eventType;\n                  try {\n                    eventType = evt.type;\n                  } catch (e) {\n                    // just accessing event properties can throw an exception in some rare circumstances\n                    // see: https://github.com/getsentry/raven-js/issues/838\n                    return;\n                  }\n                  if (eventType === 'click') return clickHandler(evt);\n                  else if (eventType === 'keypress') return keypressHandler(evt);\n                };\n              }\n              return orig.call(\n                this,\n                evtName,\n                self.wrap(\n                  {\n                    mechanism: {\n                      type: 'instrument',\n                      data: {\n                        target: global,\n                        function: 'addEventListener',\n                        handler: (fn && fn.name) || '<anonymous>'\n                      }\n                    }\n                  },\n                  fn,\n                  before\n                ),\n                capture,\n                secure\n              );\n            };\n          },\n          wrappedBuiltIns\n        );\n        fill(\n          proto,\n          'removeEventListener',\n          function(orig) {\n            return function(evt, fn, capture, secure) {\n              try {\n                fn = fn && (fn.__raven_wrapper__ ? fn.__raven_wrapper__ : fn);\n              } catch (e) {\n                // ignore, accessing __raven_wrapper__ will throw in some Selenium environments\n              }\n              return orig.call(this, evt, fn, capture, secure);\n            };\n          },\n          wrappedBuiltIns\n        );\n      }\n    }\n```\n\n以上代码兼容了通过 handleEvent 的方式进行绑定事件（如果没有听说过这种方式，可以在[这里](http://www.ayqy.net/blog/handleevent%E4%B8%8Eaddeventlistener/)补充一些相关的知识）。\n\n默认情况下，raven.js 只记录通过 `EventTarget.addEventListener` 绑定的点击和输入信息，实际上这是比较科学的，并且这些信息较为有效。另外，raven.js 也提供了记录所有点击和输入信息的可选项，其实现方式更为简单，直接在 document 上添加相关的监听即可。\n\n### raven.js 如何和框架配合使用\n\nraven.js 和框架配合使用的方式非常简单，但是我们要知道，很多框架内置了错误边界处理，或者对错误进行转义。以至于我们通过 window.onerror 的方式得不到完整的错误信息。同时，有些框架提供了错误处理的接口（比如 vue），利用错误处理的接口，我们能够获取到和错误有关的更多更重要的信息。\n\nraven.js 利用各个框架的官方接口，提供了 vue、require.js、angular、ember、react-native 等各个框架的官方插件。\n\n插件内容本身非常简单，我们可以看一下 vue 插件的代码：\n\n```\nfunction formatComponentName(vm) {\n  if (vm.$root === vm) {\n    return 'root instance';\n  }\n  var name = vm._isVue ? vm.$options.name || vm.$options._componentTag : vm.name;\n  return (\n    (name ? 'component <' + name + '>' : 'anonymous component') +\n    (vm._isVue && vm.$options.__file ? ' at ' + vm.$options.__file : '')\n  );\n}\n\nfunction vuePlugin(Raven, Vue) {\n  Vue = Vue || window.Vue;\n\n  // quit if Vue isn't on the page\n  if (!Vue || !Vue.config) return;\n\n  var _oldOnError = Vue.config.errorHandler;\n  Vue.config.errorHandler = function VueErrorHandler(error, vm, info) {\n    var metaData = {};\n\n    // vm and lifecycleHook are not always available\n    if (Object.prototype.toString.call(vm) === '[object Object]') {\n      metaData.componentName = formatComponentName(vm);\n      metaData.propsData = vm.$options.propsData;\n    }\n\n    if (typeof info !== 'undefined') {\n      metaData.lifecycleHook = info;\n    }\n\n    Raven.captureException(error, {\n      extra: metaData\n    });\n\n    if (typeof _oldOnError === 'function') {\n      _oldOnError.call(this, error, vm, info);\n    }\n  };\n}\n\nmodule.exports = vuePlugin;\n```\n\n应该不用进行过多解释。\n\n你也许想知道为什么没有提供 react 插件，事实上，react 16 以后才引入了[Error Boundaries](https://reactjs.org/blog/2017/07/26/error-handling-in-react-16.html)，这种方式由于灵活性太强，并不太适合使用插件，另外，就算不使用插件，也非常方便地使用 raven.js 进行错误上报，可以参考[这里](https://docs.sentry.io/clients/javascript/integrations/react/)\n\n>但笔者认为，目前 react 的引入方式会对源代码进行侵入，并且比较难通过构建的方式进行 sentry 的配置，也许我们可以寻找更好的方式。\n\n完。\n\n","tags":["前端监控"]},{"title":"Jenkins自动化部署","url":"/2018/07/27/pout/运维/Jenkins自动化部署/","content":"\n\n<!-- toc -->\n\n\n# 在阿里云Centos7.6上利用docker搭建Jenkins来自动化部署Django项目 \n\n \n\n​    需求分析—原型设计—开发代码—内网部署-提交测试—确认上线—备份数据—外网更新-最终测试，如果发现外网部署的代码有异常，需要及时回滚。\n\n​    整个过程相当复杂而漫长，其中还需要输入不少的命令，比如上传代码，git的拉取或者合并分支等等。\n\n​    Jenkins是目前非常流行的一款持续集成工具，可以帮助大家把更新后的代码自动部署到服务器上运行，整个流程非常自动化，你可以理解为部署命令操作的可视化界面。\n\n​    \n\n​    Jenkins主要有三种安装方式\n    下载官方war包，放到tomcat中直接运行。\n    yum安装。\n    使用官方docker镜像。\n\n​    \n\n​    毫无疑问，既然有docker这么简单方便的工具，就没必要选择前两种复杂的安装方式了。\n\n​    首先安装docker\n\n​    \n\n```\ncentos 安装docker\n1 docker 要求 CentOS 系统的内核版本高于 3.10 ，查看本页面的前提条件来验证你的CentOS 版本是否支持 Docker \n2、使用 root 权限登录 Centos。确保 yum 包更新到最新。\nsudo yum update\n3、卸载旧版本(如果安装过旧版本的话)\nsudo yum remove docker  docker-common docker-selinux docker-engine\n4、安装需要的软件包， yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n5、设置yum源\nsudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n6、可以查看所有仓库中所有docker版本，并选择特定版本安装\n yum list docker-ce --showduplicates | sort -r\n7、安装docker\nsudo yum install docker-ce \n8、启动并加入开机启动\nsudo systemctl start docker\nsudo systemctl enable docker\n9、验证安装是否成功(有client和service两部分表示docker安装启动都成功了)\ndocker version\n```\n\n然后下载jenkins官方docker镜像\n\n```\ndocker pull jenkins/jenkins\n```\n\n查看镜像 docker images\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528100231_11283.png)\n\n在主机上创建目录，并添加读写权限以便jenkins应用运行时读写文件\n\n```\nmkdir /root/j_node\nchmod 777 /root/j_node\n```\n\n后台将镜像以容器的形式起服务，对端口映射，同时把刚刚建立的目录挂载到容器中\n\n```\ndocker run -d --name jenkins -p 8081:8080 -p 50000:50000 -v /root/j_node:/var/jenkins_home jenkins/jenkins\n```\n\n这里注意，如果是阿里云的话，安全策略需要暴露8081端口\n\n通过网址访问 http://你的ip:8081\n\n然后通过命令获取安装秘钥\n\n```\ndocker logs jenkins\n```\n\n \n\n \n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528101043_90703.png)\n\n完毕后，根据提示设置登陆账户\n\n然后新建一个项目，在源代码控制那一栏，输入你的项目的线上git仓库地址，注意默认应该是master分支，因为生产环境部署的代码必须是主分支\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528101449_38616.png)\n\n保存后，点击Build Now进行部署，jenkins会自动去git版本库中抽取最新的master分支进行部署，同时每部署一次的历史记录都会被保存下来\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528101737_28436.png)\n\n此时，进入/root/j_node 目录下 发现项目已经部署在了workspace目录下\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190528/20190528101900_12377.png)\n\n整个过程非常简单，每次上线之前，项目经理只需要检查各个组员的代码，然后统一合并到主分支master，最后进入jenkins点击部署按钮即可，节约了不少时间。\n","tags":["运维"]},{"title":"可变类型和不可变类型","url":"/2018/07/03/pout/python中高级面试题/可变类型和不可变类型/","content":"\n\n<!-- toc -->\n\n# 列表和元组之间的区别是？\n\n二者的主要区别是列表是可变的，而元组是不可变的\n\n不同点一：不可变 VS 可变 两种类型除了字面上的区别(括号与方括号)之外，最重要的一点是tuple是不可变类型，大小固定，而 list 是可变类型、数据可以动态变化，这种差异使得两者提供的方法、应用场景、性能上都有很大的区别。\n\n不同点二：同构 VS 异构 tuple 用于存储异构(heterogeneous)数据，当做没有字段名的记录来用，比如用 tuple 来记录一个人的身高、体重、年龄。\n\nperson = (\"zhangsan\", 20, 180, 80) 比如记录坐标上的某个点\n\npoint = (x, y) 而列表一般用于存储同构数据(homogenous)，同构数据就是具有相同意义的数据，比如下面的都是字符串类型\n\n[\"zhangsan\", \"Lisi\", \"wangwu\"] 再比如 list 存放的多条用户记录\n\n[(\"zhangsan\", 20, 180, 80), (\"wangwu\", 20, 180, 80)] 数据库操作中查询出来的记录就是由元组构成的列表结构。\n\n因为 tuple 作为没有名字的记录来使用在某些场景有一定的局限性，所以又有了一个 namedtuple 类型的存在，namedtuple 可以指定字段名，用来当做一种轻量级的类来使用。\n\n# 列表和字典的区别\n\n列表是序列，可以理解为数据结构中的数组，字典可以理解为数据结构中的hashmap\n\n他俩都可以作为集合来存储数据\n\n从差异特征上来说\n\n1. list是有序的，dict是无需的\n2. list通过索引访问，dict使用key访问\n3. list随着数量的正常增长要想查找元素的时间复杂度为O(n), dict不随数量而增长而变化，时间负责都为O(1)\n4. dict的占用内存稍比list大，会在1.5倍左右\n\n特征决定用途：\n\nlist一般可作为队列、堆栈使用，而dict一般作为聚合统计或者快速使用特征访问等\n\n# 可变与不可变\n\n可变类型（mutable）：列表，字典\n\n不可变类型（unmutable）：数字，字符串，元组\n\n这里的可变不可变，是指内存中的那块内容（value）是否可以被改变。如果是不可变类型，在对对象本身操作的时候，必须在内存中新申请一块区域(因为老区域#不可变#)。如果是可变类型，对对象操作的时候，不需要再在其他地方申请内存，只需要在此对象后面连续申请(+/-)即可，也就是它的address会保持不变，但区域会变长或者变短。\n\ncopy.copy() 浅拷贝\n\ncopy.deepcopy() 深拷贝\n\n浅拷贝是新创建了一个跟原对象一样的类型，但是其内容是对原对象元素的引用。这个拷贝的对象本身是新的，但内容不是。拷贝序列类型对象（列表\\元组）时，默认是浅拷贝。\n\n# 列表底层实现\n\n实现细节 python中的列表的英文名是list，因此很容易和其它语言(C++, Java等)标准库中常见的链表混淆。事实上CPython的列表根本不是列表（可能换成英文理解起来容易些：python中的list不是list）。在CPython中，列表被实现为长度可变的数组。\n\n从细节上看，Python中的列表是由对其它对象的引用组成的连续数组。指向这个数组的指针及其长度被保存在一个列表头结构中。这意味着，每次添加或删除一个元素时，由引用组成的数组需要该标大小（重新分配）。幸运的是，Python在创建这些数组时采用了指数过分配，所以并不是每次操作都需要改变数组的大小。但是，也因为这个原因添加或取出元素的平摊复杂度较低。\n\n不幸的是，在普通链表上“代价很小”的其它一些操作在Python中计算复杂度相对过高。\n\n利用 list.insert方法在任意位置插入一个元素——复杂度O(N) 利用 list.delete或del删除一个元素——复杂度O(N)\n\n### 列表推导\n\n要习惯用列表推导，因为这更加高效和简短，涉及的语法元素少。在大型的程序中，这意味着更少的错误，代码也更容易阅读。\n\n```\n>>>[i for i in range(10) if i % 2 == 0]\n    [0, 2, 4, 6, 8]\n```\n\n1.使用enumerate.在循环使用序列时，这个内置函数可以方便的获取其索引：\n\n```\nfor i, element in enumerate(['one', 'two', 'three']):\n    print(i, element)\n```\n\n# 字典底层实现\n\n字典是python中最通用的数据结构之一。dict可以将一组唯一的键映射到相应的值。\n\n字典推导式\n\n```\nsquares = {number: number**2 for number in range(10)}\nprint(squares)\n```\n\n在遍历字典元素时，有一点需要特别注意。字典里的keys(), values()和items()3个方法的返回值不再是列表，而是视图对象（view objects）。\n\nkeys(): 返回dict_keys对象，可以查看字典所有键\n\nvalues():返回dict_values对象，可以查看字典的所有值\n\nitems():返回dict_items对象，可以查看字典所有的{key, value}二元元组。\n\nCPython使用伪随机探测(pseudo-random probing)的散列表(hash table)作为字典的底层数据结构。由于这个实现细节，只有可哈希的对象才能作为字典的键。\n\nPython中所有不可变的内置类型都是可哈希的。可变类型（如列表，字典和集合）就是不可哈希的，因此不能作为字典的键。\n\n字典的三个基本操作（添加元素，获取元素和删除元素）的平均事件复杂度为O(1)，但是他们的平摊最坏情况复杂度要高得多，为O(N).\n\n还有一点很重要，在复制和遍历字典的操作中，最坏的复杂度中的n是字典曾经达到的最大元素数目，而不是当前的元素数目。换句话说，如果一个字典曾经元素个数很多，后来又大大减小了，那么遍历这个字典可能会花费相当长的事件。因此在某些情况下，如果需要频繁的遍历某个词典，那么最好创建一个新的字典对象，而不是仅在旧字典中删除元素。\n\n字典的缺点和替代方案\n\n使用字典的常见陷阱就是，它并不会按照键的添加顺序来保存元素的顺序。在某些情况下，字典的键是连续的，对应的散列值也是连续值（例如整数），那么由于字典的内部实现，元\n\n素的实现可能和添加的顺序相同：\n\n```\nkeys = {num: None for num in range(5)}.keys()\nprint(keys)\n```\n\n如果我们需要保存添加顺序怎么办？python 标准库的collections模块提供了名为OrderedDicr的有序字典\n\n# 集合\n\n集合是一种鲁棒性很好的数据结构，当元素顺序的重要性不如元素的唯一性和测试元素是否包含在集合中的效率时，大部分情况下这种数据结构极其有用。\n\npython的内置集合类型有两种：\n\nset(): 一种可变的、无序的、有限的集合，其元素是唯一的、不可变的（可哈希的）对象。 frozenset(): 一种不可变的、可哈希的、无序的集合，其元素是唯一的，不可变的哈希对象。\n\nset里的元素必须是唯一的，不可变的。但是set是可变的，所以set作为set的元素会报错。\n\n实现细节\n\nCPython中集合和字典非常相似。事实上，集合被实现为带有空值的字典，只有键才是实际的集合元素。此外，集合还利用这种没有值的映射做了其它的优化。\n\n由于这一点，可以快速的向集合中添加元素、删除元素、检查元素是否存在。平均时间复杂度为O(1),最坏的事件复杂度是O(n)。\n","tags":["python中高级面试题"]},{"title":"一篇关于react历史的流水账","url":"/2018/06/10/pout/一篇关于react历史的流水账/","content":"\nreact 目前已经更新到 V16.3，其一路走来，日臻完善，笔者接触 react 两年有余，在这里做一个阶段性的整理，也对 react 的发展和我对 react 的学习做一个整体记录。\n\n笔者是在 16 年初开始关注 react，而实际上那个时候 react 已经发布快三年了， 16 年初的我写页面还是主要使用 backbone.js、Jquery，并且认为，相比于纯粹使用 Jquery 的“刀耕火种”的时代，使用 backbone.js 已经足够方便并且不需要替代品了。\n\n这篇文章会从 react 开源之初进行讲起，直到 2018 年六月。\n\n### 为什么是 react\n\n我们知道，react 并不是一个 MVC 框架，也并没有使用传统的前端模版，而是采用了纯 JS 编写（实际上用到了 JSX ），使用了虚拟 DOM，使用 diff 来保证 DOM 的更新效率，并且可以结合 facebook 的 Flux 架构，解决传统 MVC 模式的一些痛点。\n\n在 react 开源之初，相关生态体系并不完善，甚至官方还在用`Backbone.Router`加 react 来开发单页面应用。\n\n但是那个时候的 react，和现在的 react，解决的核心问题都没有变化，那就是**复杂的UI渲染问题（ complex UI rendering ）**，所有的它的组件化，虚拟 DOM 和 diff 算法，甚至目前提出的 Fiber、async rendering等等，都是围绕这个中心。\n\n### FLUX\n\n在 2014 年五月左右，也就是距离 react 开源接近一年时间，react 公开了 FLUX 架构。当然，我们现在在学习的过程中，甚至都很难听到 FLUX 这个词汇了，更多的则是 redux 甚至 dva 等更上层的框架，但是目前绝大多数 react 相关的数据管理框架都受到了 FLUX 很大启发。\n\nFLUX 和双向数据绑定的关系，我认为这里有必要援引当初官方写的一点解释（更详细的一些信息，可以看[这篇文章](https://www.10000h.top/react_flux.pdf)）：\n\n```\nTo summarize, Flux works well for us because the single directional data flow makes it easy to understand and modify an application as it becomes more complicated. We found that two-way data bindings lead to cascading updates, where changing one data model led to another data model updating, making it very difficult to predict what would change as the result of a single user interaction.\n\n总而言之，Flux对我们来说效果很好，因为单向数据流可以让应用程序变得更加复杂，从而轻松理解和修改应用程序。我们发现双向数据绑定导致级联更新，其中更改一个数据模型导致另一个数据模型更新，使得很难预测单个用户交互的结果会发生什么变化。\n```\n\n从此之后，下面这张图便多次出现在官方博客和各个网站中，相信我们也肯定见过下图：\n\n![](https://www.10000h.top/images/flux.png)\n\n### react-router\n\n2014年8月，react-router 的雏形发布，在其发布之前，不少示例应用还在使用 backbone\n.js 的 router，而 react-router 的发布，标志着 react 生态的进一步成熟。\n\n### react ES6 Class\n\n实际上，在 2015.01.27 之前，我们都是在使用 `React.createClass`来书写组件。\n\n而在 2015.01.27 这一天，也就是第一届 `reactjs conf` 的前一天，react 官方发布了 React V0.13.0 beta 版本。这一个版本的最大更新就是支持 ES6 的 Class 写法来书写组件，同时也公布了比如 propTypes 类型检查、defaultProps、AutoBind、ref 等一系列相关工作在 ES6 Class 模式下的写法。\n\n这次发布是 react 开源至此最为重大的一次更新，也因此直接将 react 的写法进行了革新，在我看来，这标志着 react 从刀耕火种的原始时代进入了石器时代。\n\n*实际上，直到一个半月后的 03.10 ，V0.13 的正式版本才发布。*\n\n而在之后的 V15.5 版本（2017年4月发布），react 才将`React.createClass`的使用设置为 Deprecation，并且宣布会在将来移除该 API，与此同时，react 团队仍然提供了一个单独的库`create-react-class` 来支持原来的 `React.createClass` 功能。\n\n### Relay & GraphQL\n\n在 2015 年的 2月，Facebook 公布了 GraphQL，GraphQL 是一种新的数据查询解决方案，事实证明，它是非常优秀的一个解决方案，到现在已经基本在行业内人尽皆知。\n\n而 Relay 则是链接 react 和 GraphQL 的一个解决方案，有点类似 redux（但是 stat 数只有 redux 的四分之一左右），但是对 GraphQL 更为友好，并且在缓存机制的设计（按照 Graph 来 cache）、声明式的数据获取等方面，有一些自己的独到之处。\n\n当然，我们使用 redux 配合相关插件，也可以不使用 Relay。\n\n\n### React Native\n\n在第一届 React.js Conf 中，react 团队首次公开了 React Native，并且在3月份真正开源了 React Native（实际上这个时候安卓版本还并不可用），之后在2015年上半年，相关团队陆陆续续披露了关于 React Native 发展情况的更多信息。\n\n并且也是在这个时候（2015年3月），react 团队开始使用 **learn once, write anywhere** 这个如今我们耳熟能详的口号。\n\n### react & react-dom & babel\n\n在2015年七月，官方发布了React v0.14 Beta 1，这也是一个变动比较大的版本，在这个版本中，主要有如下比较大的变化:\n\n* 官方宣布废弃 react-tools 和 JSTransform，这是和 JSX 解析相关的库，而从此 react 开始使用 babel，我认为这对 react 以及其使用者来说无疑是一个利好。\n* 分离 react 和 react-dom，由于 React Native 已经迭代了一段时间，这个分离同时也意味着 react 之后的发展方向，react 本身将会关注抽象层和组件本身，而 react-dom 可以将其在浏览器中落地，React Native 可以将其在客户端中落地，之后也许还会有 react-xxx ...\n\n将 react 和 react-dom 分离之后，react 团队又对 react-dom 在 dom 方面做了较为大量的更新。\n\n### Discontinuing IE 8 Support\n\n在 react V15 的版本中，放弃了对 IE 8 的支持。\n\n\n### Fiber\n\nreact 团队使用 Fiber 架构完成了 react V16 的开发，得益于 Fiber 架构，react 的性能又得到了显著提升（尤其是在某些要求交互连续的场景下），并且包大小缩小了 32%。\n\n到目前来说，关于 Fiber 架构的中英文资料都已经相当丰富，笔者在这里就不进行过多的赘述了。\n\n### 接下来的展望\n\nreact 团队目前的主要工作集中在 async rendering 方面，这方面的改进可以极大提升用户交互体验（特别是在弱网络环境下），会在 2018 年发布。\n\n如果你对这方面的内容很感兴趣，不妨看看 react 之前的[演讲视频](https://reactjs.org/blog/2018/03/01/sneak-peek-beyond-react-16.html)\n\n### 附录1 一些你可能不知道的变化\n\n* react并非直接将 JSX 渲染成 DOM，而是对某些事件和属性做了封装（优化）。 react 对表单类型的 DOM 进行了优化，比如封装了较为通用的 onChange 回调函数，这其中需要处理不少问题，react 在 V0.4 即拥有了这一特性，可以参考[这里](https://reactjs.org/blog/2013/07/23/community-roundup-5.html#cross-browser-onchange)\n* 事实上，react 在V0.8之前，一直在以“react-tools”这个名字发布，而 npm 上面叫做 react 的实际上是另外一个包，而到 V0.8 的时候，react 团队和原来的 “react” 包开发者协商，之后 react 便接管了原来的这个包，也因此，react并没有 V0.6 和 V0.7，而是从 V0.5 直接到了 V0.8\n* react 从 V0.14 之后，就直接跳跃到了 V15，官方团队给出的理由是，react 很早就已经足够稳定并且可以使用在生产版本中，更改版本的表达方式更有助于表示 react 项目本身的稳定性。\n\n### 附录2 一些比较优秀的博客\n\n* 关于React Components, Elements, 和 Instances，如果你还有一些疑问，可以看一看React官方团队的文章：[React Components, Elements, and Instances](https://reactjs.org/blog/2015/12/18/react-components-elements-and-instances.html)\n* 如果你倾向于使用 mixins，不妨看看 react 关于取消 mixin的说法：[Mixins Considered Harmful](https://reactjs.org/blog/2016/07/13/mixins-considered-harmful.html)\n* react props 相关的开发模式的建议，我认为目前在使用 react 的程序员都应该了解一下[You Probably Don't Need Derived State](https://reactjs.org/blog/2018/06/07/you-probably-dont-need-derived-state.html)","tags":["react"]},{"title":"十条编写优化的 JavaScript 代码的建议","url":"/2018/05/29/pout/前端技术/十条编写优化的JavaScript代码的建议/","content":"\n本文总结了十条编写优秀的 JavaScript 代码的习惯，主要针对 V8 引擎：\n\n1.始终以相同的顺序实例化对象属性，以便可以共享隐藏类和随后优化的代码。V8 在对 js 代码解析的时候会有构建隐藏类的过程，以相同的顺序实例化（属性赋值）的对象会共享相同的隐藏类。下面给出一个不好的实践：\n\n```javascript\nfunction Point(x, y) {\n    this.x = x;\n    this.y = y;\n}\nvar p1 = new Point(1, 2);\np1.a = 5;\np1.b = 6;\nvar p2 = new Point(3, 4);\np2.b = 7;\np2.a = 8;\n// 由于 a 和 b 的赋值顺序不同，p1 和 p2 无法共享隐藏类\n```\n\n2.避免分配动态属性。在实例化之后向对象添加属性将强制隐藏类更改，并减慢为先前隐藏类优化的所有方法。相反，在其构造函数中分配所有对象的属性。  \n\n3.重复执行相同方法的代码将比仅执行一次（由于内联缓存）执行许多不同方法的代码运行得更快。  \n\n4.避免创建稀疏数组。稀疏数组由于不是所有的元素都存在，因此是一个哈希表，因此访问稀疏数组中的元素代价更高。另外，尽量不要采用预分配数量的大数组，更好的办法是随着你的需要把它的容量增大。最后，尽量不要删除数组中的元素，它会让数组变得稀疏。  \n\n5.标记值：V8采用32位来表示对象和数字，其中用一位来区别对象（flag = 0）或数字（flag = 1），因此这被称之为 SMI (Small Integer)因为它只有31位。因此，如果一个数字大于31位，V8需要对其进行包装，将其变成双精度并且用一个对象来封装它，因此应该尽量使用31位有符号数字从而避免昂贵的封装操作。  \n\n6.检查你的依赖，去掉不需要 import 的内容。  \n\n7.将你的代码分割成一些小的 chunks ，而不是整个引入。 \n \n8.尽可能使用 defer 来推迟加载 JavaScript，另外只加载当前路由需要的代码段。\n  \n9.使用 dev tools 和 DeviceTiming 来寻找代码瓶颈。  \n\n10.使用诸如Optimize.js这样的工具来帮助解析器决定何时需要提前解析以及何时需要延后解析。  \n  \n以上内容来源：\n* [How JavaScript works: Parsing, Abstract Syntax Trees (ASTs) + 5 tips on how to minimize parse time](https://blog.sessionstack.com/how-javascript-works-parsing-abstract-syntax-trees-asts-5-tips-on-how-to-minimize-parse-time-abfcf7e8a0c8)\n* [How JavaScript works: inside the V8 engine + 5 tips on how to write optimized code](https://blog.sessionstack.com/how-javascript-works-inside-the-v8-engine-5-tips-on-how-to-write-optimized-code-ac089e62b12e)\n\n","tags":["javascript"]},{"title":"浅谈前端中的二进制数据类型","url":"/2018/05/09/pout/前端技术/浅谈前端中的二进制数据类型/","content":"\n>目前在一个项目中，WebSocket部分由于后端使用了gzip压缩，前端处理起来废了一点时间，从而发现自己在二进制数据类型这个知识点还存在一定的盲区，因此这里进行总结。\n\n本文主要简单介绍ArrayBuffer对象、TypedArray对象、DataView对象以及Blob原始数据类型，和它们之间的互相转换方法。部分代码参考[这里](http://javascript.ruanyifeng.com/stdlib/arraybuffer.html#toc4)而非本人原创，仅做个人学习使用。\n\n这些类型化对象，一般会在以下场景中使用：\n\n* WebGL 中，浏览器和显卡之间需要使用二进制数据进行通信。\n* 在一些 Rest 接口或者 WebSocket 中，采用压缩过的数据进行通信，这个压缩和解压缩的过程可能需要借助二进制对象。\n* 在 Canvas 中，我们可能需要通过生成 Blob 的方式保存当前内容。\n* 在 Img 等资源文件中，URL 可以为 Blob 原始数据类型。\n* 在读取用户上传文件时，可能需要用到二进制数据类型进行中间转换。\n\n下文分两部分，前一部分概述各个二进制数据类型，后一部分将它们之间的互相转换。\n\n### 二进制数据类型概述\n\n#### ArrayBuffer\n\nArrayBuffer对象代表储存二进制数据的一段内存，它不能直接读写，只能通过视图（TypedArray视图和DataView视图)来读写，视图的作用是以指定格式解读二进制数据。\n\nArrayBuffer也是一个构造函数，可以分配一段可以存放数据的连续内存区域。\n\n```\nvar buf = new ArrayBuffer(32);\n```\n\n上面代码生成了一段32字节的内存区域，每个字节的值默认都是0。可以看到，ArrayBuffer构造函数的参数是所需要的内存大小（单位字节）。\n\n为了读写这段内容，需要为它指定视图。DataView视图的创建，需要提供ArrayBuffer对象实例作为参数。\n\n```\nvar buf = new ArrayBuffer(32);\nvar dataView = new DataView(buf);\ndataView.getUint8(0) // 0\n```\n\n上面代码对一段32字节的内存，建立DataView视图，然后以不带符号的8位整数格式，读取第一个元素，结果得到0，因为原始内存的ArrayBuffer对象，默认所有位都是0。\n\n另外，我们可以将ArrayBuffer生成的结果，传入TypedArray中：\n\n```\nvar buffer = new ArrayBuffer(12);\n\nvar x1 = new Int32Array(buffer);\nx1[0] = 1;\nvar x2 = new Uint8Array(buffer);\nx2[0]  = 2;\n\nx1[0] // 2\n```\n\nArrayBuffer实例的byteLength属性，返回所分配的内存区域的字节长度。\n\n```\nvar buffer = new ArrayBuffer(32);\nbuffer.byteLength\n// 32\n```\n如果要分配的内存区域很大，有可能分配失败（因为没有那么多的连续空余内存），所以有必要检查是否分配成功。\n\n```\nif (buffer.byteLength === n) {\n  // 成功\n} else {\n  // 失败\n}\n```\n\nArrayBuffer实例有一个slice方法，允许将内存区域的一部分，拷贝生成一个新的ArrayBuffer对象。\n\n```\nvar buffer = new ArrayBuffer(8);\nvar newBuffer = buffer.slice(0, 3);\n```\n\n上面代码拷贝buffer对象的前3个字节（从0开始，到第3个字节前面结束），生成一个新的ArrayBuffer对象。slice方法其实包含两步，第一步是先分配一段新内存，第二步是将原来那个ArrayBuffer对象拷贝过去。\n\nslice方法接受两个参数，第一个参数表示拷贝开始的字节序号（含该字节），第二个参数表示拷贝截止的字节序号（不含该字节）。如果省略第二个参数，则默认到原ArrayBuffer对象的结尾。\n\n除了slice方法，ArrayBuffer对象不提供任何直接读写内存的方法，只允许在其上方建立视图，然后通过视图读写。\n\nArrayBuffer有一个静态方法isView，返回一个布尔值，表示参数是否为ArrayBuffer的视图实例。这个方法大致相当于判断参数，是否为TypedArray实例或DataView实例。\n\n```\nvar buffer = new ArrayBuffer(8);\nArrayBuffer.isView(buffer) // false\n\nvar v = new Int32Array(buffer);\nArrayBuffer.isView(v) // true\n```\n\n#### TypedArray\n\n目前，TypedArray对象一共提供9种类型的视图，每一种视图都是一种构造函数。\n\n* Int8Array：8位有符号整数，长度1个字节。\n* Uint8Array：8位无符号整数，长度1个字节。\n* Uint8ClampedArray：8位无符号整数，长度1个字节，溢出处理不同。\n* Int16Array：16位有符号整数，长度2个字节。\n* Uint16Array：16位无符号整数，长度2个字节。\n* Int32Array：32位有符号整数，长度4个字节。\n* Uint32Array：32位无符号整数，长度4个字节。\n* Float32Array：32位浮点数，长度4个字节。\n* Float64Array：64位浮点数，长度8个字节。\n\n这9个构造函数生成的对象，统称为TypedArray对象。它们很像正常数组，都有length属性，都能用方括号运算符（[]）获取单个元素，所有数组的方法，在类型化数组上面都能使用。两者的差异主要在以下方面。\n\n* TypedArray数组的所有成员，都是同一种类型和格式。\n* TypedArray数组的成员是连续的，不会有空位。\n* Typed化数组成员的默认值为0。比如，new Array(10)返回一个正常数组，里面没有任何成员，只是10个空位；new Uint8Array(10)返回一个类型化数组，里面10个成员都是0。\n* TypedArray数组只是一层视图，本身不储存数据，它的数据都储存在底层的ArrayBuffer对象之中，要获取底层对象必须使用buffer属性。\n\n##### 构造函数\n\nTypedArray数组提供9种构造函数，用来生成相应类型的数组实例。\n\n构造函数有多种用法。\n\n* TypedArray(buffer, byteOffset=0, length?)\n\n同一个ArrayBuffer对象之上，可以根据不同的数据类型，建立多个视图。\n\n```\n// 创建一个8字节的ArrayBuffer\nvar b = new ArrayBuffer(8);\n\n// 创建一个指向b的Int32视图，开始于字节0，直到缓冲区的末尾\nvar v1 = new Int32Array(b);\n\n// 创建一个指向b的Uint8视图，开始于字节2，直到缓冲区的末尾\nvar v2 = new Uint8Array(b, 2);\n\n// 创建一个指向b的Int16视图，开始于字节2，长度为2\nvar v3 = new Int16Array(b, 2, 2);\n```\n\n对于以上代码，v1、v2和v3是重叠的：v1[0]是一个32位整数，指向字节0～字节3；v2[0]是一个8位无符号整数，指向字节2；v3[0]是一个16位整数，指向字节2～字节3。只要任何一个视图对内存有所修改，就会在另外两个视图上反应出来。\n\n注意，byteOffset必须与所要建立的数据类型一致，否则会报错。\n\n```\nvar buffer = new ArrayBuffer(8);\nvar i16 = new Int16Array(buffer, 1);\n// Uncaught RangeError: start offset of Int16Array should be a multiple of 2\n```\n\n上面代码中，新生成一个8个字节的ArrayBuffer对象，然后在这个对象的第一个字节，建立带符号的16位整数视图，结果报错。因为，带符号的16位整数需要两个字节，所以byteOffset参数必须能够被2整除。\n\n如果想从任意字节开始解读ArrayBuffer对象，必须使用DataView视图，因为TypedArray视图只提供9种固定的解读格式。\n\n* TypedArray(length)\n\n视图还可以不通过ArrayBuffer对象，直接分配内存而生成。\n\n```\nvar f64a = new Float64Array(8);\nf64a[0] = 10;\nf64a[1] = 20;\nf64a[2] = f64a[0] + f64a[1];\n```\n\n* TypedArray(typedArray)\n\n类型化数组的构造函数，可以接受另一个视图实例作为参数。\n\n```\nvar typedArray = new Int8Array(new Uint8Array(4));\n```\n\n上面代码中，Int8Array构造函数接受一个Uint8Array实例作为参数。\n\n注意，此时生成的新数组，只是复制了参数数组的值，对应的底层内存是不一样的。新数组会开辟一段新的内存储存数据，不会在原数组的内存之上建立视图。\n\n```\nvar x = new Int8Array([1, 1]);\nvar y = new Int8Array(x);\nx[0] // 1\ny[0] // 1\n\nx[0] = 2;\ny[0] // 1\n```\n\n上面代码中，数组y是以数组x为模板而生成的，当x变动的时候，y并没有变动。\n\n如果想基于同一段内存，构造不同的视图，可以采用下面的写法。\n\n```\nvar x = new Int8Array([1, 1]);\nvar y = new Int8Array(x.buffer);\nx[0] // 1\ny[0] // 1\n\nx[0] = 2;\ny[0] // 2\n```\n\n* TypedArray(arrayLikeObject)\n\n构造函数的参数也可以是一个普通数组，然后直接生成TypedArray实例。\n\n```\nvar typedArray = new Uint8Array([1, 2, 3, 4]);\n```\n\n注意，这时TypedArray视图会重新开辟内存，不会在原数组的内存上建立视图。\n\n上面代码从一个普通的数组，生成一个8位无符号整数的TypedArray实例。\n\nTypedArray数组也可以转换回普通数组。\n\n```\nvar normalArray = Array.prototype.slice.call(typedArray);\n```\n\n##### BYTES_PER_ELEMENT属性\n\n每一种视图的构造函数，都有一个BYTES_PER_ELEMENT属性，表示这种数据类型占据的字节数。\n\n```\nInt8Array.BYTES_PER_ELEMENT // 1\nUint8Array.BYTES_PER_ELEMENT // 1\nInt16Array.BYTES_PER_ELEMENT // 2\nUint16Array.BYTES_PER_ELEMENT // 2\nInt32Array.BYTES_PER_ELEMENT // 4\nUint32Array.BYTES_PER_ELEMENT // 4\nFloat32Array.BYTES_PER_ELEMENT // 4\nFloat64Array.BYTES_PER_ELEMENT // 8\n```\n\n##### ArrayBuffer与字符串的互相转换\n\nArrayBuffer转为字符串，或者字符串转为ArrayBuffer，有一个前提，即字符串的编码方法是确定的。假定字符串采用UTF-16编码（JavaScript的内部编码方式），可以自己编写转换函数。\n\n```\n// ArrayBuffer转为字符串，参数为ArrayBuffer对象\nfunction ab2str(buf) {\n  return String.fromCharCode.apply(null, new Uint16Array(buf));\n}\n\n// 字符串转为ArrayBuffer对象，参数为字符串\nfunction str2ab(str) {\n  var buf = new ArrayBuffer(str.length * 2); // 每个字符占用2个字节\n  var bufView = new Uint16Array(buf);\n  for (var i = 0, strLen = str.length; i < strLen; i++) {\n    bufView[i] = str.charCodeAt(i);\n  }\n  return buf;\n}\n```\n\n##### TypedArray.prototype.set()\n\nTypedArray数组的set方法用于复制数组（正常数组或TypedArray数组），也就是将一段内容完全复制到另一段内存。\n\n```\nvar a = new Uint8Array(8);\nvar b = new Uint8Array(8);\n\nb.set(a);\n```\n\n上面代码复制a数组的内容到b数组，它是整段内存的复制，比一个个拷贝成员的那种复制快得多。set方法还可以接受第二个参数，表示从b对象哪一个成员开始复制a对象。\n\n```\nvar a = new Uint16Array(8);\nvar b = new Uint16Array(10);\n\nb.set(a, 2)\n```\n上面代码的b数组比a数组多两个成员，所以从b[2]开始复制。\n\n##### TypedArray.prototype.subarray()\n\nsubarray方法是对于TypedArray数组的一部分，再建立一个新的视图。\n\n```\nvar a = new Uint16Array(8);\nvar b = a.subarray(2,3);\n\na.byteLength // 16\nb.byteLength // 2\n```\n\nsubarray方法的第一个参数是起始的成员序号，第二个参数是结束的成员序号（不含该成员），如果省略则包含剩余的全部成员。所以，上面代码的a.subarray(2,3)，意味着b只包含a[2]一个成员，字节长度为2。\n\n##### TypedArray.prototype.slice()\n\nTypeArray实例的slice方法，可以返回一个指定位置的新的TypedArray实例。\n\n```\nlet ui8 = Uint8Array.of(0, 1, 2);\nui8.slice(-1)\n// Uint8Array [ 2 ]\n```\n\n\n上面代码中，ui8是8位无符号整数数组视图的一个实例。它的slice方法可以从当前视图之中，返回一个新的视图实例。\n\nslice方法的参数，表示原数组的具体位置，开始生成新数组。负值表示逆向的位置，即-1为倒数第一个位置，-2表示倒数第二个位置，以此类推。\n\n##### TypedArray.of()\n\nTypedArray数组的所有构造函数，都有一个静态方法of，用于将参数转为一个TypedArray实例。\n\n```\nFloat32Array.of(0.151, -8, 3.7)\n// Float32Array [ 0.151, -8, 3.7 ]\n```\n\n##### TypedArray.from()\n\n静态方法from接受一个**可遍历的数据结构（比如数组）**作为参数，返回一个基于这个结构的TypedArray实例。\n\n```\nUint16Array.from([0, 1, 2])\n// Uint16Array [ 0, 1, 2 ]\n```\n\n这个方法还可以将一种TypedArray实例，转为另一种。\n\n```\nvar ui16 = Uint16Array.from(Uint8Array.of(0, 1, 2));\nui16 instanceof Uint16Array // true\n```\n\nfrom方法还可以接受一个函数，作为第二个参数，用来对每个元素进行遍历，功能类似map方法。\n\n```\nInt8Array.of(127, 126, 125).map(x => 2 * x)\n// Int8Array [ -2, -4, -6 ]\n\nInt16Array.from(Int8Array.of(127, 126, 125), x => 2 * x)\n// Int16Array [ 254, 252, 250 ]\n```\n\n上面的例子中，from方法没有发生溢出，这说明遍历是针对新生成的16位整数数组，而不是针对原来的8位整数数组。也就是说，from会将第一个参数指定的TypedArray数组，拷贝到另一段内存之中（占用内存从3字节变为6字节），然后再进行处理。\n\n#### DataView\n\n如果一段数据包括多种类型（比如服务器传来的HTTP数据），这时除了建立ArrayBuffer对象的复合视图以外，还可以通过DataView视图进行操作。\n\nDataView视图提供更多操作选项，而且支持设定字节序。本来，在设计目的上，ArrayBuffer对象的各种TypedArray视图，是用来向网卡、声卡之类的本机设备传送数据，所以使用本机的字节序就可以了；而DataView视图的设计目的，是用来处理网络设备传来的数据，所以大端字节序或小端字节序是可以自行设定的。\n\nDataView视图本身也是构造函数，接受一个ArrayBuffer对象作为参数，生成视图。\n\n```\nDataView(ArrayBuffer buffer [, 字节起始位置 [, 长度]]);\n```\n下面是一个例子。\n\n```\nvar buffer = new ArrayBuffer(24);\nvar dv = new DataView(buffer);\n```\n\nDataView实例有以下属性，含义与TypedArray实例的同名方法相同。\n\n* DataView.prototype.buffer：返回对应的ArrayBuffer对象\n* DataView.prototype.byteLength：返回占据的内存字节长度\n* DataView.prototype.byteOffset：返回当前视图从对应的ArrayBuffer对象的哪个字节开始\n\nDataView实例提供8个方法读取内存。\n\n* getInt8：读取1个字节，返回一个8位整数。\n* getUint8：读取1个字节，返回一个无符号的8位整数。\n* getInt16：读取2个字节，返回一个16位整数。\n* getUint16：读取2个字节，返回一个无符号的16位整数。\n* getInt32：读取4个字节，返回一个32位整数。\n* getUint32：读取4个字节，返回一个无符号的32位整数。\n* getFloat32：读取4个字节，返回一个32位浮点数。\n* getFloat64：读取8个字节，返回一个64位浮点数。\n\n这一系列get方法的参数都是一个字节序号（不能是负数，否则会报错），表示从哪个字节开始读取。\n\n```\nvar buffer = new ArrayBuffer(24);\nvar dv = new DataView(buffer);\n\n// 从第1个字节读取一个8位无符号整数\nvar v1 = dv.getUint8(0);\n\n// 从第2个字节读取一个16位无符号整数\nvar v2 = dv.getUint16(1);\n\n// 从第4个字节读取一个16位无符号整数\nvar v3 = dv.getUint16(3);\n```\n\n上面代码读取了ArrayBuffer对象的前5个字节，其中有一个8位整数和两个十六位整数。\n\n如果一次读取两个或两个以上字节，就必须明确数据的存储方式，到底是小端字节序还是大端字节序。默认情况下，DataView的get方法使用大端字节序解读数据，如果需要使用小端字节序解读，必须在get方法的第二个参数指定true。\n\n```\n// 小端字节序\nvar v1 = dv.getUint16(1, true);\n\n// 大端字节序\nvar v2 = dv.getUint16(3, false);\n\n// 大端字节序\nvar v3 = dv.getUint16(3);\n```\n\nDataView视图提供8个方法写入内存。\n\n* setInt8：写入1个字节的8位整数。\n* setUint8：写入1个字节的8位无符号整数。\n* setInt16：写入2个字节的16位整数。\n* setUint16：写入2个字节的16位无符号整数。\n* setInt32：写入4个字节的32位整数。\n* setUint32：写入4个字节的32位无符号整数。\n* setFloat32：写入4个字节的32位浮点数。\n* setFloat64：写入8个字节的64位浮点数。\n\n这一系列set方法，接受两个参数，第一个参数是字节序号，表示从哪个字节开始写入，第二个参数为写入的数据。对于那些写入两个或两个以上字节的方法，需要指定第三个参数，false或者undefined表示使用大端字节序写入，true表示使用小端字节序写入。\n\n```\n// 在第1个字节，以大端字节序写入值为25的32位整数\ndv.setInt32(0, 25, false);\n\n// 在第5个字节，以大端字节序写入值为25的32位整数\ndv.setInt32(4, 25);\n\n// 在第9个字节，以小端字节序写入值为2.5的32位浮点数\ndv.setFloat32(8, 2.5, true);\n```\n\n如果不确定正在使用的计算机的字节序，可以采用下面的判断方式。\n\n```\nvar littleEndian = (function() {\n  var buffer = new ArrayBuffer(2);\n  new DataView(buffer).setInt16(0, 256, true);\n  return new Int16Array(buffer)[0] === 256;\n})();\n```\n\n#### Blob\n\nBlob 对象表示一个不可变、原始数据的类文件对象。Blob 表示的不一定是JavaScript原生格式的数据。File 接口基于Blob，继承了 blob 的功能并将其扩展使其支持用户系统上的文件。\n\n要从其他非blob对象和数据构造一个Blob，请使用 Blob() 构造函数。要创建包含另一个blob数据的子集blob，请使用 slice()方法。要获取用户文件系统上的文件对应的Blob对象，请参阅 File文档。\n\n从Blob中读取内容的唯一方法是使用 FileReader。以下代码将 Blob 的内容作为类型数组读取：\n\n```\nvar reader = new FileReader();\nreader.addEventListener(\"loadend\", function() {\n   // reader.result 包含转化为类型数组的blob\n});\nreader.readAsArrayBuffer(blob);\n```\n\n更多关于Blob的内容，请直接查看[这里](https://developer.mozilla.org/zh-CN/docs/Web/API/Blob)\n\n### 数据格式转换\n\n#### String转Blob\n\n```\n//将字符串 转换成 Blob 对象\nvar blob = new Blob([\"Hello World!\"], {\n    type: 'text/plain'\n});\nconsole.info(blob);\nconsole.info(blob.slice(1, 3, 'text/plain'));\n```\n#### TypeArray转Blob\n\n```\n//将 TypeArray  转换成 Blob 对象\nvar array = new Uint16Array([97, 32, 72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33]);\n//测试成功\n//var blob = new Blob([array], { type: \"application/octet-binary\" });\n//测试成功， 注意必须[]的包裹\nvar blob = new Blob([array]);\n//将 Blob对象 读成字符串\nvar reader = new FileReader();\nreader.readAsText(blob, 'utf-8');\nreader.onload = function (e) {\n    console.info(reader.result); //a Hello world!\n}\n```\n\n#### ArrayBuffer转Blob\n\n```\nvar buffer = new ArrayBuffer(32);\nvar blob = new Blob([buffer]);       // 注意必须包裹[]\n```\n\n#### Blob转String\n\n这里需要注意的是readAsText方法的使用。\n\n```\n//将字符串转换成 Blob对象\nvar blob = new Blob(['中文字符串'], {\n    type: 'text/plain'\n});\n//将Blob 对象转换成字符串\nvar reader = new FileReader();\nreader.readAsText(blob, 'utf-8');\nreader.onload = function (e) {\n    console.info(reader.result);\n}\n```\n\n#### Blob转ArrayBuffer\n\n这里需要注意的是readAsArrayBuffer方法的使用。\n\n```\n//将字符串转换成 Blob对象\nvar blob = new Blob(['中文字符串'], {\n    type: 'text/plain'\n});\n//将Blob 对象转换成 ArrayBuffer\nvar reader = new FileReader();\nreader.readAsArrayBuffer(blob);\nreader.onload = function (e) {\n    console.info(reader.result); //ArrayBuffer {}\n    //经常会遇到的异常 Uncaught RangeError: byte length of Int16Array should be a multiple of 2\n    //var buf = new int16array(reader.result);\n    //console.info(buf);\n\n    //将 ArrayBufferView  转换成Blob\n    var buf = new Uint8Array(reader.result);\n    console.info(buf); //[228, 184, 173, 230, 150, 135, 229, 173, 151, 231, 172, 166, 228, 184, 178]\n    reader.readAsText(new Blob([buf]), 'utf-8');\n    reader.onload = function () {\n        console.info(reader.result); //中文字符串\n    };\n\n    //将 ArrayBufferView  转换成Blob\n    var buf = new DataView(reader.result);\n    console.info(buf); //DataView {}\n    reader.readAsText(new Blob([buf]), 'utf-8');\n    reader.onload = function () {\n        console.info(reader.result); //中文字符串\n    };\n}\n```\n\n","tags":["javascript"]},{"title":"Supervisor安装及配置","url":"/2018/05/05/pout/运维/Supervisor安装与配置/","content":"\n\n<!-- toc -->\n\n\n# 在阿里云Centos7.6上部署主管来监控和操作类别服务\n\n\n\n​    Supervisor是用Python开发的一个客户端/服务器服务，是Linux / Unix系统下的一个进程管理工具，不支持Windows系统。它可以很方便的监听，启动，停止，重启一个或多个进程。的进程，当一个进程意外被杀死，supervisort监听到进程死后，会自动将它重新拉起，很方便的做到进程自动恢复的功能，不再需要自己写shell脚本来控制。\n\n​    说白了，它真正有用的功能是俩个将非daemon（守护进程）程序变成deamon方式运行对程序进行监控，当程序退出时，可以自动拉起程序。\n\n​    但是它无法控制本身就是daemon的服务。\n\n​    \n\n​    安装主管\n\n​    \n\n```\n百胜安装EPEL - 释放\nyum install - y主管\n```\n\n生成配置文件\n\n```\nsupervisord - ç / 等/ supervisord 。conf\n```\n\n然后修改配置文件vim /etc/supervisord.conf\n\n将网络服务打开，需要注意ip地址要写*，否则外网访问不了，而用户名和密码就是登录服务页面的用户名和密码，可以改的复杂一点，另外阿里云也需要大量网暴露一下9001端口\n\n```\n[ inet_http_server ] ; 默认情况下，Inet （TCP ）服务器已禁用         \n端口= *：9001 ; （IP_ADDRESS ：端口说明符，*：端口用于所有IFACE ）\n的用户名= 用户               ; （默认是没有用户名（打开服务器））\n密码= 123 ; （默认为无密码（打开服务器））                           \n```\n\n然后添加uwsgi服务的配置\n\n```\n[ 程序：mypro ] \n命令= uwsgi / USR / 本地/ 仓/ uwsgi - INI / 根/ js_back / js_back_uwsgi 。ini ; 启动命令，可以抛光与手动在命令行启动的命令是一样的\nautostart = false      ; 在supervisord 启动的时候也自动启动\nstopsignal = QUIT      \n用户= 根\nstartsecs = 5 ; 启动5 秒后没有异常退出，就当作已经正常启动了\nstartretries = 3 ; 启动失败自动重试次数，默认是3 \n自动重= 真    ; 程序异常退出后自动重启\n了redirect_stderr = 真   ; 把标准错误重定向到stdout ，默认为false                 \nstdout_logfile_maxbytes = 20MB ; stdout 日志文件大小，\n大约50 MB stdout_logfile = / root / js_back_uwsgi 。日志     \nstderr_logfile = / root / js_back_err 。日志 \n```\n\n最后我们知道，Supervisord只能控制非守护进程，而uwsgi本身就已经成为守护进程的配置，所以需要修改项目的uwsgi配置，将守护进程配置注掉vim js_back_uwsgi.ini\n\n```\n[ uwsgi ]\n\nchdir            = / 根/ js_back \n模块           = js_back 。gi\n主           = 真\n进程        = 3 \n套接字             = 0.0 。0.0 ：8001 \n真空           = 真  \npythonpath       = / usr / bin中/ python3 \npidfile = / 根/ js_back / js_back 。pid\n ＃注释掉daemonize模式，因为Supervisor无法控制守护进程服务#daemonize = /root/js_back/uwsgi.log \n```\n\n最后启动服务\n\n```\nsupervisord - ç / 等/ supervisord 。conf\n```\n\n \n\n如果想杀死服务可以输入命令，因为supervisor是基于python2的，那么不用担心python3的进程\n\n```\nkillall - s INT / usr / bin中/ python\n```\n\n最后访问服务管理界面http：// ip：9001，就可以管理你服务器上的服务啦\n\n![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190517/20190517100427_43553.png)\n\n","tags":["运维"]},{"title":"Linux服务器初始化设置用户和ssh公私钥登陆","url":"/2018/04/11/pout/Linux/Linux服务器初始化设置用户和ssh公私钥登陆/","content":"\n>当我们开始使用一个新的服务器的时候，首先一定要对服务器的登陆等做一些修改工作，笔者曾经就因为对服务器登陆安全没有重视，导致服务器数据全部丢失。接下来我们按照步骤，罗列出应该做的一些事情。\n\n### 修改ssh端口号\n\n第一件事情：\n\n修改ssh端口号： 之后加上一个端口比如说50000\n\n`vi /etc/ssh/sshd_config`之后在port字段加上一个端口比如说50000，原来的端口号字段可能是被注释掉的，要先解除注释。\n\n然后执行：\n\n```\nservice sshd restart\n```\n\n这个时候可能还要重新配置一下防火墙，开放50000端口，具体如何配置也可以参考[这里](https://blog.csdn.net/ul646691993/article/details/52104082)的后半部分。但是目前，阿里云的服务器实测是不需要再配置防火墙的，但是需要去登陆到网页后台修改安全组。\n\n之后就可以通过这样的方式登录了：(注意登录方式一定要写对)\n\n```shell\nssh root@115.29.102.81 -p 50000\n```\n\n### 创建用户\n\n这个时候我们还是用root进行操作，所以我们接下来要给自己创建一个账户，比如创建一个如下的用户：\n\n```\nuseradd xiaotao\npasswd xiaotao\n```\n\n可以用`ls -al /home/``查看一下账户\n\n对创建的这个用户增加sudo权限： 相关配置文件/etc/sudoers中，但是这个文件是只读的，所以要更改一下权限\n\n```\nchmod u+w sudoers\n```\n\n然后进入这个文件在这里进行更改：\n\n```\nroot    ALL=(ALL)       ALL\nxiaotao  ALL=(ALL)       ALL\n```\n\n然后再改回权限：\n\n```\nchmod u-w sudoers\n```\n\n注意一点，CentOS 7预设容许任何帐号透过ssh登入（也就是说自己根本不用改改，直接新建帐号登录即可），包括根和一般帐号，为了不受根帐号被黑客暴力入侵，我们必须禁止 root帐号的ssh功能，事实上root也没有必要ssh登入伺服器，因为只要使用su或sudo（当然需要输入root的密码）普通帐号便可以拥有root的权限。使用vim（或任何文本编辑器）开启的/ etc/ SSH/ sshd_config中，寻找：\n\n```\n＃PermitRootLogin yes\n```\n修改：\n\n```\nPermitRootLogin no\n```\n\n### 配置公私钥加密登录\n\n**这一步骤要切换到自己新建的用户，不能再用 root 用户了，否则可能无法正常登陆。**\n\n很多时候以上所说的还是不够安全，为了更加安全方便，我们采用公私钥对称加密登录，简单的讲做法就是再客户端生成一把私钥一把公钥，私钥是在客户端的，公钥上传到服务端，对称加密进行登录。\n\n在客户端先进到这个目录：\n\n```\ncd ~/.ssh\n```\n\n生成公钥和私钥（实际上如果之前有的话就不用重新生成了）\n\n```\nssh-keygen -t rsa\n```\n\n接下来把公钥上传到服务端\n\n```\nscp ~/.ssh/id_rsa.pub xiaotao@<ssh_server_ip>:~\n```\n\n在服务端执行以下命令(如果没有相关的文件和文件夹要先进行创建，注意不要使用 sudo )\n\n```\ncat  id_rsa.pub >> ～/.ssh/authorized_keys\n```\n\n配置服务器的/etc/ssh/sshd_config，下面是一些建议的配置：\n\n```\nvim /etc/ssh/sshd_config\n# 禁用root账户登录，非必要，但为了安全性，请配置\nPermitRootLogin no\n\n# 是否让 sshd 去检查用户家目录或相关档案的权限数据，\n# 这是为了担心使用者将某些重要档案的权限设错，可能会导致一些问题所致。\n# 例如使用者的 ~.ssh/ 权限设错时，某些特殊情况下会不许用户登入\nStrictModes no\n\n# 是否允许用户自行使用成对的密钥系统进行登入行为，仅针对 version 2。\n# 至于自制的公钥数据就放置于用户家目录下的 .ssh/authorized_keys 内\nRSAAuthentication yes\nPubkeyAuthentication yes\nAuthorizedKeysFile      %h/.ssh/authorized_keys\n\n#有了证书登录了，就禁用密码登录吧，安全要紧\nPasswordAuthentication no\n```\n\n然后不要忘记 `sudo service sshd restart`\n\n\n一般来讲，这样就算是成功了，我们可以在客户端尝试：\n\n```\nssh -i ~/.ssh/id_rsa remote_username@remote_ip\n```\n\n如果不行，可能是服务端或客户端相关 `.ssh` 文件权限不对，可以进行如下尝试：\n\n```\n服务端\nchown -R 0700  ~/.ssh\nchown -R 0644  ~/.ssh/authorized_keys\n\n客户端改一下\nchmod 600 id_rsa\n```","tags":["ssh"]},{"title":"mysql常规操作及查看运行中的sql","url":"/2018/04/11/pout/数据库/mysql/mysql常规操作及查看运行中的sql/","content":"\n# MYSQL常用命令  \n\n###### 1.导出整个数据库  \n\n​\tmysqldump -u 用户名 -p –default-character-set=latin1 数据库名 > 导出的文件名(数据库默认编码是latin1)  \n\n​\tmysqldump -u wcnc -p smgp_apps_wcnc > wcnc.sql  \n\n###### 2.导出一个表  \n\n​\tmysqldump -u 用户名 -p 数据库名 表名> 导出的文件名  \n\n​\tmysqldump -u wcnc -p smgp_apps_wcnc users> wcnc_users.sql  \n\n###### 3.导出一个数据库结构  \n\n​\tmysqldump -u wcnc -p -d –add-drop-table smgp_apps_wcnc >d:wcnc_db.sql  \n\n​\t-d 没有数据 –add-drop-table 在每个create语句之前增加一个drop table  \n\n###### 4.导入数据库  \n\n​\tA:常用source 命令  \n\n​\t\t进入mysql数据库控制台，  \n\n​\t\t如mysql -u root -p  \n\n​\t\tmysql>use 数据库  \n\n​\t\t然后使用source命令，后面参数为脚本文件(如这里用到的.sql)  \n\n​\t\tmysql>source wcnc_db.sql  \n\n​\tB:使用mysqldump命令  \n\n​\t\tmysqldump -u username -p dbname < filename.sql  \n\n​\tC:使用mysql命令  \n\n​\t\tmysql -u username -p -D dbname < filename.sql  \n\n### 一、启动与退出  \n\n​\t1、进入MySQL：启动MySQL Command Line Client（MySQL的DOS界面），直接输入安装时的密码即可。此时的提示符是：mysql>  \n\n​\t2、退出MySQL：quit或exit  \n\n### 二、库操作  \n\n###### 1.创建数据库  \n\n命令：create database <数据库名>  \n\n​\t例如：建立一个名为xhkdb的数据库  \n\n​\t\tmysql> create database xhkdb;  \n\n###### 2、显示所有数据库  \n\n命令：show databases （注意：最后有个s）  \n\n​\tmysql> show databases;  \n\n###### 3、删除数据库  \n\n命令：drop database <数据库名>  \n\n​\t例如：删除名为 xhkdb的数据库  \n\n​\t\tmysql> drop database xhkdb;  \n\n###### 4、连接数据库  \n\n命令：use <数据库名>  \n\n​\t例如：如果xhkdb数据库存在，尝试存取它：  \n\n​\t\tmysql> use xhkdb;  \n\n​\t\t屏幕提示：Database changed  \n\n###### 5、查看当前使用的数据库  \n\n​\tmysql> select database();  \n\n###### 6、当前数据库包含的表信息：  \n\n​\tmysql> show tables; （注意：最后有个s）  \n\n### 三、表操作，操作之前应连接某个数据库  \n\n###### 1、建表  \n\n​\t命令：create table <表名> ( <字段名> <类型> [,..<字段名n> <类型n>]);  \n\n​\t\tmysql> create table MyClass(  \n\n​\t\t\t\\> id int(4) not null primary key auto_increment,  \n\n​\t\t\t> name char(20) not null,  \n\n​\t\t\t> sex int(4) not null default ’′,  \n\n​\t\t\t\\> degree double(16,2));  \n\n###### 2、获取表结构  \n\n​\t命令：desc 表名，或者show columns from 表名  \n\n​\t\tmysql>DESCRIBE MyClass  \n\n​\t\t\tmysql> desc MyClass;  \n\n​\t\t\tmysql> show columns from MyClass;  \n\n###### 3、删除表  \n\n​\t命令：drop table <表名>  \n\n​\t\t例如：删除表名为 MyClass 的表  \n\n​\t\t\tmysql> drop table MyClass;  \n\n###### 4、插入数据  \n\n​\t命令：insert into <表名> [( <字段名>[,..<字段名n > ])] values ( 值 )[, ( 值n )]  \n\n​\t\t例如，往表 MyClass中插入二条记录, 这二条记录表示：编号为的名为Tom的成绩\t\t\t\t\t\t     \t为.45, 编\t号为 的名为Joan 的成绩为.99，编号为 的名为Wang 的成绩为.5.  \n\n​\t\t\tmysql> insert into MyClass values(1,’Tom’,96.45),(2,’Joan’,82.99), (2,’Wang’, \t\t\t96.59);  \n\n###### 5、查询表中的数据  \n\n​\t1)、查询所有行  \n\n​\t\t命令：select <字段，字段，…> from < 表名 > where < 表达式 >  \n\n​\t\t例如：查看表 MyClass 中所有数据  \n\n​\t\t\tmysql> select  from MyClass;  \n\n​\t2）、查询前几行数据  \n\n​\t\t例如：查看表 MyClass 中前行数据  \n\n​\t\t\tmysql> select  from MyClass order by id limit 0,2;  \n\n​\t\t或者：  \n\n​\t\t\tmysql> select  from MyClass limit 0,2;  \n\n###### 6、删除表中数据  \n\n​\t\t命令：delete from 表名 where 表达式  \n\n​\t\t\t例如：删除表 MyClass中编号为 的记录  \n\n​\t\t\t\tmysql> delete from MyClass where id=1;  \n\n###### 7、修改表中数据：update 表名 set 字段=新值,…where 条件  \n\n​\t\tmysql> update MyClass set name=’Mary’where id=1;  \n\n###### 8、在表中增加字段：  \n\n​\t\t命令：alter table 表名 add字段 类型 其他;  \n\n​\t\t\t例如：在表MyClass中添加了一个字段passtest，类型为int(4)，默认值为  \n\n​\t\t\t\tmysql> alter table MyClass add passtest int(4) default ’′  \n\n###### 9、更改表名：  \n\n​\t\t命令：rename table 原表名 to 新表名;  \n\n​\t\t\t例如：在表MyClass名字更改为YouClass  \n\n​\t\t\t\tmysql> rename table MyClass to YouClass;  \n\n​\t\t\t更新字段内容  \n\n​\t\t\t\tupdate 表名 set 字段名 = 新内容  \n\n​\t\t\t\tupdate 表名 set 字段名 = replace(字段名,’旧内容’,’新内容’)  \n\n​\t\t\t\tupdate article set content=concat(‘　　’,content); \n\n# MYSQL查看正在运行的SQL\n\n有2个方法：\n\n###### \t1、使用processlist\n\n​\t但是有个弊端，就是只能查看正在执行的sql语句，对应历史记录，查看不到。好处是不用设置，不会保存。\n\n```\n    -- use information_schema;\n    -- show processlist;\n    或者：\n    -- select * from information_schema.`PROCESSLIST` where info is not null;\n```\n\n###### \t2、开启日志模式\n\n```\n    -- 1、设置\n    -- SET GLOBAL log_output = 'TABLE';  SET GLOBAL general_log = 'ON';\n    -- SET GLOBAL log_output = 'TABLE';  SET GLOBAL general_log = 'OFF';\n\n    -- 2、查询\n    SELECT * from mysql.general_log ORDER BY    event_time DESC\n\n    -- 3、清空表（delete对于这个表，不允许使用，只能用truncate）\n    -- truncate table mysql.general_log;\n```\n","tags":["mysql"]},{"title":"dva源码解读","url":"/2018/04/11/pout/dva源码解读/","content":"\n### 声明\n\n本文章用于个人学习研究，并不代表 dva 团队的任何观点。\n\n原文以及包含一定注释的代码见[这里](https://github.com/aircloud/dva-analysis)，若有问题也可以在[这里](https://github.com/aircloud/dva-analysis/issues)进行讨论\n\n### 起步\n\n#### 为什么是dva?\n\n笔者对 dva 的源代码进行解读，主要考虑到 dva 并不是一个和我们熟知的主流技术无关的从0到1的框架，相反，它是对主流技术进行整合，提炼，从而形成一种最佳实践，分析 dva，意味着我们可以对自己掌握的很多相关技术进行回顾，另外，dva 的代码量并不多，也不至于晦涩难懂，可以给我们平时的业务开发以启发。\n\n本文章作为 dva 的源码解读文章，并不面向新手用户，读者应当有一定的 react 使用经验和 ECMAscript 2015+ 的使用经验，并且应当了解 redux 和 redux-saga，以及对 dva 的使用有所了解(可以从[这里](https://github.com/dvajs/dva/blob/master/README_zh-CN.md#%E4%B8%BA%E4%BB%80%E4%B9%88%E7%94%A8-dva-)了解为什么需要使用 dva)\n\n重点推荐:\n\n* 通过[这里](https://github.com/dvajs/dva-knowledgemap)的内容了解使用dva的最小知识集\n* 通过[这里](https://redux-saga-in-chinese.js.org/docs/introduction/index.html)学习 redux-saga\n\n其他推荐：\n\n* [dva的概念](https://github.com/dvajs/dva/blob/master/docs/Concepts_zh-CN.md)\n* [dva的全部API](https://github.com/dvajs/dva/blob/master/docs/API_zh-CN.md)\n* [React+Redux 最佳实践](https://github.com/sorrycc/blog/issues/1)\n* [React在蚂蚁金服的实践](http://slides.com/sorrycc/dva#/)\n* [dva 2.0的改进](https://github.com/sorrycc/blog/issues/48)\n* [ReSelect介绍](http://cn.redux.js.org/docs/recipes/ComputingDerivedData.html)\n* [浅析Redux 的 store enhancer](https://www.jianshu.com/p/04d3fefea8d7)\n\n\n几个 dva 版本之间的关系:\n\n* dva@2.0：基于 react 和 react-router@4\n* dva-react-router-3@1.0：基于 react 和 react-router@3\n* dva-no-router@1.0：无路由版本，适用于多页面场景，可以和 next.js 组合使用\n* dva-core@1.0：仅封装了 redux 和 redux-saga\n\n我们本次主要分析目标为 dva@2.0 和 dva-core@1.0\n\n\n### 我们为什么需要 redux-saga\n\n目前，在大多数项目开发中，我们现在依然采用的是redux-thunk + async/await (或 Promise)。\n\n实际上这个十几行的插件已经完全可以解决大多是场景下的问题了，如果你在目前的工作中正在使用这一套方案并且能够完全将当下的需求应付自如并且没有什么凌乱的地方，其实也是没有必要换成redux-saga的。\n\n接下来我们讲 redux-saga，先看名字：saga，这个术语常用于CQRS架构，代表查询与责任分离。\n\n相比于 redux-thunk，前者通常是把数据查询等请求放在 actions 中(不纯净的 actions)，并且这些 actions 可以继续回调调用其他 actions(纯净的 actions)，从而完成数据的更新；而 redux-saga，则保持了 actions 的纯粹性，单独抽出一层专门来处理数据请求等操作(saga函数)。\n\n这样做还有另外一些好处：\n\n* 由于我们已经将数据处理数据请求等异步操作抽离出来了，并且通过 generator 来处理，我们便可以方便地进行多种异步管理：比如同时按顺序执行多个任务、在多个异步任务中启动race等。\n* 这样做可以延长任务的生命周期，我们的一次调用可以不再是一个\"调完即走\"的过程，还可以是一个LLT（Long Lived Transaction)的事物处理过程，比如我们可以将用户的登入、登出的管理放在一个saga函数中处理。\n\n当然，redux-saga还有比如拥有有诸多常用并且声明式易测的 Effects、可以无阻塞的fork等一些更复杂的异步操作和管理方法，如果应用中有较多复杂的异步操作流程，使用redux-saga无疑会让条理更加清楚。\n\n当然，本文的目的不是介绍或者安利redux-saga，只是因为redux-saga是 dva 的一个基础，相关概念点到为止，如需了解更多请自行参考资料。\n\n### dva 源码解读\n\n我们的源码分析流程是这样的：通过一个使用 dva 开发的例子，随着其对 dva 函数的逐步调用，来分析内部 dva 相关函数的实现原理。\n\n我们分析采用的例子是 dva 官方提供的一个增删改查的应用，可以在[这里](https://github.com/dvajs/dva/tree/rewrite-dynamic)找到它的源代码。\n\n我们先看该例子的入口文件：\n\n```\nimport dva from 'dva';\nimport createHistory from 'history/createBrowserHistory';\nimport createLoading from 'dva-loading';\nimport { message } from 'antd';\nimport './index.css';\n\nconst ERROR_MSG_DURATION = 3; // 3 秒\n\n// 1. Initialize\nconst app = dva({\n  history: createHistory(),\n  onError(e) {\n    message.error(e.message, ERROR_MSG_DURATION);\n  },\n});\n\n// 2. Plugins\napp.use(createLoading());\n\n// 3. Model\n// Moved to router.js\n// 这里的 Model 被转移到了动态加载的 router 里面，我们也可以如下写：\n// app.model(require('./models/users'));\n\n// 4. Router\napp.router(require('./router'));\n\n// 5. Start\napp.start('#root');\n```\n\n我们发现dva从初始化配置到最后的start(现在的dva start函数在不传入container的情况下可以返回React Component，便于服务端渲染等，但这里我们还是按照例子的写法来)。\n\n这里我们先有必要解释一下，dva 在当前依据能力和依赖版本的不同，有多个可引入的版本，我们的例子和所要分析的源代码都是基于 react-router V4 的 dva 版本。\n\n在源代码中，相关目录主要为 dva 目录(packages/dva) 和 dva-core(packages/dva-core)目录，前者主要拥有history管理、router、动态加载等功能，而后者是不依赖这些内容的基础模块部分，为前者所引用\n\n#### 第一步\n\n第一步这里传入了两个内容：(dva构造函数总共可以传入那些 opts，会在下文中进行说明)\n\n```\nconst app = dva({\n  history: createHistory(),\n  onError(e) {\n    message.error(e.message, ERROR_MSG_DURATION);\n  },\n});\n```\n\n这一步的相关核心代码如下:\n\n```\nexport default function (opts = {}) {\n  const history = opts.history || createHashHistory(); // 默认为 HashHistory\n  const createOpts = {\n    initialReducer: {\n      routing, // 来自 react-router-redux 的 routerReducer\n    },\n    setupMiddlewares(middlewares) {\n      return [\n        routerMiddleware(history), // 来自 react-router-redux 的 routerMiddleware\n        ...middlewares,\n      ];\n    },\n    setupApp(app) {\n      app._history = patchHistory(history); \n    },\n  };\n\n  const app = core.create(opts, createOpts);\n  const oldAppStart = app.start;\n  app.router = router;\n  app.start = start;\n  return app;\n  \n  // 一些用到的函数的定义...\n  \n}  \n```\n\n这里面大多数内容都比较简单，这里面提两个地方：\n\n1. patchHistory：\n\n```\nfunction patchHistory(history) {\n  const oldListen = history.listen;\n  history.listen = (callback) => {\n    callback(history.location);\n    return oldListen.call(history, callback);\n  };\n  return history;\n}\n```\n\n显然，这里的意思是让第一次被绑定 listener 的时候执行一遍 callback，可以用于初始化相关操作。\n\n我们可以在`router.js`中添加如下代码来验证：\n\n```\n  history.listen((location, action)=>{\n    console.log('history listen:', location, action)\n  })\n```\n\n2. 在完成可选项的构造之后，调用了 dva-core 中暴露的 create 函数。\n\ncreate 函数本身也并不复杂，核心代码如下：\n\n```javascript\nexport function create(hooksAndOpts = {}, createOpts = {}) {\n  const {\n    initialReducer,\n    setupApp = noop,\n  } = createOpts;\n\n  const plugin = new Plugin(); // 实例化钩子函数管理类\n  plugin.use(filterHooks(hooksAndOpts)); // 这个时候先对 obj 进行清理，清理出在我们定义的类型之外的 hooks，之后进行统一绑定\n\n  const app = {\n    _models: [\n      prefixNamespace({ ...dvaModel }), // 前缀处理\n    ],\n    _store: null,\n    _plugin: plugin,\n    use: plugin.use.bind(plugin),\n    model, // 下文定义\n    start, // 下文定义\n  };\n  return app;\n \n  //一些函数的定义\n  \n}  \n```\n\n这里面我们可以看到，这里的 `hooksAndOpts` 实际上就是一开始我们构造 dva 的时候传入的 opts 对象经过处理之后的结果。\n\n我们可以传入的可选项，实际上都在 `Plugin.js` 中写明了:\n\n```\nconst hooks = [\n  'onError',\n  'onStateChange',\n  'onAction',\n  'onHmr',\n  'onReducer',\n  'onEffect',\n  'extraReducers',\n  'extraEnhancers',\n];\n```\n\n具体 [hooks的作用可以在这里进行查阅](https://github.com/dvajs/dva/blob/master/docs/API_zh-CN.md#appusehooks)。\n\nPlugin 插件管理类(实际上我认为称其为钩子函数管理类比较合适)除了定义了上文的使用到的use方法(挂载插件)、还有apply方法(执行某一个钩子下挂载的所有回调)、get方法(获取某一个钩子下的所有回调，返回数组)\n\n\n#### 第二步\n\n\n这里的第二步比较简洁：我们知道实际上这里就是使用了`plugin.use`方法挂载了一个插件\n\n```javascript\napp.use(createLoading()); // 需要注意，插件挂载需要在 app.start 之前\n```\n\ncreateLoading 这个插件实际上是官方提供的 Loading 插件，通过这个插件我们可以非常方便地进行 Loading 的管理，无需进行手动管理，我们可以先[看一篇文章](https://www.jianshu.com/p/61fe7a57fad4)来简单了解一下。\n\n这个插件看似神奇，实际上原理也比较简单，主要用了`onEffect`钩子函数(装饰器)：\n\n```javascript\nfunction onEffect(effect, { put }, model, actionType) {\n    const { namespace } = model;\n    if (\n        (only.length === 0 && except.length === 0)\n        || (only.length > 0 && only.indexOf(actionType) !== -1)\n        || (except.length > 0 && except.indexOf(actionType) === -1)\n    ) {\n        return function*(...args) {\n            yield put({ type: SHOW, payload: { namespace, actionType } });\n            yield effect(...args);\n            yield put({ type: HIDE, payload: { namespace, actionType } });\n        };\n    } else {\n        return effect;\n    }\n  }\n```\n\n结合基于的redux-saga，在目标异步调用开始的时候`yield put({ type: SHOW, payload: { namespace, actionType } });`，在异步调用结束的时候`yield put({ type: HIDE, payload: { namespace, actionType } });`，这样就可以管理异步调用开始和结束的Loading状态了。\n\n\n#### 第三步\n\n第三步这里其实省略了，因为使用了动态加载，将 Models 定义的内容和 React Component 进行了动态加载，实际上也可以按照注释的方法来写。\n\n但是没有关系，我们还是可以分析 models 引入的文件中做了哪些事情(下面列出的代码在原基础上进行了一些简化):\n\n```javascript\nimport queryString from 'query-string';\nimport * as usersService from '../services/users';\n\nexport default {\n  namespace: 'users',\n  state: {\n    list: [],\n    total: null,\n    page: null,\n  },\n  reducers: {\n    save(state, { payload: { data: list, total, page } }) {\n      return { ...state, list, total, page };\n    },\n  },\n  effects: {\n    *fetch({ payload: { page = 1 } }, { call, put }) {\n      const { data, headers } = yield call(usersService.fetch, { page });\n      yield put({\n        type: 'save',\n        payload: {\n          data,\n          total: parseInt(headers['x-total-count'], 10),\n          page: parseInt(page, 10),\n        },\n      });\n    },\n    //...\n    *reload(action, { put, select }) {\n      const page = yield select(state => state.users.page);\n      yield put({ type: 'fetch', payload: { page } });\n    },\n  },\n  subscriptions: {\n    setup({ dispatch, history }) {\n      return history.listen(({ pathname, search }) => {\n        const query = queryString.parse(search);\n        if (pathname === '/users') {\n          dispatch({ type: 'fetch', payload: query });\n        }\n      });\n    },\n  },\n};\n```\n\n这些内容，我们通过`app.model(require('./models/users'));`就可以引入。\n\n实际上，model 函数本身还是比较简单的，但由于 dva 拥有 model 动态加载的能力，实际上调用 app.start 前和 app.start 后model函数是不一样的。\n\n调用 start 函数前，我们直接挂载即可(因为start函数中会对所有model进行遍历性统一处理，所以无需过多处理)：\n\n```javascript\nfunction model(m) {\n    if (process.env.NODE_ENV !== 'production') {\n      checkModel(m, app._models);\n    }\n    app._models.push(prefixNamespace(m));\n    // 把 model 注册到 app 的 _models 里面，但是当 app start 之后，就不能仅仅用这种方法了，需要 injectModel\n  }\n```\n\n调用了 start 函数之后，model函数被替换成如下:\n\n```javascript\nfunction injectModel(createReducer, onError, unlisteners, m) {\n    model(m);\n\n    const store = app._store;\n    if (m.reducers) {\n      store.asyncReducers[m.namespace] = getReducer(m.reducers, m.state);\n      store.replaceReducer(createReducer(store.asyncReducers));\n    }\n    if (m.effects) {\n      store.runSaga(app._getSaga(m.effects, m, onError, plugin.get('onEffect')));\n    }\n    if (m.subscriptions) {\n      unlisteners[m.namespace] = runSubscription(m.subscriptions, m, app, onError);\n    }\n  }\n```\n\n**我们首先分析第一个 if 中的内容**：首先通过getReducer函数将转换好的 reducers 挂载(或替换)到 store.asyncReducers[m.namespace] 中，然后通过 redux 本身提供的能力 replaceReducer 完成 reducer 的替换。\n\n这里我们需要注意 getReducer 函数，实际上，dva 里面 reducers 写法和我们之前直接使用 redux 的写法略有不同：\n\n我们这里的 reducers，实际上要和 action 中的 actionType 同名的 reducer，所以这里我们没有必要去写 switch case 了，对于某一个 reducer 来说其行为应该是确定的，这给 reducers 的写法带来了一定的简化，当然，我们可以使用 extraReducers 定义我们之前习惯的那种比较复杂的 reducers。\n\n**接下来我们分析第二个 if 中的内容**：第二个函数首先获取到了我们定义的 effects 并通过 _getSaga 进行处理，然后使用 `runSaga`(实际上就是createSagaMiddleware().run，来自于redux-saga) 进行执行。\n\n实际上，这里的 `_getSaga` 函数比较复杂，我们接下来重点介绍这个函数。\n\n`_getSaga` 函数由 `getSaga.js` 暴露，其定义如下：\n\n```javascript\nexport default function getSaga(resolve, reject, effects, model, onError, onEffect) {\n  return function *() {  // 返回一个函数\n    for (const key in effects) {  // 这个函数对 effects 里面的所有键\n      if (Object.prototype.hasOwnProperty.call(effects, key)) { // 先判断一下键是属于自己的\n        const watcher = getWatcher(resolve, reject, key, effects[key], model, onError, onEffect);\n        // 然后调用getWatch获取watcher\n        const task = yield sagaEffects.fork(watcher); // 利用 fork 开启一个 task\n        yield sagaEffects.fork(function *() { // 这样写的目的是，如果我们移除了这个 model 要及时结束掉\n          yield sagaEffects.take(`${model.namespace}/@@CANCEL_EFFECTS`);\n          yield sagaEffects.cancel(task);\n        });\n      }\n    }\n  };\n}\n```\n\ngetWatcher 的一些核心代码如下:\n\n```javascript\n\nfunction getWatcher(resolve, reject, key, _effect, model, onError, onEffect) {\n  let effect = _effect;\n  let type = 'takeEvery';\n  let ms;\n\n  if (Array.isArray(_effect)) {\n    effect = _effect[0];\n    const opts = _effect[1];\n    // 对 opts 进行一定的校验\n    //...\n  }\n\n  function *sagaWithCatch(...args) { // 都会调用这个过程\n    try {\n      yield sagaEffects.put({ type: `${key}${NAMESPACE_SEP}@@start` });\n      const ret = yield effect(...args.concat(createEffects(model)));\n      yield sagaEffects.put({ type: `${key}${NAMESPACE_SEP}@@end` });\n      resolve(key, ret);\n    } catch (e) {\n      onError(e);\n      if (!e._dontReject) {\n        reject(key, e);\n      }\n    }\n  }\n\n  const sagaWithOnEffect = applyOnEffect(onEffect, sagaWithCatch, model, key); \n  // 挂载 onEffect 钩子\n\n  switch (type) {\n    case 'watcher':\n      return sagaWithCatch;\n    case 'takeLatest':\n      return function*() {\n        yield takeLatest(key, sagaWithOnEffect);\n      };\n    case 'throttle': // 起到节流的效果，在 ms 时间内仅仅会被触发一次\n      return function*() {\n        yield throttle(ms, key, sagaWithOnEffect);\n      };\n    default:\n      return function*() {\n        yield takeEvery(key, sagaWithOnEffect);\n      };\n  }\n}\n```\n\n这个函数的工作，可以主要分为以下三个部分：\n\n1.将 effect 包裹成 sagaWithCatch，除了便于错误处理和增加前后钩子，值得我们注意的是 resolve 和 reject，\n\n这个 resolve 和 reject，实际上是来自`createPromiseMiddleware.js`\n\n我们知道，我们在使用redux-saga的过程中，实际上是监听未来的action，并执行 effects，所以我们在一个 effects 函数中执行一些异步操作，然后 put(dispatch) 一个 action，还是会被监听这个 action 的其他 saga 监听到。\n\n所以就有如下场景：我们 dispatch 一个 action，这个时候如果我们想获取到什么时候监听这个 action 的 saga 中的异步操作执行结束，是办不到的(因为不是所有的时候我们都把所有处理逻辑写在 saga 中)，所以我们的 dispatch 有的时候需要返回一个 Promise 从而我们可以进行异步结束后的回调(这个 Promise 在监听者 saga 异步执行完后被决议，见上文`sagaWithCatch`函数源代码)。\n\n如果我讲的还是比较混乱，也可以参考[这个issue](https://github.com/dvajs/dva/issues/175)\n\n对于这个情况，我认为这是 dva 代码最精彩的地方之一，作者通过定义如下的middleware:\n\n```javascript\n const middleware = () => next => (action) => {\n    const { type } = action;\n    if (isEffect(type)) {\n      return new Promise((resolve, reject) => {\n        map[type] = {\n          resolve: wrapped.bind(null, type, resolve),\n          reject: wrapped.bind(null, type, reject),\n        };\n      });\n    } else {\n      return next(action);\n    }\n  };\n\n  function wrapped(type, fn, args) {\n    if (map[type]) delete map[type];\n    fn(args);\n  }\n\n  function resolve(type, args) {\n    if (map[type]) {\n      map[type].resolve(args);\n    }\n  }\n\n  function reject(type, args) {\n    if (map[type]) {\n      map[type].reject(args);\n    }\n  }\n```\n\n并且在上文的`sagaWithCatch`相关effect执行结束的时候调用 resolve，让 dispatch 返回了一个 Promise。\n\n当然，上面这段代码还是有点问题的，这样会导致同名 reducer 和 effect 不会 fallthrough（即两者都执行），因为都已经返回了，action 便不会再进一步传递，关于这样设计的好坏，在[这里](https://github.com/sorrycc/blog/issues/48)有过一些讨论，笔者不进行展开表述。\n\n2.在上面冗长的第一步之后，又通过`applyOnEffect`函数包裹了`OnEffect`的钩子函数，这相当于是一种`compose`，(上文的 dva-loading 中间件实际上就是在这里被处理的)其实现对于熟悉 redux 的同学来说应该不难理解：\n\n```javascript\nfunction applyOnEffect(fns, effect, model, key) {\n  for (const fn of fns) {\n    effect = fn(effect, sagaEffects, model, key);\n  }\n  return effect;\n}\n```\n\n3.最后，根据我们定义的type(默认是`takeEvery`，也就是都执行)，来选择不同的 saga，takeLatest 即为只是执行最近的一个，throttle则起到节流的效果，一定时间内仅仅允许被触发一次，这些都是 redux-saga 的内部实现，dva 也是基本直接引用，因此在这里不进行展开。\n\n**最后我们分析`injectModel`第三个`if`中的内容**:处理`subscriptions`:\n\n```javascript\nif (m.subscriptions) {\n  unlisteners[m.namespace] = runSubscription(m.subscriptions, m, app, onError);\n}\n```\n\n`subscriptions`可以理解为和这个model有关的全局监听，但是相对独立。这一个步骤首先调用`runSubscription`来一个一个调用我们的`subscriptions`:\n\n```javascript\nexport function run(subs, model, app, onError) { // 在index.js中被重命名为 runSubscription\n  const funcs = [];\n  const nonFuncs = [];\n  for (const key in subs) {\n    if (Object.prototype.hasOwnProperty.call(subs, key)) {\n      const sub = subs[key];\n      const unlistener = sub({\n        dispatch: prefixedDispatch(app._store.dispatch, model),\n        history: app._history,\n      }, onError);\n      if (isFunction(unlistener)) {\n        funcs.push(unlistener);\n      } else {\n        nonFuncs.push(key);\n      }\n    }\n  }\n  return { funcs, nonFuncs };\n}\n```\n\n正如我们所期待的，`run`函数就是一个一个执行`subscriptions`，但是这里有一点需要我们注意的，我们定义的`subscriptions`应该是需要返回一个`unlistener`来返回接触函数，这样当整个 model 被卸载的时候 dva 会自动调用这个接解除函数(也就是为什么这里的返回函数被命名为`unlistener`)\n\n#### 第四步\n\n源代码中的第四步，是对 router 的挂载：\n\n```javascript\napp.router(require('./router'));\n```\n\n`require('./router')`返回的内容在源代码中经过一系列引用传递最后直接被构造成 React Component 并且最终调用 ReactDom.render 进行渲染，这里没有什么好说的，值得一提的就是 router 的动态加载。\n\n动态加载在该样例中是这样使用的：\n\n```javascript\nimport React from 'react';\nimport { Router, Switch, Route } from 'dva/router';\nimport dynamic from 'dva/dynamic';\n\nfunction RouterConfig({ history, app }) {\n  const IndexPage = dynamic({\n    app,\n    component: () => import('./routes/IndexPage'),\n  });\n\n  const Users = dynamic({\n    app,\n    models: () => [\n      import('./models/users'),\n    ],\n    component: () => import('./routes/Users'),\n  });\n\n  history.listen((location, action)=>{\n    console.log('history listen:', location, action)\n  })\n\n  return (\n    <Router history={history}>\n      <Switch>\n        <Route exact path=\"/\" component={IndexPage} />\n        <Route exact path=\"/users\" component={Users} />\n      </Switch>\n    </Router>\n  );\n}\n```\n\n我们可以看出，主要就是利用`dva/dynamic.js`暴露的 dynamic 函数进行动态加载，接下来我们简单看一下 dynamic 函数做了什么:\n\n```javascript\nexport default function dynamic(config) {\n  const { app, models: resolveModels, component: resolveComponent } = config;\n  return asyncComponent({\n    resolve: config.resolve || function () {\n      const models = typeof resolveModels === 'function' ? resolveModels() : [];\n      const component = resolveComponent();\n      return new Promise((resolve) => {\n        Promise.all([...models, component]).then((ret) => {\n          if (!models || !models.length) {\n            return resolve(ret[0]);\n          } else {\n            const len = models.length;\n            ret.slice(0, len).forEach((m) => {\n              m = m.default || m;\n              if (!Array.isArray(m)) {\n                m = [m];\n              }\n              m.map(_ => registerModel(app, _)); // 注册所有的 model\n            });\n            resolve(ret[len]);\n          }\n        });\n      });\n    },\n    ...config,\n  });\n}\n```\n\n这里主要调用了 asyncComponent 函数，接下来我们再看一下这个函数：\n\n```javascript\nfunction asyncComponent(config) {\n  const { resolve } = config;\n\n  return class DynamicComponent extends Component {\n    constructor(...args) {\n      super(...args);\n      this.LoadingComponent =\n        config.LoadingComponent || defaultLoadingComponent;\n      this.state = {\n        AsyncComponent: null,\n      };\n      this.load();\n    }\n\n    componentDidMount() {\n      this.mounted = true;\n    }\n\n    componentWillUnmount() {\n      this.mounted = false;\n    }\n\n    load() {\n      resolve().then((m) => {\n        const AsyncComponent = m.default || m;\n        if (this.mounted) {\n          this.setState({ AsyncComponent });\n        } else {\n          this.state.AsyncComponent = AsyncComponent; // eslint-disable-line\n        }\n      });\n    }\n\n    render() {\n      const { AsyncComponent } = this.state;\n      const { LoadingComponent } = this;\n      if (AsyncComponent) return <AsyncComponent {...this.props} />;\n\n      return <LoadingComponent {...this.props} />;\n    }\n  };\n}\n```\n\n这个函数逻辑比较简洁，我们分析一下动态加载流程；\n\n* 在 constructor 里面调用 `this.load();` ( LoadingComponent 为占位 component)\n* 在 `this.load();` 函数里面调用 `dynamic` 函数返回的 resolve 方法\n* resolve 方法实际上是一个 Promise，把相关 models 和 component 加载完之后 resolve (区分这两个 resolve)\n* 加载完成之后返回 AsyncComponent (即加载的 Component)\n\n动态加载主流程结束，至于动态加载的代码分割工作，可以使用 webpack3 的 `import()` 动态加载能力(例子中也是这样使用的)。\n\n\n#### 第五步\n\n第五步骤就是 start 了：\n\n```javascript\napp.start('#root');\n```\n\n这个时候如果我们在 start 函数中传入 DomElement 或者 DomQueryString，就会直接启动应用了，如果我们这个时候不传入任何内容，实际上返回的是一个`<Provider />` (React Component)，便于服务端渲染。 相关判断逻辑如下：\n\n```javascript\n if (container) {\n      render(container, store, app, app._router);\n      app._plugin.apply('onHmr')(render.bind(null, container, store, app));\n    } else {\n      return getProvider(store, this, this._router);\n    }\n```\n\n至此，主要流程结束，以上几个步骤也包括了 dva 源码做的主要工作。\n\n当然 dva 源码中还有一些比如前缀处理等工作，但是相比于以上内容非常简单，所以在这里不进行分析了。\n\n\n### dva-core 文件目录\n\ndva-core中的源码文件目录以及其功能:\n\n* checkModel 对我们定义的 Model 进行检查是否符合要求\n* constants 非常简单的常量文件，目前只定义了一个常量：NAMESPACE_SEP(/)\n* cratePromiseMiddleware 笔者自己定义的 redux 插件\n* createStore 封装了 redux 原生的 createStore\n* getReducer 这里面的函数其实主要就是调用了 handleActions 文件导出的函数\n* getSaga 将用户输入的 effects 部分的键值对函数进行管理\n* handleActions 是将 dva 风格的 reducer 和 state 转化成 redux 本来接受的那种方式\n* index 主入口文件\n* Plugin 插件类：可以管理不同钩子事件的回调函数，拥有增加、获取、执行钩子函数的功能\n* perfixedDispatch 该文件提供了对 Dispatch 增加前缀的工具性函数 prefixedDispatch\n* prefixNamespace 该文件提供了对 reducer 和 effects 增加前缀的工具性函数 prefixNamespace\n* prefixType 判断是 reducer 还是 effects\n* subscriptions 该文件提供了运行 subscriptions 和调用用户返回的 unlisten 函数以及删除缓存的功能\n* utils 提供一些非常基础的工具函数\n\n\n### 优势总结\n\n* 动态 model，已经封装好了整套调用，动态添加/删除 model 变得非常简单\n* 默认封装好了管理 effects 的方式，有限可选可配置，降低学习成本的同时代码更利于维护\n* 易于上手，集成redux、redux-saga、react-router等常用功能\n\n\n### 劣势总结\n\n* 版本区隔不明显，dva 有 1.x 和 2.x 两种版本，之间API有些差异，但是官网提供的一些样例等中没有说明基于的版本，并且有的还是基于旧版本的，会给新手带来很多疑惑。\n* 内容繁杂，但是却没有一个整合性质的官方网站，大都是通过 list 的形式列下来写在README的。\n* 目前比如动态加载等还存在着一些问题，和直接采用react配套工具写的效果有所区别。\n* 很多 issues 不知道为什么就被关闭了，作者在最后也并未给出合理的解释。\n* dva2 之后有点将 effects 和 actions 混淆，这一点我也并不是非常认同，当然原作者可能有自己的考虑，这里不过多评议。\n\n总之，作为一个个人主力的项目(主要开发者贡献了99%以上的代码)，可以看出作者的功底深厚，经验丰富，但是由于这样一个体系化的东西牵扯内容较多，并且非常受制于react、redux、react-router、redux-saga等的版本影响，**不建议具备一定规模的非阿里系团队在生产环境中使用**，但是如果是快速成型的中小型项目或者个人应用，使用起来还是有很大帮助的。\n\n### TODOS\n\n笔者也在准备做一个和 dva 处于同一性质，但是设计、实现和使用有所区别的框架，希望能够尽快落成。\n","tags":["前端框架"]},{"title":"构建利用Proxy和Reflect实现双向数据绑定的微框架","url":"/2018/04/09/pout/构建利用Proxy和Reflect实现双向数据绑定的微框架/","content":">写在前面：这篇文章讲述了如何利用Proxy和Reflect实现双向数据绑定，个人系Vue早期玩家，写这个小框架的时候也没有参考Vue等源代码，之前了解过其他实现，但没有直接参考其他代码，如有雷同，纯属巧合。\n\n代码下载地址：[这里下载](https://github.com/aircloud/Polar.js)\n\n### 综述\n\n*关于Proxy和Reflect的资料推荐阮老师的教程:http://es6.ruanyifeng.com/ 这里不做过多介绍。*\n\n实现双向数据绑定的方法有很多，也可以参考本专栏之前的其他实现，我之所以选择用Proxy和Reflect，一方面是因为可以大量节约代码，并且简化逻辑，可以让我把更多的经历放在其他内容的构建上面，另外一方面本项目直接基于ES6，用这些内容也符合面向未来的JS编程规范，第三点最后说。\n\n由于这个小框架是自己在PolarBear这个咖啡馆在一个安静的午后开始写成，暂且起名Polar，日后希望我能继续完善这个小框架，给添加上更多有趣的功能。\n\n首先我们可以看整体功能演示：  \n[一个gif动图，如果不能看，请点击[这里的链接](https://www.10000h.top/images/data_img/gif1.gif)]\n\n![](https://www.10000h.top/images/data_img/gif1.gif)\n\n### 代码分析\n\n我们要做这样一个小框架，核心是要监听数据的改变，并且在数据的改变的时候进行一些操作，从而维持数据的一致。\n\n我的思路是这样的：\n\n* 将所有的数据信息放在一个属性对象中(this._data),之后给这个属性对象用Proxy包装set,在代理函数中我们更新属性对象的具体内容，同时通知所有监听者，之后返回新的代理对象(this.data)，我们之后操作的都是新的代理对象。\n* 对于input等表单，我们需要监听input事件，在回调函数中直接设置我们代理好的数据对象，从而触发我们的代理函数。\n* 我们同时也应该支持事件机制，这里我们以最常用的click方法作为例子实现。\n\n下面开始第一部分，我们希望我们之后使用这个库的时候可以这样调用:\n\n```\n<div id=\"app\">\n    <form>\n        <label>name:</label>\n        <input p-model = \"name\" />\n    </form>\n    <div>name:{{name}} age:{{age}}</div>\n    <i>note:{{note}}</i><br/>\n    <button p-click=\"test(2)\">button1</button>\n</div>\n<script>\n var myPolar = new Polar({\n        el:\"#app\",\n        data: {\n            name: \"niexiaotao\",\n            age:16,\n            note:\"Student of Zhejiang University\"\n        },\n        methods:{\n            test:function(e,addNumber){\n                console.log(\"e:\",e);\n                this.data.age+=Number(addNumber);\n            }\n        }\n});\n</script>\n```\n\n没错，和Vue神似吧，所以这种调用方式应当为我们所熟悉。\n\n我们需要建立一个Polar类，这个类的构造函数应该进行一些初始化操作:\n\n```\n constructor(configs){\n        this.root = this.el = document.querySelector(configs.el);\n        this._data = configs.data;\n        this._data.__bindings = {};\n        //创建代理对象\n        this.data = new Proxy(this._data, {set});\n        this.methods = configs.methods;\n\n        this._compile(this.root);\n}\n```\n\n这里面的一部份内容是直接将我们传入的configs按照属性分别赋值，另外就是我们创建代理对象的过程，最后的`_compile`方法可以理解为一个私有的初始化方法。\n\n实际上我把剩下的内容几乎都放在`_compile`方法里面了，这样理解起来方便，但是之后可能要改动。\n\n我们还是先不能看我们代理的set该怎么写，因为这个时候我们还要先继续梳理思路：\n\n假设我们这样`<div>name:{{name}}</div>`将数据绑定到dom节点，这个时候我们需要做什么呢，或者说，我们通过什么方式让dom节点和数据对应起来，随着数据改变而改变。\n\n看上文的`__bindings`。这个对象用来存储所有绑定的dom节点信息，`__bindings`本身是一个对象，每一个有对应dom节点绑定的数据名称都是它的属性，对应一个数组，数组中的每一个内容都是一个绑定信息，这样，我们在自己写的set代理函数中，我们一个个调用过去，就可以更新内容了：\n\n```\ndataSet.__bindings[key].forEach(function(item){\n       //do something to update...\n});\n```\n\n我这里创建了一个用于构造调用的函数，这个函数用于创建存储绑定信息的对象：\n\n```\nfunction Directive(el,polar,attr,elementValue){\n    this.el=el;//元素本身dom节点\n    this.polar = polar;//对应的polar实例\n    this.attr = attr;//元素的被绑定的属性值，比如如果是文本节点就可以是nodeValue\n    this.el[this.attr] = this.elementValue = elementValue;//初始化\n}\n```\n\n这样，我们的set可以这样写:\n\n```\nfunction set(target, key, value, receiver) {\n    const result = Reflect.set(target, key, value, receiver);\n    var dataSet = receiver || target;\n    dataSet.__bindings[key].forEach(function(item){\n        item.el[item.attr] = item.elementValue = value;\n    });\n    return result;\n}\n```\n\n接下来可能还有一个问题：我们的`{{name}}`实际上只是节点的一部分，这并不是节点啊，另外我们是不是还可以这么写：`<div>name:{{name}} age:{{age}}</div>`？\n\n关于这两个问题，前者的答案是我们将`{{name}}`替换成一个文本节点，而为了应对后者的情况，我们需要将两个被绑定数据中间和前后的内容，都变成新的文本节点，然后这些文本节点组成文本节点串。(这里多说一句，html5的normalize方法可以将多个文本节点合并成一个，如果不小心调用了它，那我们的程序就要GG了)\n\n所以我们在`_compile`函数首先：\n\n```\nvar _this = this;\n\n        var nodes = root.children;\n\n        var bindDataTester = new RegExp(\"{{(.*?)}}\",\"ig\");\n\n        for(let i=0;i<nodes.length;i++){\n            var node=nodes[i];\n\n            //如果还有html字节点，则递归\n            if(node.children.length){\n                this._compile(node);\n            }\n\n            var matches = node.innerHTML.match(bindDataTester);\n            if(matches){\n                var newMatches = matches.map(function (item) {\n                    return  item.replace(/{{(.*?)}}/,\"$1\")\n                });\n                var splitTextNodes  = node.innerHTML.split(/{{.*?}}/);\n                node.innerHTML=null;\n                //更新DOM，处理同一个textnode里面多次绑定情况\n                if(splitTextNodes[0]){\n                    node.append(document.createTextNode(splitTextNodes[0]));\n                }\n                for(let ii=0;ii<newMatches.length;ii++){\n                    var el = document.createTextNode('');\n                    node.appendChild(el);\n                    if(splitTextNodes[ii+1]){\n                        node.append(document.createTextNode(splitTextNodes[ii+1]));\n                    }\n                //对数据和dom进行绑定\n                let returnCode = !this._data.__bindings[newMatches[ii]]?\n                    this._data.__bindings[newMatches[ii]] = [new Directive(el,this,\"nodeValue\",this.data[newMatches[ii]])]\n                    :this._data.__bindings[newMatches[ii]].push(new Directive(el,this,\"nodeValue\",this.data[newMatches[ii]]))\n                }\n            }\n\n```\n\n这样，我们的数据绑定阶段就写好了，接下来，我们处理`<input p-model = \"name\" />`这样的情况。\n\n这实际上是一个指令，我们只需要当识别到这一个指令的时候，做一些处理，即可：\n\n```\nif(node.hasAttribute((\"p-model\"))\n                && node.tagName.toLocaleUpperCase()==\"INPUT\" || node.tagName.toLocaleUpperCase()==\"TEXTAREA\"){\n                node.addEventListener(\"input\", (function () {\n\n                    var attributeValue = node.getAttribute(\"p-model\");\n\n                    if(_this._data.__bindings[attributeValue]) _this._data.__bindings[attributeValue].push(new Directive(node,_this,\"value\",_this.data[attributeValue])) ;\n                    else _this._data.__bindings[attributeValue] = [new Directive(node,_this,\"value\",_this.data[attributeValue])];\n\n                    return function (event) {\n                        _this.data[attributeValue]=event.target.value\n                    }\n                })());\n}\n```\n\n请注意，上面调用了一个`IIFE`，实际绑定的函数只有返回的函数那一小部分。\n\n最后我们处理事件的情况：`<button p-click=\"test(2)\">button1</button>`\n\n实际上这比处理`p-model`还简单，但是我们为了支持函数参数的情况，处理了一下传入参数，另外我实际上将`event`始终作为一个参数传递，这也许并不是好的实践，因为使用的时候还要多注意。\n\n```\nif(node.hasAttribute(\"p-click\")) {\n                node.addEventListener(\"click\",function(){\n                    var attributeValue=node.getAttribute(\"p-click\");\n                    var args=/\\(.*\\)/.exec(attributeValue);\n                    //允许参数\n                    if(args) {\n                        args=args[0];\n                        attributeValue=attributeValue.replace(args,\"\");\n                        args=args.replace(/[\\(\\)\\'\\\"]/g,'').split(\",\");\n                    }\n                    else args=[];\n                    return function (event) {\n                        _this.methods[attributeValue].apply(_this,[event,...args]);\n                    }\n                }());\n}\n```\n\n现在我们已经将所有的代码分析完了，是不是很清爽？代码除去注释约100行，所有源代码可以在[这里下载](https://github.com/aircloud/Polar.js)。这当然不能算作一个框架了，不过可以学习学习，这学期有时间的话，还要继续完善，也欢迎大家一起探讨。\n\n一起学习，一起提高，做技术应当是直接的，有问题欢迎指出～\n\n---\n\n\n最后说的第三点：是自己还是一个学生，做这些内容也仅仅是出于兴趣，因为找暑期实习比较艰难，在等待鹅厂面试间隙写的这个程序，压压惊(然而并没有消息)。","tags":["MVVM"]},{"title":"Gil","url":"/2018/02/23/pout/python中高级面试题/Gil/","content":"\n\n<!-- toc -->\n\n## 1.GIL是什么？\n\nGIL全称Global Interpreter Lock，即全局解释器锁。 作用就是，限制多线程同时执行，保证同一时间内只有一个线程在执行。 GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念。python 与 python解释器是两个概念，切不可混为一谈，也就是说，GIL只存在于使用C语言编写的解释器CPython中。 通俗地说，就是如果你不用Python官方推荐的CPython解释器，而使用其他语言编写的Python解释器（比如 JPython: 运行在Java上的解释器，直接把python代码编译成Java字节码执行 ），就不会有GIL问题。然而因为CPython是大部分环境下默认的Python执行环境。所以在很多人的概念里CPython就是Python，也就想当然的把GIL归结为Python语言的缺陷。所以这里要先明确一点：GIL并不是Python的特性，Python完全可以不依赖于GIL。\n\n## 2.GIL有什么作用？\n\n为了更有效的利用多核处理器的性能，就出现了多线程的编程方式，而随之带来的就是线程间数据的一致性和状态同步的完整性。 python为了利用多核，开始支持多线程，但线程是非独立的，所以同一进程里线程是数据共享，当各个线程访问数据资源时会出现竞状态，即数据可能会同时被多个线程占用，造成数据混乱，这就是线程的不安全。而解决多线程之间数据完整性和状态同步最简单的方式就是加锁。GIL能限制多线程同时执行，保证同一时间内只有一个线程在执行。\n\n## 3.GIL有什么影响？\n\nGIL无疑就是一把全局排他锁。毫无疑问全局锁的存在会对多线程的效率有不小影响。甚至就几乎等于Python是个单线程的程序。\n\n## 4.如何避免GIL带来的影响？\n\n方法一：用进程+协程 代替 多线程的方式 在多进程中，由于每个进程都是独立的存在，所以每个进程内的线程都拥有独立的GIL锁，互不影响。但是，由于进程之间是独立的存在，所以进程间通信就需要通过队列的方式来实现。\n\n## 方法二：更换解释器\n\n像JPython和IronPython这样的解析器由于实现语言的特性，他们不需要GIL的帮助。然而由于用了Java/C#用于解析器实现，他们也失去了利用社区众多C语言模块有用特性的机会。所以这些解析器也因此一直都比较小众。\n\n## 一些其他的解释器\n\nCPython 当我们从Python官方网站下载并安装好Python 2.7后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用C语言开发的，所以叫CPython。在命令行下运行python就是启动CPython解释器。\n\nCPython是使用最广的Python解释器。教程的所有代码也都在CPython下执行。\n\nIPython IPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了IE。\n\nCPython用>>>作为提示符，而IPython用In [序号]:作为提示符。\n\nPyPy PyPy是另一个Python解释器，它的目标是执行速度。PyPy采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。\n\n绝大部分Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，这就导致相同的Python代码在两种解释器下执行可能会有不同的结果。如果你的代码要放到PyPy下执行，就需要了解PyPy和CPython的不同点。\n\nJython Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。\n\nIronPython IronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器，可以直接把Python代码编译成.Net的字节码。\n","tags":["python中高级面试题"],"categories":["python中高级面试题"]},{"title":"微服务和RPC框架具体操作","url":"/2018/02/23/pout/python中高级面试题/微服务和RPC框架/微服务和RPC框架具体操作/","content":"\n\n<!-- toc -->\n\n# 9012年，你需要Thrift这样一个RPC框架来拯救一下传统HTTP接口了\n\n​    目前市面上类似Django的drf框架基于json的http接口解决方案大行其道，人们也热衷于在接口不多、系统与系统交互较少的情况下使用它，http接口的优点就是简单、直接、开发方便，门槛低，利用现成的http协议进行传输。\n\n​    但是事情往往有两面，如果是一个大型的网站，内部子系统较多、接口非常多的情况下，RPC框架的好处就显示出来了，首先就是长链接，不必每次通信都要像http 一样去3次握手4次挥手，减少了网络开销；其次就是RPC框架一般都有注册中心，有丰富的监控管理；发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作。第三个来说就是安全性。最后就是最近流行的服务化架构、服务化治理，RPC框架是一个强力的支撑。\n\n​    论复杂度，RPC框架肯定是高于简单的HTTP接口的。但毋庸置疑，HTTP接口由于受限于HTTP协议，需要带HTTP请求头，导致传输起来效率或者说安全性不如RPC，目前市面上流行的rpc框架有dubbo/hessian Thrift，阿里开源的dubbo固然还不错，但是本人更倾向于facebook开源的Thrift框架，这款框架在github上好评如潮，这一次我们使用的就是基于Thrift的thriftpy2框架。\n\n​    Thrift是一种接口描述语言和二进制通讯协议，它被用来定义和创建跨语言的服务，这是维基百科的描述。简单来说就是你可以按照Thrift定义语法编写.thrift,然后用Thrift命令行生成各种语言的代码，比如OC、Java、C++、JS，调用这些代码就可以完成客户端与服务器的通信了，不需要自己去写网络请求、数据解析等接口。\n\n​    其实在本人的实际教学工作中主要考虑到这两个优点：\n\n​    RPC。通过简单定义Thrift描述语言文件，使用Thrift -gen命令可以生成多种语言的代码，这些代码包含了网络通信,数据编解码的功能。这就免去了前后台编写这部分繁琐的代码，同时也统一了前后台的实现逻辑。\n\n​    Thrift的二进制数据的编码比json更加紧凑、减少了无用的数据的传输。\n\n​    \n\n​    安装:\n\n​    \n\n```\npip3 install thriftpy2\n```\n\n​    首先定义 thrift 通讯文件，无论是server端还是clinet端都是基于这个文件进行通信 pingpong.thrift\n\n​    \n\n```\nservice PingPong {\n    string ping(),\n    string check_login(\n        1: string username,\n        2: string password\n    ),\n}\n```\n\n​    可以看到我们定义了两个方法，一个有参一个无参，第一个方法用来检测接口是否通信成功，也就是传统的ping命令，第二个方法顾名思义，用户登录\n\n​    然后建立一个thrift_server.py 建立服务端的代码\n\n​    \n\n```\nimport thriftpy2\npingpong_thrift = thriftpy2.load(\"pingpong.thrift\", module_name=\"pingpong_thrift\")\n\nfrom thriftpy2.rpc import make_server\n\nclass Dispatcher(object):\n    \n    def ping(self):\n        return \"pong\"\n\n    def check_login(self,username,password):\n        print(username,password)\n        return '123'\n\nserver = make_server(pingpong_thrift.PingPong, Dispatcher(), '127.0.0.1', 6000)\nserver.serve()\n```\n\n​    服务端首先读取通信文件，然后建立起一个服务，监听6000端口，等待客户端请求，实际上服务端的方法也是主要业务逻辑编写的地方。\n\n​    随后建立一个thrift_client.py文件，编写客户端代码\n\n​    \n\n```\nimport thriftpy2\npingpong_thrift = thriftpy2.load(\"pingpong.thrift\", module_name=\"pingpong_thrift\")\n\nfrom thriftpy2.rpc import make_client\n\nclient = make_client(pingpong_thrift.PingPong, '127.0.0.1', 6000)\n\nprint(client.ping())\n\nprint(client.check_login('admin','123456'))\n```\n\n​    我们看到客户端同样读取通信文件，严格按照通信文件的方法调用方式进行传参调用，获取返回值\n\n​    运行服务器端的服务\n\n​    \n\n```\npython3 thrift_server.py\n```\n\n​    然后再执行客户端脚本python3 thrift_client.py\n\n​    ![img](https://v3u.cn/v3u/Public/js/editor/attached/image/20190827/20190827084349_38931.png)\n\n​    可以看到服务端和客户端就可以通信了\n\n​    可以说非常简单，这里着重提到的一点是Thrift的数据编解码，我们知道传统http接口通常以json为数据介质，json中一个对象类似于这样的：{\"key\":\"content\"},但实际上这个对象只有“content”才是我们真正想要的数据，而“key”这个字符串并不是我们实际需要的，只是为了做一个标记，方便我们查找“content”。而Thrift则可以省去“key”这个多余的字符串。\n\n​    定义thrift的结构里的属性名称实际上在thrift数据二进制编解码是被忽略的（thrift的json编解码未验证），这个名称的作用只是作为生成的OC代码类的属性名称。这也解释了为什么Thrift的二进制编码会比平时使用的json更省流量。同时也说明了只要我们在.thrift文件中定义struct的时候保证struct的属性的顺序不变，即使通信双方使用了各自使用不同的属性名称也不会有问题。\n\n​    随着请求并发量的提高，简单的HTTP肯定达不到预期的效果，Thrift或许才是你寻找的答案。\n\n​    \n","tags":["微服务和RPC框架"]},{"title":"微服务和RPC框架","url":"/2018/02/23/pout/python中高级面试题/微服务和RPC框架/微服务和RPC框架/","content":"\n\n<!-- toc -->\n\n# 微服务和RPC框架\n\n什么是微服务\n\n一般情况下，业务应用我们都会采用模块化的分层式架构，所有的业务逻辑代码最终会在一个代码库中并统一部署，我们称这种应用架构为单体应用。 单体应用的问题是，全部开发人员会共享一个代码库，不同模块的边界模糊，实现高内聚、松耦合极其困难。 肯定大家会碰到过这类场景，当尝试去重构改进代码时，改了一个地方好几个其他模块也需要同步改动， 当初划分的模块边界轻易被穿透，有人给这种应用的架构起了一个很形象的名字叫 “洋葱架构”。\n\n特点：\n\n1、服务注册、服务发现、健康检查\n\n如果我们采用进程内LB方案，那么服务自注册一般统一做在服务器端框架中，健康检查逻辑由具体业务服务定制，框架层提供调用健康检查逻辑的机制，服务发现和负载均衡则集成在服务客户端框架中。\n\n2、RPC/REST和序列化\n\n框架层要支持将业务逻辑以HTTP/REST或者RPC方式暴露出来，HTTP/REST是当前主流API暴露方式，在性能要求高的场合则可采用Binary/RPC方式。针对当前多样化的设备类型(浏览器、普通PC、无线设备等)，框架层要支持可定制的序列化机制，例如，对浏览器，框架支持输出Ajax友好的JSON消息格式，而对无线设备上的Native App，框架支持输出性能高的Binary消息格式。\n\n3、管理接口\n\n框架集成管理接口，一方面可以在线查看框架和服务内部状态，同时还可以动态调整内部状态，对调试、监控和管理能提供快速反馈。Spring Boot微框架的Actuator模块就是一个强大的管理接口。对于框架层和服务的内部异常，如果框架层能够统一处理并记录日志，对服务监控和快速问题定位有很大帮助。\n\n4、安全控制\n\n安全和访问控制逻辑可以在框架层统一进行封装，可做成插件形式，具体业务服务根据需要加载相关安全插件。\n\n5、配置管理\n\n除了支持普通配置文件方式的配置，框架层还可集成动态运行时配置，能够在运行时针对不同环境动态调整服务的参数和配置。\n\n6、监控日志\n\n框架一方面要记录重要的框架层日志、服务调用链数据，还要将日志、调用链数据等接口暴露出来，让业务层能根据需要记录业务日志数据。在运行环境中，所有日志数据一般由日志系统做进一步分析和处理。\n\n7、统一错误处理\n\n对于框架层和服务的内部异常，如果框架层能够统一处理并记录日志，对服务监控和快速问题定位有很大帮助。\n\n8、流控和容错\n\n框架集成限流容错组件，能够在运行时自动限流和容错，保护服务，如果进一步和动态配置相结合，还可以实现动态限流和熔断。\n\n# RPC框架\n\nRPC=Remote Produce Call 是一种技术的概念名词. HTTP是一种协议,RPC可以通过HTTP来实现,也可以通过Socket自己实现一套协议来实现.所以楼主可以换一个问法,为何RPC还有除HTTP 之外的实现法,有何必要.毕竟除了HTTP实现外,私有协议不具备通用性.那么我想唯一的答案就在于HTTP不能满足其业务场景的地方,所以这个就要具体 案例具体分析了.\n\nhttp接口是在接口不多、系统与系统交互较少的情况下，解决信息孤岛初期常使用的一种通信手段；优点就是简单、直接、开发方便。利用现成的http协议 进行传输。但是如果是一个大型的网站，内部子系统较多、接口非常多的情况下，RPC框架的好处就显示出来了，首先就是长链接，不必每次通信都要像http 一样去3次握手什么的，减少了网络开销；其次就是RPC框架一般都有注册中心，有丰富的监控管理；发布、下线接口、动态扩展等，对调用方来说是无感知、统 一化的操作。第三个来说就是安全性。最后就是最近流行的服务化架构、服务化治理，RPC框架是一个强力的支撑\n\nRPC是一个软件结构概念，是构建分布式应用的理论基础。就好比为啥你家可以用到发电厂发出来的电？是因为电是可以传输的。至于用铜线还是用铁丝还是其他 种类的导线，也就是用http还是用其他协议的问题了。这个要看什么场景，对性能要求怎么样。比如在java中的最基本的就是RMI技术，它是java原 生的应用层分布式技术。我们可以肯定的是在传输性能方面，RMI的性能是优于HTTP的。那为啥很少用到这个技术？那是因为用这个有很多局限性，首先它要 保证传输的两端都要要用java实现，且两边需要有相同的对象类型和代理接口，不需要容器，但是加大了编程的难度，在应用内部的各个子系统之间还是会看到 他的身影，比如EJB就是基于rmi技术的。这就与目前的bs架构的软件大相径庭。用http必须要服务端位于http容器里面，这样减少了网络传输方面 的开发，只需要关注业务开发即可。所以在架构一个软件的时候，不能一定根据需求选定技术。\n\n# Thrift\n\nThrift实际上是实现了C/S模式，通过代码生成工具将接口定义文件生成服务器端和客户端代码（可以为不同语言），从而实现服务端和客户端跨语言的支持。用户在Thirft描述文件中声明自己的服务，这些服务经过编译后会生成相应语言的代码文件，然后用户实现服务（客户端调用服务，服务器端提服务）便可以了。其中protocol（协议层, 定义数据传输格式，可以为二进制或者XML等）和transport（传输层，定义数据传输方式，可以为TCP/IP传输，内存共享或者文件共享等）被用作运行时库。\n\nThrift最初由facebook开发，07年四月开放源码，08年5月进入apache孵化器。\n\nApache Thrift 是一款跨语言的服务框架，传输数据采用二进制格式，相对 XML 和 JSON 体积更小，对于高并发、大数据量和多语言的环境更有优势。\n\n![img](img/wei1.png)\n\n数据类型\n\nThrift 支持 8 种数据类型：\n\nbool: true or false\n\nbyte: signed byte\n\ni16/i32/i64: 16/32/64位 signed integer\n\ndouble: 64位\n\nbinary: byte array\n\nstring\n\n3 种容器：\n\nlist: 排序数组，可以重复\n\nset: 集合，每个元素唯一\n\nmap: t1 唯一\n\n# 什么是Thrift\n\nThrift是一种接口描述语言和二进制通讯协议，它被用来定义和创建跨语言的服务，这是维基百科的描述。简单来说就是你可以按照Thrift定义语法编写.thrift,然后用Thrift命令行生成各种语言的代码，比如OC、Java、C++、JS，调用这些代码就可以完成客户端与服务器的通信了，不需要自己去写网络请求、数据解析等接口。更多详情可以通过这里了解。\n\n# 为什么使用Thrift\n\n在实际项目中主要考虑到这两个优点：\n\nRPC。通过简单定义Thrift描述语言文件，使用Thrift -gen命令可以生成多种语言的代码，这些代码包含了网络通信,数据编解码的功能。这就免去了前后台编写这部分繁琐的代码，同时也统一了前后台的实现逻辑。\n\nThrift的二进制数据的编码比json更加紧凑、减少了无用的数据的传输。这也是本文讨论的重点。\n\n# ThriftPy2是Apache Thrift 的纯 python 实现\n\n```\npip3 install thriftpy2\n```\n","tags":["微服务和RPC框架"]},{"title":"[PWA实践]serviceWorker生命周期、请求代理与通信","url":"/2018/02/11/pout/PWA实践-serviceWorker生命周期、请求代理与通信/","content":"\n本文主要讲 serviceWorker 生命周期和挂载、卸载等问题，适合对 serviceWorker 的作用有所了解但是具体细节不是特别清楚的读者\n\n**以下所有分析基于 Chrome V63**\n\n### serviceWorker的挂载\n\n先来一段代码感受serviceWorker注册:\n\n```\nif ('serviceWorker' in navigator) {\n      window.addEventListener('load', function () {\n          navigator.serviceWorker.register('/sw.js', {scope: '/'})\n              .then(function (registration) {\n                  // 注册成功\n                  console.log('ServiceWorker registration successful with scope: ', registration.scope);\n              })\n              .catch(function (err) {\n                  // 注册失败:(\n                  console.log('ServiceWorker registration failed: ', err);\n              });\n      });\n}\n```\n通过上述代码，我们定义在`/sw.js`里的内容就会生效(对于当前页面之前没有 serviceWorker 的情况而言，我们注册的 serviceWorker 肯定会生效，如果当前页面已经有了我们之前注册的 serviceWorker，这个时候涉及到 serviceWorker的更新机制，下文详述)\n\n如果我们在`sw.js`没有变化的情况下刷新这个页面，每次还是会有注册成功的回调以及相应的log输出，但是这个时候浏览器发现我们的 serviceWorker 并没有发生变化，并不会重置一遍 serviceWorker\n\n### serviceWorker更新\n\n我们如果想更新一个 serviceWorker，根据我们的一般web开发策略，可能会想到以下几种策略：\n\n* 仅变更文件名(比如把`sw.js`变成`sw-v2.js`或者加一个hash)\n* 仅变更文件内容(仅仅更新`sw.js`的内容，文件名不变)\n* 同时变更：同时执行以上两条\n\n在这里，我可以很负责的告诉你，**变更serviceWorker文件名绝对不是一个好的实践**，浏览器判断 serviceWorker 是否相同基本和文件名没有关系，甚至有可能还会造成浏览器抛出404异常(因为找不到原来的文件名对应的文件了)。\n\n所以我们只需要变更内容即可，实际上，我们每次打开或者刷新该页面，浏览器都会重新请求一遍 serviceWorker 的定义文件，如果发现文件内容和之前的不同了，这个时候:\n\n(*下文中，我们使用“有关 tab”来表示受 serviceWorker 控制的页面*，刷新均指普通刷新(F5/CommandR)并不指Hard Reload)\n\n* 这个新的 serviceWorker 就会进入到一个 “waiting to activate” 的状态，并且只要我们不关闭这个网站的所有tab(更准确地说，是这个 serviceWorker 控制的所有页面)，新的 serviceWorker 始终不会进入替换原有的进入到 running 状态(就算我们只打开了一个有关 tab，直接刷新也不会让新的替换旧的)。\n\n* 如果我们多次更新了 serviceWorker 并且没有关闭当前的 tab 页面，那么新的 serviceWorker 就会挤掉原先处于第二顺位(waiting to activate)的serviceWorker，变成`waiting to activate`状态\n\n也就是说，我们只有关闭当前旧的 serviceWorker 控制的所有页面 的所有tab，之后浏览器才会把旧的 serviveWorker 移除掉，换成新的，再打开相应的页面就会使用新的了。\n\n当然，也有一个特殊情况：如果我们在新的 serviceWorker 使用了`self.skipWaiting();`，像这样：\n\n```\nself.addEventListener('install', function(event) {\n    self.skipWaiting();\n});\n```\n\n这个时候，要分为以下两种情况：\n\n* 如果当前我们只打开了一个有关 tab，这个时候，我们直接刷新，发现新的已经替换掉旧的了。\n* 如果我们当前打开了若干有关 tab，这个时候，无论我们刷新多少次，新的也不会替换掉旧的，只有我们一个一个关掉tab(或者跳转走)只剩下最后一个了，这个时候刷新，会让新的替换旧的(也就是上一种情况)\n\nChrome 的这种机制，防止了同一个页面先后被新旧两个不同的 serviceWorker 接管的情况出现。\n\n#### 手动更新\n\n虽然说，在页面每次进入的时候浏览器都会检查一遍 serviceWorker 是否更新，但如果我们想要手动更新 serviceWorker 也没有问题：\n\n```\nnavigator.serviceWorker.register(\"/sw.js\").then(reg => {\n  reg.update();\n  // 或者 一段时间之后更新\n});\n```\n\n这个时候如果 serviceWorker 变化了，那么会重新触发 install 执行一遍 install 的回调函数，如果没有变，就不会触发这个生命周期。\n\n#### install 生命周期钩子\n\n我们一般会在 sw.js 中，添加`install`的回调，一般在回调中，我们会进行缓存处理操作，像这样：\n\n```\nself.addEventListener('install', function(event) {\n    console.log('[sw2] serviceWorker Installed successfully', event)\n\n    event.waitUntil(\n        caches.open('mysite-static-v1').then(function(cache) {\n            return cache.addAll([\n                '/stylesheets/style.css',\n                '/javascripts/common.39c462651d449a73b5bb.js',\n            ]);\n        })\n    )\n}    \n```\n\n如果我们新打开一个页面，如果之前有 serviceWorker，那么会触发`install`，如果之前没有， 那么在 serviceWorker 装载后会触发 `install`。\n\n如果我们刷新页面，serviceWorker 和之前没有变化或者 serviceWorker 已经处在 `waiting to activate`，不会触发`install`，如果有变化，会触发`install`，但不会接管页面(上文中提到)。\n\n#### activate 生命周期钩子\n\nactivate 在什么时候被触发呢？\n\n如果当前页面没有 serviceworker ，那么会在 install 之后触发。\n\n如果当前页面有 serviceWorker，并且有 serviceWorker更新，新的 serviceWorker 只会触发 install ，不会触发 activate\n\n换句话说，当前变成 active 的 serviceWorker 才会被触发这个生命周期钩子\n\n\n### serviceWorker 代理请求\n\nserviceWorker 代理请求相对来说比较好理解，以下是一个很简单的例子：\n\n```\nself.addEventListener('install', function(event) {\n    console.log('[sw2] serviceWorker Installed successfully', event)\n\n    event.waitUntil(\n        caches.open('mysite-static-v1').then(function(cache) {\n            return cache.addAll([\n                '/stylesheets/style.css',\n                '/javascripts/common.39c462651d449a73b5bb.js',\n            ]);\n        })\n    );\n});\n\nself.addEventListener('fetch', function(event) {\n    console.log('Handling fetch event for', event.request.url);\n    // console.log('[sw2]fetch but do nothing')\n\n    event.respondWith(\n        // caches.match() will look for a cache entry in all of the caches available to the service worker.\n        // It's an alternative to first opening a specific named cache and then matching on that.\n        caches.match(event.request).then(function(response) {\n            if (response) {\n                console.log('Found response in cache:', response);\n\n                return response;\n            }\n\n            console.log('No response found in cache. About to fetch from network...');\n\n            // event.request will always have the proper mode set ('cors, 'no-cors', etc.) so we don't\n            // have to hardcode 'no-cors' like we do when fetch()ing in the install handler.\n            return fetch(event.request).then(function(response) {\n                console.log('Response from network is:', response);\n\n                return response;\n            }).catch(function(error) {\n                // This catch() will handle exceptions thrown from the fetch() operation.\n                // Note that a HTTP error response (e.g. 404) will NOT trigger an exception.\n                // It will return a normal response object that has the appropriate error code set.\n                console.error('Fetching failed:', error);\n\n                throw error;\n            });\n        })\n    );\n});\n```\n\n有两点要注意的：\n\n我们如果这样代理了，哪怕没有 cache 命中，实际上也会在控制台写from serviceWorker，而那些真正由serviceWorker发出的请求也会显示，有一个齿轮图标，如下图：\n\n![](https://www.10000h.top/images/sw_1.png)\n\n第二点就是我们如果在 fetch 的 listener 里面 do nothing， 也不会导致这个请求直接假死掉的。\n\n另外，通过上面的代码我们发现，实际上由于现在我们习惯给我们的文件资源加上 hash，所以我们基本上不可能手动输入需要缓存的文件列表，现在大多数情况下，我们都是借助 webpack 插件，完成这部分工作。\n\n### serviceWorker 和 页面之间的通信\n\nserviceWorker向页面发消息：\n\n```\nsw.js:\n\nself.clients.matchAll().then(clients => {\n    clients.forEach(client => {\n        console.log('%c [sw message]', 'color:#00aa00', client)\n        client.postMessage(\"This message is from serviceWorker\")\n    })\n})\n\n主页面:\n\nnavigator.serviceWorker.addEventListener('message', function (event) {\n    console.log('[Main] receive from serviceWorker:', event.data, event)\n});\n```\n\n当然，这里面是有坑的：\n\n* 主界面的事件监听需要等serviceWorker注册完毕后，所以一般`navigator.serviceWorker.register`的回调到来之后再进行注册(或者延迟足够的时间)。\n* 如果在主界面事件监听还没有注册成功的时候 serviceWorker 发送消息，自然是收不到的。如果我们把 serviceWorker 直接写在 install 的回调中，也是不能被正常收到的。\n\n从页面向 serviceWorker 发送消息：\n\n```\n主页面:\n\nnavigator.serviceWorker.controller && navigator.serviceWorker.controller.postMessage('hello serviceWorker');\n\nsw.js:\nself.addEventListener('message', function (event) {\n    console.log(\"[sw from main]\",event.data); // 输出：'sw.updatedone'\n});\n```\n\n同样的，这也要求主界面的代码需要等到serviceWorker注册完毕后触发，另外还有一点值得注意， serviceWorker 的事件绑定代码要求主界面的serviceWorker已经注册完毕后才可以。\n\n也就是说，如果当前页面没有该serviceWorker 第一次注册是不会收到主界面接收到的消息的。\n\n记住，只有当前已经在 active 的 serviceWorker， 才能和主页面收发消息等。\n\n**以上就是和 serviceWorker 有关的一些内容，在下一篇文章中，我会对PWA 添加至主屏幕等功能进行总结**\n\n","tags":["PWA"]},{"title":"上下文管理","url":"/2018/01/23/pout/python中高级面试题/上下文管理/","content":"\n\n<!-- toc -->\n\n# 上下文管理(with)\n\nwith是一种上下文管理协议，目的在于从流程图中把 try,except 和finally 关键字和资源分配释放相关代码统统去掉，简化try….except….finlally的处理流程。with通过enter方法初始化，然后在exit中做善后以及处理异常。所以使用with处理的对象必须有**enter**()和**exit**()这两个方法。其中**enter**()方法在语句体（with语句包裹起来的代码块）执行之前进入运行，exit()方法在语句体执行完毕退出后运行。 with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。\n\nwith的使用场景 如果某项工作完成后需要有释放资源或者其他清理工作，比如说文件操作时，就可以使用with优雅的处理，不用自己手动关闭文件句柄，而且with还能很好的管理上下文异常。\n\n```\nwith open('a.py') as fp:\n      for line in fp:\n            print(line,end=\" \")\n```\n\nwith工作原理 with后面的语句被求值后，该语句返回的对象的**enter**()方法被调用，这个方法将返回的值赋给as后面的变量，当with包围的语句块全部执行完毕后，自动调用对象的**exit**()方法。 with处理异常会更方便，省去try…else…finally复杂的流程，这个用到的就是对象的**exit**()方法： exit( exc_type, exc_value, exc_trackback) 后面三个参数是固定的，用来接收with执行过程中异常类型，异常名称和异常的详细信息\n\n其他场景应用\n\n```\nimport contextlib\n\n@contextlib.contextmanager\ndef database():\n    db = Database()\n    try:\n        if not db.connected:\n            db.connect()\n        yield db\n    except Exception as e:\n        db.close()\n\ndef handle_query():\n    with database() as db:\n        print 'handle ---', db.query()\n```\n","tags":["python中高级面试题"]},{"title":"python特殊方法","url":"/2018/01/03/pout/python中高级面试题/python特殊方法/","content":"\n\n<!-- toc -->\n\n# python特殊方法(magic方法也叫魔术方法)\n\n构造和初始化\n\n```\n每个人都知道一个最基本的魔术方法， __init__ 。\n\n通过此方法我们可以定义一个对象的初始操作。\n\n然而，当我调用 x = SomeClass() 的时候， __init__ 并不是第一个被调用的方法。\n\n实际上，还有一个叫做 __new__ 的方法，来构造这个实例。\n\n然后给在开始创建时候的初始化函数来传递参数。\n\n在对象生命周期的另一端，也有一个 __del__ 方法。我们现在来近距离的看一看这三个方法:\n\n__new__(cls, [...) __new__ 是在一个对象实例化的时候所调用的第一个方法。它的第一个参数是这个类，其他的参数是用来直接传递给 __init__ 方法。 __new__ 方法相当不常用,但是它有自己的特性，特别是当继承一个不可变的类型比如一个tuple或者string。我不希望在 __new__ 上有太多细节，因为并不是很有用处，但是在 Python文档 中有详细的阐述。\n\n__init__(self, […) 此方法为类的初始化方法。当构造函数被调用的时候的任何参数都将会传给它。(比如如果我们调用 x = SomeClass(10, 'foo'))，那么 __init__ 将会得到两个参数10和foo。 __init__ 在Python的类定义中被广泛用到。\n\n__del__(self) 如果 __new__ 和 __init__ 是对象的构造器的话，那么 __del__ 就是析构器。它不实现语句 del x (以上代码将不会翻译为 x.__del__() )。它定义的是当一个对象进行垃圾回收时候的行为。当一个对象在删除的时需要更多的清洁工作的时候此方法会很有用，比如套接字对象或者是文件对象。注意，如果解释器退出的时候对象还存存在，就不能保证 __del__ 能够被执行，所以 __del__ can’t serve as a replacement for good coding practices ()~~~~~~~\n\n放在一起的话，这里是一个 __init__ 和 __del__\n```\n\n实际使用的例子\n\n```\nfrom os.path import join\n\nclass FileObject:\n    '''给文件对象进行包装从而确认在删除时文件流关闭'''\n\n    def __init__(self, filepath='~', filename='sample.txt'):\n        #读写模式打开一个文件\n        self.file = open(join(filepath, filename), 'r+')\n\n    def __del__(self):\n        self.file.close()\n        del self.file\n```\n","tags":["python中高级面试题"]},{"title":"进程线程协程","url":"/2017/12/23/pout/python中高级面试题/进程线程协程/","content":"\n\n<!-- toc -->\n\n# Python中 进程 线程 协程 (多任务概念/重点)\n\n## 进程，是执行中的计算机程序。也就是说，每个代码在执行的时候，首先本身即是一个进程。\n\n\n\n一个进程具有:就绪，运行，中断，僵死，结束等状态(不同操作系统不一样)。\n\n生命周期：\n\n用户编写代码(代码本身是以进程运行的)\n\n启动程序，进入进程“就绪”状态\n\n操作系统调度资源，做“程序切换”，使得进程进入“运行”状态\n\n结束/中断\n\n特性\n\n每个程序，本身首先是一个进程\n\n运行中每个进程都拥有自己的地址空间、内存、数据栈及其它资源。\n\n操作系统本身自动管理着所有的进程(不需要用户代码干涉)，并为这些进程合理分配可以执行时间。\n\n进程可以通过派生新的进程来执行其它任务，不过每个进程还是都拥有自己的内存和数据栈等。\n\n进程间可以通讯(发消息和数据)，采用 进程间通信(IPC) 方式。\n\n说明\n\n多个进程可以在不同的 CPU 上运行，互不干扰\n\n同一个CPU上，可以运行多个进程，由操作系统来自动分配时间片\n\n由于进程间资源不能共享，需要进程间通信，来发送数据，接受消息等\n\n多进程，也称为“并行”。\n\n### 进程间通信\n\n进程彼此之间互相隔离，要实现进程间通信（IPC），multiprocessing模块支持两种形式：队列和管道，这两种方式都是使用消息传递的。\n\n### 进程队列queue\n\n不同于线程queue，进程queue的生成是用multiprocessing模块生成的。\n\n在生成子进程的时候，会将代码拷贝到子进程中执行一遍，及子进程拥有和主进程内容一样的不同的名称空间。\n\nmultiprocess.Queue 是跨进程通信队列\n\n常用方法\n\n```\nq.put方法用以插入数据到队列中，put方法还有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，该方法会阻塞timeout指定的时间，直到该队列有剩余的空间。如果超时，会抛出Queue.Full异常。如果blocked为False，但该Queue已满，会立即抛出Queue.Full异常。\nq.get方法可以从队列读取并且删除一个元素。同样，get方法有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，那么在等待时间内没有取到任何元素，会抛出Queue.Empty异常。如果blocked为False，有两种情况存在，如果Queue有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出Queue.Empty异常.\nq.get_nowait():同q.get(False)\nq.put_nowait():同q.put(False)\nq.empty():调用此方法时q为空则返回True，该结果不可靠，比如在返回True的过程中，如果队列中又加入了项目。\nq.full()：调用此方法时q已满则返回True，该结果不可靠，比如在返回True的过程中，如果队列中的项目被取走。\nq.qsize():返回队列中目前项目的正确数量，结果也不可靠，理由同q.empty()和q.full()一样\n```\n\n### 管道pipe\n\n管道就是管道，就像生活中的管道，两头都能进能出\n\n默认管道是全双工的，如果创建管道的时候映射成False，左边只能用于接收，右边只能用于发送，类似于单行道\n\n```\nimport multiprocessing\n\ndef foo(sk):\n    sk.send('hello world')\n    print(sk.recv())\n\nif __name__ == '__main__':\n    conn1,conn2=multiprocessing.Pipe()    #开辟两个口，都是能进能出，括号中如果False即单向通信\n    p=multiprocessing.Process(target=foo,args=(conn1,))  #子进程使用sock口，调用foo函数\n    p.start()\n    print(conn2.recv())  #主进程使用conn口接收\n    conn2.send('hi son') #主进程使用conn口发送\n```\n\n常用方法\n\n```\nconn1.recv():接收conn2.send(obj)发送的对象。如果没有消息可接收，recv方法会一直阻塞。如果连接的另外一端已经关闭，那么recv方法会抛出EOFError。\nconn1.send(obj):通过连接发送对象。obj是与序列化兼容的任意对象\n注意：send()和recv()方法使用pickle模块对对象进行序列化\n```\n\n### 共享数据manage\n\nQueue和pipe只是实现了数据交互，并没实现数据共享，即一个进程去更改另一个进程的数据。\n\n注：进程间通信应该尽量避免使用共享数据的方式\n\n### 进程池\n\n开多进程是为了并发，通常有几个cpu核心就开几个进程，但是进程开多了会影响效率，主要体现在切换的开销，所以引入进程池限制进程的数量。\n\n进程池内部维护一个进程序列，当使用时，则去进程池中获取一个进程，如果进程池序列中没有可供使用的进进程，那么程序就会等待，直到进程池中有可用进程为止。\n\n## 线程\n\n线程，是在进程中执行的代码。\n\n一个进程下可以运行多个线程，这些线程之间共享主进程内申请的操作系统资源。\n\n在一个进程中启动多个线程的时候，每个线程按照顺序执行。现在的操作系统中，也支持线程抢占，也就是说其它等待运行的线程，可以通过优先级，信号等方式，将运行的线程挂起，自己先运行。\n\n使用\n\n用户编写包含线程的程序(每个程序本身都是一个进程)\n\n操作系统“程序切换”进入当前进程\n\n当前进程包含了线程，则启动线程\n\n多个线程，则按照顺序执行，除非抢占\n\n特性\n\n线程，必须在一个存在的进程中启动运行\n\n线程使用进程获得的系统资源，不会像进程那样需要申请CPU等资源\n\n线程无法给予公平执行时间，它可以被其他线程抢占，而进程按照操作系统的设定分配执行时间\n\n每个进程中，都可以启动很多个线程\n\n说明\n\n多线程，也被称为”并发“执行。\n\n### 线程池\n\n系统启动一个新线程的成本是比较高的，因为它涉及与操作系统的交互。在这种情形下，使用线程池可以很好地提升性能，尤其是当程序中需要创建大量生存期很短暂的线程时，更应该考虑使用线程池。\n\n线程池在系统启动时即创建大量空闲的线程，程序只要将一个函数提交给线程池，线程池就会启动一个空闲的线程来执行它。当该函数执行结束后，该线程并不会死亡，而是再次返回到线程池中变成空闲状态，等待执行下一个函数。\n\n此外，使用线程池可以有效地控制系统中并发线程的数量。当系统中包含有大量的并发线程时，会导致系统性能急剧下降，甚至导致 Python 解释器崩溃，而线程池的最大线程数参数可以控制系统中并发线程的数量不超过此数。\n\n### 多线程通信\n\n#### 共享变量\n\n创建全局变量，多个线程公用一个全局变量，方便简单。但是坏处就是共享变量容易出现数据竞争，不是线程安全的，解决方法就是使用互斥锁。\n\n#### 变量共享引申出线程同步问题\n\n如果多个线程共同对某个数据修改，则可能出现不可预料的结果，为了保证数据的正确性，需要对多个线程进行同步。 使用Thread对象的Lock和Rlock可以实现简单的线程同步，这两个对象都有acquire方法和release方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到acquire和release方法之间。\n\n#### 队列\n\n线程间使用队列进行通信，因为队列所有方法都是线程安全的，所以不会出现线程竞争资源的情况\n\nQueue.Queue 是进程内非阻塞队列\n\n## 进程和线程的区别\n\n一个进程中的各个线程与主进程共享相同的资源，与进程间互相独立相比，线程之间信息共享和通信更加容易(都在进程中，并且共享内存等)。\n\n线程一般以并发执行，正是由于这种并发和数据共享机制，使多任务间的协作成为可能。\n\n进程一般以并行执行，这种并行能使得程序能同时在多个CPU上运行;\n\n区别于多个线程只能在进程申请到的的“时间片”内运行(一个CPU内的进程，启动了多个线程，线程调度共享这个进程的可执行时间片)，进程可以真正实现程序的“同时”运行(多个CPU同时运行)。\n\n进程和线程的常用应用场景\n\n一般来说,在Python中编写并发程序的经验:\n\n计算密集型任务使用多进程\n\nIO密集型(如:网络通讯)任务使用多线程，较少使用多进程.\n\n这是由于 IO操作需要独占资源，比如:\n\n网络通讯(微观上每次只有一个人说话，宏观上看起来像同时聊天)每次只能有一个人说话\n\n文件读写同时只能有一个程序操作(如果两个程序同时给同一个文件写入 'a', 'b'，那么到底写入文件的哪个呢?)\n\n都需要控制资源每次只能有一个程序在使用，在多线程中，由主进程申请IO资源，多线程逐个执行，哪怕抢占了，也是逐个运行，感觉上“多线程”并发执行了。\n\n如果多进程，除非一个进程结束，否则另外一个完全不能用，显然多进程就“浪费”资源了。\n\n当然如上解释可能还不足够立即理解问题所在，让我们通过不断的实操来体验其中的“门道”。\n\n## 协程\n\n协程: 协程，又称微线程，纤程，英文名Coroutine。协程的作用，是在执行函数A时，可以随时中断，去执行函数B，然后中断继续执行函数A（可以自由切换）。但这一过程并不是函数调用（没有调用语句），这一整个过程看似像多线程，然而协程只有一个线程执行.\n\n协程由于由程序主动控制切换，没有线程切换的开销，所以执行效率极高。对于IO密集型任务非常适用，如果是cpu密集型，推荐多进程+协程的方式。\n\n协程，又称微线程。\n\n说明\n\n协程的主要特色是:\n\n协程间是协同调度的，这使得并发量数万以上的时候，协程的性能是远远高于线程。\n\n注意这里也是“并发”，不是“并行”。\n\n常用库：greenlet gevent\n\n协程优点：\n\n　　1. 协程的切换开销更小，属于程序级别的切换，操作系统完全感知不到，因而更加轻量级\n\n　　2. 单线程内就可以实现并发的效果，最大限度地利用cpu\n\n协程缺点：\n\n　　1.协程的本质是单线程下，无法利用多核，可以是一个程序开启多个进程，每个进程内开启多个线程，每个线程内开启协程\n\n　　2.协程指的是单个线程，因而一旦协程出现阻塞，将会阻塞整个线程\n\n## python中的协程\n\n一个协程是一个函数/子程序（可以认为函数和子程序是指一个东西）。这个函数可以暂停执行， 把执行权让给 YieldInstruction，等 YieldInstruction 执行完成后，这个函数可以继续执行。 这个函数可以多次这样的暂停与继续。\n\n注：这里的 YieldInstruction, 我们其实也可以简单理解为函数。\n\n协程可以在“卡住”的时候可以干其它事情。\n\n```\nasync def long_task():\n...     print('long task started')\n...     await asyncio.sleep(1)\n...     print('long task finished')\n...\n>>> loop.create_task(long_task())\n<Task pending coro=<long_task() running at <stdin>:1>>\n>>> loop.create_task(job1())                                                                                                                                                                                                                            >>>>> loop.create_task(job1())\n<Task pending coro=<job1() running at <stdin>:1>>\n>>>\n>>> try:\n...     loop.run_forever()\n... except KeyboardInterrupt:\n...     pass\n...\nlong task started\njob1 started...\njob1 paused\nhello world\njob1 resumed\njob1 finished\nlong task finished\n```\n\n从这段程序的输出可以看出，程序本来是在执行 long task 协程，但由于 long task 要 await sleep 1 秒，于是 long task 自动暂停了，hello_world 协程自动开始执行， hello world 执行完之后，long task 继续执行。\n\n协程有两种定义的方法， 其中使用生成器形式定义的协程叫做 generator-based coroutine, 通过 async/await 声明的协程叫做 native coroutine，两者底层实现都是生成器。接着， 我们阐述了协程的概念，从概念和例子出发，讲了协程和生成器最主要的特征：可以暂停执行和恢复执行。\n\n### 协程异常处理\n\n使用协程的时候一定加了很多的异常,但百密而一疏,总是会有想象不到的异常发生,这个时候为了不让程序整体崩溃应该使用协程的额外异常处理方法,这个方法会去执行绑定的回调函数.\n\n```\ng_dict=dict{}\ng = gevent.spawn(self._g_fetcher, feed_name)   # 创建协程\ng_dict[feed_name] = [g,False]\ng.link_exception(self._link_exception_callback)  # 给该协程添加出现处理不了的异常时候的回调函数\ndef _link_exception_callback(self, g):\n    # 可能遇到无法修复的错误，需要修改代码 todo 报警\n    # 可以在这个函数里面做一些错误异常的打印,或者文件的关闭,连接的关闭.\n    self.terminated_flag = True # 停止整个程序 让 supervior重启\n    logger.info(\"_link_exception_callback {0} {1}\".format(g, g.exception))\n    self._kill_sleep_gevent()   # 轮询结束休眠的协程\n\ndef _kill_sleep_gevent(self):\n    for i,is_sleep in g_dict.items():\n        if is_sleep[1] == \"True\":\n            gevent.kill(is_sleep[0])\n```\n\n### greenlet框架实现协程（封装yield的基础库）\n\ngreenlet机制的主要思想是：生成器函数或者协程函数中的yield语句挂起函数的执行，直到稍后使用next()或send()操作进行恢复为止。可以使用一个调度器循环在一组生成器函数之间协作多个任务。greentlet是python中实现我们所谓的\"Coroutine(协程)\"的一个基础库。\n\n### 基于greenlet框架的高级库gevent模块\n\ngevent是第三方库，通过greenlet实现协程，其基本思想是：\n\n当一个greenlet遇到IO操作时，比如访问网络，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行。由于IO操作非常耗时，经常使程序处于等待状态，有了gevent为我们自动切换协程，就保证总有greenlet在运行，而不是等待IO。\n\n# 原生协程\n\n协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存，在调度回来的时候，恢复先前保存的寄存器上下文和栈。因此协程能保留上一次调用时的状态，即所有局部状态的一个特定组合\n\nasync函数完全可以看作多个异步操作，包装成的一个 Promise 对象，而await命令就是内部then命令的语法糖\n\n```\nimport time\ndef job(t):\n    time.sleep(t) \n    print('用了%s' % t)\ndef main():\n    [job(t) for t in range(1,3)]\nstart = time.time()\nmain()\nprint(time.time()-start)\n\nimport time\nimport asyncio\nasync def job(t):  # 使用 async 关键字将一个函数定义为协程\n    await asyncio.sleep(t)  # 等待 t 秒, 期间切换执行其他任务\n    print('用了%s秒' % t)\nasync def main(loop):  # 使用 async 关键字将一个函数定义为协程\n    tasks = [loop.create_task(job(t)) for t in range(1,3)]  # 创建任务, 不立即执行\n    await asyncio.wait(tasks)  # 执行并等待所有任务完成\nstart = time.time()\nloop = asyncio.get_event_loop()  # 建立 loop\nloop.run_until_complete(main(loop))  # 执行 loop\nloop.close()  # 关闭 loop\n\nprint(time.time()-start)\n```\n","tags":["python中高级面试题"]},{"title":"CentOS安装node8.x版本","url":"/2017/12/15/centos/CentOS安装node8-x版本/","content":"### CentOS 安装 node 8.x 版本\n\n由于一些原因需要给CentOS服务器安装8.0以上版本的node, 本来直接通过yum管理安装管理，但是没找到好办法，在此记录一下自己最后使用的简单过程：\n\n安装之前删除原来的node和npm (我原来是用yum安装的，如果是第一次安装可以省略这一步):\n\n```\nyum remove nodejs npm -y\n```\n\n首先我们随便进入服务器的一个目录，然后从淘宝的源拉取内容:\n\n```\nwget https://npm.taobao.org/mirrors/node/v8.0.0/node-v8.0.0-linux-x64.tar.xz \n```\n\n解压缩:\n\n```\nsudo tar -xvf node-v8.0.0-linux-x64.tar.xz \n```\n\n进入解压目录下的 bin 目录，执行 ls 命令\n\n```\ncd node-v8.0.0-linux-x64/bin && ls \n```\n\n我们发现有node 和 npm\n\n这个时候我们测试:\n\n```\n./node -v\n```\n\n这个时候我们发现实际上已经安装好了，接下来就是要建立链接文件。\n\n这里还是，如果我们之前已经安装过了，那么我们要先删除之前建立的链接文件：\n\n```\nsudo rm -rf /usr/bin/node\nsudo rm -rf /usr/bin/npm\n```\n\n然后建立链接文件:\n\n```\nsudo ln -s /usr/share/node-v8.0.0-linux-x64/bin/node /usr/bin/node\nsudo ln -s /usr/share/node-v8.0.0-linux-x64/bin/npm /usr/bin/npm\n```\n\n注意这里的第一个路径不要直接复制粘贴，要写当前文件的真正的路径，这个可以通过pwd获取。\n\n然后我们可以通过`node -v`等测试已经安装成功。\n","tags":["Node.js"]},{"title":"深入浏览器web渲染与优化-续","url":"/2017/08/31/pout/后端技术/深入浏览器web渲染与优化-续/","content":">本篇文章接上一篇继续分析浏览器web渲染相关内容，但是更侧重优化工作。当然，主要还是基于X5来分析\n\n上一篇文章我们主要是从浏览器内核的线程角度来分析相关工作的，对整体流程没有宏观清晰的分析，这次我们从宏观到微观，从整体到局部，来进行分析和探究可以优化的地方。\n\n首先，一个网页的加载，需要什么工作呢？\n\n![](https://www.10000h.top/images/data_img/webRender2/P1.png)\n\n这个工作可以分为三部分：云(云端)、管(传输链路)、端(客户端)，从云经过管传到端，然后经过加载解析排版渲染，从而完成网页从请求到呈现的工作(当然，我们这里没有涉及协议的分析，实际上根据协议不同，这个传输可能是多次传输)。\n\n数据到端之后，又经过以下过程，才最终显示出来：\n\n![](https://www.10000h.top/images/data_img/webRender2/P2.png)\n\n在这个过程中，我们怎么衡量性能呢？\n\n固然，我们有诸多浏览器提供的API，这些API能让我们获取到较多信息并且记录上报：\n\n![](https://www.10000h.top/images/data_img/webRender2/P3.png)\n\n但是这些具体数值表达的含义有限，并且他们实际上也不等于用户体验。\n\n所以，找到一个科学并且可以检测的标准，并且这个标准可以和用户体验有正相关关系，这个是至关重要的。\n\n目前这个标准是**首屏时间**(就之前自己的了解，具体的还区分首屏展示时间和首屏可交互时间，但是这里讲师不做区分，就下文提供的测算方法而言，显然这里指的是首屏展示时间，*另外，展示后到用户的第一次操作都会有一个至少1s的延时，毕竟用户手指按下的动作是会比较慢的，这个时间js的交互都能完成了，所以首屏展示时间更加重要--from dorsywang*)\n\n那么**首屏时间**怎么测量呢？\n\n**拿摄像机快速拍照测量的**。这个答案可能有些吃惊，但是目前X5内核业务的相关开发人员的确就是采用这种方式测算的，通过高速相机不断拍照，然后辅助图像识别，判断首屏是否已经加载完成，最终再通过人工回归校对。  \n因为如果采用程序检测的话，基本上都会对过程本身造成一定的影响，所以没有采用这种方式。\n当然，通过摄像+图像识别的这种方式也是有一定的弊端，比如说，假设首屏有一个图片，而图片的加载通常比较慢并且不影响css、js的加载，这个时候直接通过图片识别的话就可能会有一定的误判。\n\n知道了怎么测算，那么接下来分析影响这个指标的一些原因：\n\n* 资源阻塞内核线程\n\n我们知道，一般情况下，css和JS是阻塞页面的，当然也会对首屏时间造成影响。\n\n对这个问题，X5内核有关键子资源(阻塞资源)缓存，这里的关键资源，指的是内核经过统计判断得出的业务常用的关键子资源。\n\n当然，这个统计也可能缺乏一定的准确性，所以相关团队也正在推进这方面的内容规范化(比如写入Web App Manifest)\n\n* 中文Layout的时间过长\n\n这个问题我之前没有听说过，但是的确是这样子，实际上，浏览器在绘制文字的时候经历的过程非常的多，其中有一个环节是找到文字的宽度和高度(因为在英文状态下，每一个字符的宽度是不同的，所以每一个字符都要查找，但是英文总共只有26个字符)，而中文由于字符比较多，常用得就有6000多个，完整的更是有2万个以上，所以这个过程需要花费更多的时间。\n\n为了解决这个问题，X5内核考虑到中文文字几乎都是等宽等高的，所以这个过程对一个文字串来说只需要查询一次即可，实际上是节约了这个环节。\n\n* 首次渲染太慢\n\n为了解决这个问题，可以采用先绘制首屏的方式，这个也就是基于第一篇文章中讲到的浏览器的分块渲染机制\n\n* 一次解析内容过多\n\n采用首屏探测机制，优先解析首屏内容。\n\n另外，这里可以前端配合去做首屏优化：\n\n\n在首屏的位置插入首屏标签，内核解析到标签后立即终止解析并且排版上屏\n\n```\n<meta name=‘x5-pagetype’ content=‘optpage'>\n```\n然后在首屏分界的地方：\n\n```\n<first-screen/>\n```\n\n有了这，可以专门去优化首屏标签之前的内容(这个标签前尽量展现耗时少和不需要阻塞解析的资源)。\n\n另外，X5内核也提供了主资源预拉取的接口，并且考虑到预拉取的cookie问题，还提供了preconnect预链接。  \n*TIP:主资源中关联的子资源预拉取不用主动调用*\n\n* 预先操作\n\n另外为了提供更加极致的优化，X5内核(QQ浏览器、手Q Webview)还提供了如下诸多预操作：\n\n* 在\"黏贴并转到\"之前就开始进行网络请求和预渲染\n* 经常访问的站点可以预解析DNS\n* 点击地址栏时进行搜索预连接\n* 点击链接时，先预链接，再做跳转。\n* ......\n\n### 其他方式优化\n\n实际上上文主要讲了客户端方面的优化工作，实际上对于\"云\"、\"管\"两端，还是有很多优化工作可以讲的，但是由于这个和前端关系不是特别密切，我挑一部分讲一讲。这些在我们前端做个人项目的后台时候也可以参考\n\n##### 后台提速\n\n* 直接使用IP，节省dns的查询时间\n* 维持长连接\n* HTTP1.1启用包头节省\n* 服务器缓存\n* 文本资源压缩传输GZIP(6)\n* 图片尺寸压缩、图片质量压缩、支持webp和sharpp/hevc格式。\n\n##### 降低网络时延\n\n* 就快接入和就近接入\n\n在选择接入点的时候，如果采用就近接入，可以保持路由稳定，有利于负载均衡，并且实现简单，便于维护。但是也有一定的缺点：经验判断，准确度不够高 ； 无法自动切换路由。\n\n相比较而言，选择就快接入，是一个能够提效的方式。\n\n##### 内容防劫持\n\n运营商劫持对我们来说已经是不陌生的话题了，但是X5内核有一个比较新的防劫持手段，就是客户端和云加速服务器同时采用轻量级http加密，虽然这种方式普适性不强，但是的确可以解决腾讯自身业务的防劫持问题。\n\n#### QUIC和http2\n\nQUIC 基于UDP的协议通讯方式，有这些优势：\n\n* 延迟少\n* 前向纠错\n* 没有**线头阻塞[注1]**的多路复用\n* 通信通道的定义基于ID而不是IP+端口，使得切换网络后继续转发链接成为可能\n\n——————\n\n注1：线头阻塞：\n\n![](https://www.10000h.top/images/data_img/webRender2/P4.png)\n\n——————\n\n附1: 带宽和延迟对网页加载的影响：\n\n![](https://www.10000h.top/images/data_img/webRender2/X1.png)\n","tags":["性能优化"]},{"title":"深入浏览器web渲染与优化","url":"/2017/08/27/pout/后端技术/深入浏览器web渲染与优化/","content":">本文主要分析和总结web内核渲染的相关内容，以及在这方面前端可以做的性能优化工作。\n\n文章主要分为以下几个部分：\n\n* blink内核的渲染机制\n* chrome内核架构变迁\n* 分层渲染\n* 动画 & canvas & WebGl\n\n*这里的前两部分可能会有些枯燥，如果是前端工程师并且想立即获得实际项目的建议的，可以直接阅读第三部分和第四部分*\n\n### blink内核的渲染机制\n\nblink内核是Google基于Webkit内核开发的新的分支，而实际上，目前Chrome已经采用了blink内核，所以，我们接下来的有关分析大多基于blink内核的浏览器(Chrome)，就不再详细指明，当然，部分内容也会涉及到腾讯研发的X5内核(X5内核基于安卓的WebView，目前已经在手机QQ等产品中使用，基于X5内核的项目累计有数亿UV，上百亿PV)。\n\n一个页面的显示，实际上主要经历了下面的四个流程：\n\n加载 => 解析 => 排版 => 渲染\n\n实际上，这里的渲染主要是指排版之后到最后的上屏绘制(这个时候内容已经排版好了)，一部分前端工程师通常会把一部分的排版工作理解到“渲染”的流程中(也就是下图中全部工作)，实际上这个理解是不准确的。\n\n![](https://www.10000h.top/images/data_img/webRender/P6.PNG)\n\n目前，浏览器的渲染采用的是分块渲染的机制，所谓的分块渲染的机制，其实应该这么理解：\n\n* 浏览器首先把整个网页分成一些低分辨率的块，再把网页分成高分辨率的块，然后给这些块排列优先级。\n* 处在可视区域内的低分辨率块的优先级会比较高，会被较先绘制。\n* 之后浏览器会把高分辨率的块进行绘制，同样也是先绘制处于可视区域内的，再绘制可视区域外的(由近到远)。\n\n以上讲的这些策略可以使可以使得浏览器优先展示可视区域内的内容，并且先展示大致内容，再展示高精度内容(当然，由于这个过程比较快，实际上我们大多时候是感受不到的)。\n\n另外这里值得提醒的一点是，分块的优先级是会根据到可视区域的距离来决定的，所以有些横着的内容(比如banner的滚动实现，通常会设置横向超出屏幕来表示隐藏)，也是会按照到可视区域的距离来决定优先级的。\n\n绘制的过程，可以被硬件加速，这里硬件加速的主要手段主要是指：\n\n* 硬件加速合成上屏\n* 2D Canvas、Video的硬件加速\n* GPU光栅化\n\t* GPU光栅化速度更快，内存和CPU的消耗更少\n\t* 目前还没有办法对包含复杂矢量绘制的页面进行GPU光栅化\n\t* GPU光栅化是未来趋势\n\n\n### chrome内核架构变迁\n\n在渲染架构上，chrome也是经历了诸多变迁，早期的Chrome是这样的：\n\n![](https://www.10000h.top/images/data_img/webRender/P1.PNG)\n\n早期的chrome的架构实际上有以下缺点：\n\n* Renderer线程任务繁重\n* 无法实时响应缩放滑动操作\n* 脏区域与滑动重绘区域有冲突\n\t* 这里举个场景，假设一个gif，这个时候如果用户滑动，滑动新的需要绘制的内容和gif下一帧内容就会产生绘制冲突\n\n当然，经过一系列的发展，Chrome现在是这样的：\n\n![](https://www.10000h.top/images/data_img/webRender/P2.PNG)\n\n在安卓上，Android 4.4的 Blink内核架构如下(4.4之前并不支持OpenGL)\n\n![](https://www.10000h.top/images/data_img/webRender/P3.PNG)\n\n当然，这种架构也有如下缺点：\n\n* UI线程过于繁忙\n* 无法支持Canvas的硬件加速以及WebGL\n\n所以，后期发展成了这样：\n\n![](https://www.10000h.top/images/data_img/webRender/P4.PNG)\n\n总结看来，内核发展的趋势是：\n\n* 多线程化(可以充分利用多核心CPU)\n* 硬件加速(可以利用GPU)\n\n### 分层渲染\n\n在阅读这一章之前，我建议读者先去亲自体验一下所谓的“分层渲染”：\n\n>打开Chrome浏览器，打开控制台，找到\"Layers\"，如果没有，那么在控制台右上角更多的图标->More tools 找到\"Layers\"，然后随便找个网页打开即可\n\n网页的分层渲染流程主要是下面这样的：\n\n![](https://www.10000h.top/images/data_img/webRender/P7.PNG)\n\n(*注意：多个RenderObject可能又会对应一个或多个RenderLayer*)\n\n既然才用了分层渲染，那么肯定可以来分层处理，分层渲染有如下优点：\n\n* 减少不必要的重新绘制\n* 可以实现较为复杂的动画\n* 能够方便实现复杂的CSS样式\n\n当然，分层渲染是会很影响渲染效率的，可以有好的影响，使用不当也会有差的影响，我们需要合理的控制和使用分层：\n\n* 如果小豆腐块分层较多，页面整体的分层数量较大，会导致每帧渲染时遍历分层和计算分层位置耗时较长啊(比较典型的是腾讯网移动端首页)。\n* 如果可视区域内分层太多且需要绘制的面积太大，渲染性能非常差，甚至无法达到正常显示的地步(比如有一些全屏H5)。\n* 如果页面几乎没有分层，页面变化时候需要重绘的区域较多。元素内容无变化只有位置发生变化的时候，可以利用分层来避免重绘。\n\n那么，是什么原因可以导致分层呢？目前每一个浏览器或者不同版本的浏览器分层策略都是有些不同的(虽然总体差不太多)，但最常见的几个分层原因是：transform、Z-index；还有可以使用硬件加速的video、canvas；fixed元素；混合插件(flash等)。关于其他更具体的内容，可以见下文。\n\n```\n//注:Chrome中符合创建新层的情况：\nLayer has 3D or perspective transform CSS properties(有3D元素的属性)\nLayer is used by <video> element using accelerated video decoding(video标签并使用加速视频解码)\nLayer is used by a <canvas> element with a 3D context or accelerated 2D context(canvas元素并启用3D)\nLayer is used for a composited plugin(插件，比如flash)\nLayer uses a CSS animation for its opacity or uses an animated webkit transform(CSS动画)\nLayer uses accelerated CSS filters(CSS滤镜)\nLayer with a composited descendant has information that needs to be in the composited layer tree, such as a clip or reflection(有一个后代元素是独立的layer)\nLayer has a sibling with a lower z-index which has a compositing layer (in other words the layer is rendered on top of a composited layer)(元素的相邻元素是独立layer)\n```\n\n最后，我们总结一下如何合理的设计分层：分层总的原则是，减少渲染重绘面积与减少分层个数和分层总面积：\n\n* 相对位置会发生变化的元素需要分层(比如banner图、滚动条)\n* 元素内容更新比较频繁的需要分层(比如页面中夹杂的倒计时等)\n* 较长较大的页面注意总的分层个数\n* 避免某一块区域分层过多，面积过大\n\n(*如果你给一个元素添加上了-webkit-transform: translateZ(0);或者 -webkit-transform: translate3d(0,0,0);属性，那么你就等于告诉了浏览器用GPU来渲染该层，与一般的CPU渲染相比，提升了速度和性能。(我很确定这么做会在Chrome中启用了硬件加速，但在其他平台不做保证。就我得到的资料而言，在大多数浏览器比如Firefox、Safari也是适用的)*)\n\n另外值得一提的是，X5对分层方面做了一定的优化工作，当其检测到分层过多可能会出现显示问题的时候会进行层合并，牺牲显示性能换取显示正确性。\n\n最后再提出一个小问题：\n\n以下哪种渲染方式是最优的呢？\n\n![](https://www.10000h.top/images/data_img/webRender/P8.PNG)\n\n这里实际上后者虽然在分层上满足总体原则，但是之前讲到浏览器的分块渲染机制，是按照到可视区域的距离排序的，考虑到这个因素，实际上后者这种方式可能会对分块渲染造成一定的困扰，并且也不是最优的。\n\n### 动画 & canvas & WebGl\n\n讲最后一部分开始，首先抛出一个问题：CSS动画 or JS动画?\n\n对内核来说，实际上就是Renderer线程动画还是Compositor线程动画，二者实际上过程如下：\n\n![](https://www.10000h.top/images/data_img/webRender/P9.PNG)\n\n所以我们可以看出，Renderer线程是比Compositor线程动画性能差的(在中低端尤其明显)\n\n另外，无论是JS动画还是CSS动画，动画过程中的重绘以及样式变化都会拖慢动画执行以及引起卡顿\n以下是一些不会触发重绘或者排版的CSS动画属性：\n\n* cursor\n* font-variant\n* opacity\n* orphans\n* perspective\n* perspecti-origin\n* pointer-events\n* transform\n* transform-style\n* widows\n\n想要了解更多内容，可以参考[这里](https://csstriggers.com/)\n\n这方面最终的建议参考如下：\n\n* 尽量使用不会引起重绘的CSS属性动画，例如transform、opacity等\n* 动画一定要避免触发大量元素重新排版或者大面积重绘\n* 在有动画执行时，避免其他动画不相关因素引起排版和重绘\n\n\n#### requestAnimationFrame\n\n另外当我们在使用动画的时候，为了避免出现掉帧的情况，最好采用requestAnimationFrame这个API，这个API迎合浏览器的流程，并且能够保证在下一帧绘制的时候上一帧一定出现了：\n\n![](https://www.10000h.top/images/data_img/webRender/P11.PNG)\n\n### 3D canvas\n\n还有值得注意的是，有的时候我们需要涉及大量元素的动画(比如雪花飘落、多个不规则图形变化等)，这个时候如果用CSS动画，Animation动画的元素很多。，导致分层个数非常多，浏览器每帧都需要遍历计算所有分层，导致比较耗时、\n\n这个时候该怎么办呢？\n\n2D canvas上场。 \n\n和CSS动画相比，2D canvas的优点是这样的：\n\n* 硬件加速渲染\n* 渲染流程更优\n\n其渲染流程如下：\n\n![](https://www.10000h.top/images/data_img/webRender/P10.PNG)\n\n实际上以上流程比较耗时的是JS Call这一部分，执行opengl的这一部分还是挺快的。\n\nHTML 2D canvas 主要绘制如下三种元素：\n\n* 图片\n* 文字\n* 矢量\n\n这个过程可以采用硬件加速，硬件加速图片绘制的主要流程：\n\n![](https://www.10000h.top/images/data_img/webRender/P12.PNG)\n\n硬件加速文字绘制的主要流程：\n\n![](https://www.10000h.top/images/data_img/webRender/P13.PNG)\n\n但对于矢量绘制而言，简单的图形，比如点、直线等可以直接使用OpenGL渲染，复杂的图形，如曲线等，无法采用OpenGL绘制。\n\n对于绘制效率来说，2D Canvas对绘制图片效率较高，绘制文字和矢量效率较低(**所以建议是，我们如果能使用贴图就尽量使用贴图了**)\n\n还有，有的时候我们需要先绘制到离屏canvas上面，然后再上屏，这个可以充分利用缓存。\n\n### 3D canvas(WebGL)\n\n目前，3D canvas(WebGL)的应用也越来越多，对于这类应用，现在已经有了不少已经成型的庫:\n\n\n* 通用引擎：threeJS、Pixi\n* VR视频的专业引擎：krpano、UtoVR\n* H5游戏引擎：Egret、Layabox、Cocos\n\nWebGL虽然包含Web，但本身对前端的要求最低，但是对OpenGL、数学相关的知识要求较高，所以如果前端工程师没有一定的基础，还是采用现在的流行庫。\n\nX5内核对于WebGl进行了性能上和耗电上的优化，并且也对兼容性错误上报和修复做了一定的工作。\n\n___\n\n本文参考腾讯内部讲座资料整理而成，并融入一部分笔者的补充，谢绝任何形式的转载。\n\n其他优质好文：\n\n[Javascript高性能动画与页面渲染](http://qingbob.com/javascript-high-performance-animation-and-page-rendering/)\n\n\n","tags":["性能优化"]},{"title":"类方法和静态方法","url":"/2017/05/23/pout/python中高级面试题/类方法和静态方法/","content":"\n\n<!-- toc -->\n\n普通实例方法，第一个参数需要是自己，它表示一个具体的实例本身。\n\n如果用了静态方法，那么就可以无视这个自己，而将这个方法当成一个普通的函数使用。\n\n而对于classmethod，它的第一个参数不是self，是cls，它表示这个类本身。\n\n@classmethod修饰符对应的函数不需要实例化，不需要自我参数，第一个参数需要是表示自身类的cls参数，cls参数可以调用调用类的属性，类的方法，实例化对象等。\n\n@staticmethod返回函数的静态方法，该方法不强制要求传递参数\n\n示例：\n\n```\nclass Foo(object):\n    \"\"\"类三种方法语法形式\"\"\"\n\n    def instance_method(self):\n        print(\"是类{}的实例方法，只能被实例对象调用\".format(Foo))\n\n    @staticmethod\n    def static_method():\n        print(\"是静态方法\")\n\n    @classmethod\n    def class_method(cls):\n        print(\"是类方法\")\n\nfoo = Foo()\nfoo.instance_method()\nfoo.static_method()\nfoo.class_method()\nprint('----------------')\nFoo.static_method()\nFoo.class_method()\nclass C(object):\n    @staticmethod\n    def f():\n        print('runoob');\n\nC.f();          # 静态方法无需实例化\ncobj = C()\ncobj.f()        # 也可以实例化后调用\nclass A(object):\n\n    # 属性默认为类属性（可以给直接被类本身调用）\n    num = \"类属性\"\n\n    # 实例化方法（必须实例化类之后才能被调用）\n    def func1(self): # self : 表示实例化类后的地址id\n        print(\"func1\")\n        print(self)\n\n    # 类方法（不需要实例化类就可以被类本身调用）\n    @classmethod\n    def func2(cls):  # cls : 表示没用被实例化的类本身\n        print(\"func2\")\n        print(cls)\n        print(cls.num)\n        cls().func1()\n\n    # 不传递默认self参数的方法（该方法也是可以直接被类调用的，但是这样做不标准）\n    def func3():\n        print(\"func3\")\n        print(A.num) # 属性是可以直接用类本身调用的\n```\n","tags":["python中高级面试题"]},{"title":"Zabbix 监控系统","url":"/2017/02/13/pout/运维/Zabbix 监控系统/","content":"\n## Zabbix 监控系统\n\n\n\n\n\n#### 什么是监控系统？\n\n例如：开车时的行车记录仪、班级里面的监控摄像头、医院使用的血压监测仪等等\n\n在IT领域中，监控系统就是**监控系统资源以及性能的硬件或者软件**\n\n**监控软件**\n\n1. 单一监控程序\n   1. windows系统的任务管理\n   2. linux系统中的top、vmstat、iostat\n2. 分布式监控程序\n\n#### 为什么需要监控系统？\n\n为用户提供稳定、高效、安全的服务\n\n但是像zabbix和open-falcon只能监控某些方面。而为了业务高效运行还需要和APM也就是应用性能监控、全局链路调用追踪等一起实现。而安全性需要安全团队和相关系统紧密结合才行\n\n#### 监控系统都有哪些功能？\n\n1. **数据收集.**\n2. **数据展示**\n3. **告警策略**\n4. **告警发送**\n5. **事件管理**\n6. **报表管理.**\n7. **认证权限.**\n\n#### 开源监控系统现状\n\n1. Nagios.\n   + 1999年发布的初始版本。它可以监控网络、主机等设备。支持丰富的监控插件。用户可以根据自己的实际环境定义监控\n2. Cacti.\n   + 2001年发布的。是一套基于`SNMP`和`RRDTool`的网络流量监控分析的系统\n   + **RRDTool**: 用来处理时间序列数据的套件\n   + **SNMP**: 简单网络管理协议\n3. Ganglia\n4. Zabbix\n5. Prometheus\n6. Falcon\n7. Grafana\n   + Grafana是可以美观的展示和分析监控数据的工具，收集数据并支持告警等功能\n8. Zenoss\n9. Graphite\n10. Open-falcon ...\n    + 小米公司开源的，高可用、可扩扎的开源监控解决方案\n\n#### 监控系统可以干什么？\n\n1. KPI聚类\n2. 瓶颈分析\n3. KPI异常检测、定位\n4. 故障预测\n5. 容量预估\n\n#### Zabbix 简介\n\n> 市场上还有一款叫做`open-falcon`和zabbix类似，而且市场上也是在广泛使用，都是属于**分布式监控程序**\n\n> **zabbix** 是一个基于web界面的提供分布式系统监控的企业级的开源解决方案。\n>\n\n> **zabbix** 能监视各种网络参数， 保证服务器系统的安全稳定的运行，并提供灵活的通知机制以让SA快速定位并解决存在的各种问题。\n\n##### 了解zabbix\n\n我们通过zabbix能够监控到哪些硬件资源呢？理论上说，只要是与我饿们的业务相关的硬件资源，又应该被监控。比如主机、交换机、路由器、UPS等等。但是，监控她们的额前提是能与他们进行通讯，那么问题来了，由于硬件不同，导致我们无法使用统一的方法去监控他们，这个时候就需要监控程序有一定的通用性，或者说，监控程序需要能够与多种硬件设备通讯，才能满足我们的监控需求。所以zabbix如果想要能够全面的监控这些对象，则需要能够通过各种方法与他们通讯。\n\n##### Zabbix 的优点\n\n1. 支持自动发现服务器和网络设置\n2. 支持底层自动发现\n3. 分布式的监控体系和集中式的web管理\n4. 支持主动监控和被动监控模式\n5. 服务器支持多种操作系统：\n   1. linux\n   2. solaris\n   3. HP-UX\n   4. AIX\n   5. FreeBSD\n   6. OpenBSD\n   7. MAC ...\n6. `Agent客户端`支持多种操作系统：\n   1. linux\n   2. Solaris\n   3. HP-UX\n   4. AIX\n   5. FreeBSD\n   6. Windows ...\n7. 基于SNMP、IPMI接口方式也可以 监控Agent\n8. 安全的用户认证及权限配置\n9. 基于WEB的管理方法，支持自由的自定义事件和邮件发送\n10. 高水平的业务视图监控资源，支持日志审计、资产管理等功能\n11. 支持高水平API二次开发、脚本监控、自Key定义、自动化运维整合调用\n\n##### Zabbix 监控组件及流程\n\nzabbix 主要有三大组件组成，分别是\n\n1. **Zabbix server端**\n\n   1. `Zabbix WEB GUI`\n   2. `Zabbix Datebase`\n   3. `Zabbix Server`\n\n   ![1576579661368](C:\\Users\\Lenovo\\AppData\\Local\\Temp\\1576579661368.png)\n\n2. **Zabbix Proxy端**\n\n3. **Zabbix Agent端**\n\n![1576579876393](C:\\Users\\Lenovo\\AppData\\Local\\Temp\\1576579876393.png)\n\n##### zabbix的好处：\n\n占用资源少、可以获取CPU、内存、网卡、磁盘、日志等信息。\n\n对于无法安装客户端的设备，zabbix支持通过`SNMP`获取监控数据\n\nzabbix支持通过`IPMI`(智能平台管理接口)获取硬件的温度、风扇、硬盘、电源等\n\n#####zabbix监控系统的意义\n\n通过这些监控系统我们可以了解设备的繁忙程度、是否有异常的进程占用资源\n\n比较常见的是：通过传感器获取设备的监控信息\n\n##### Zabbix 能监控哪些硬件资源呢？\n\n> 如果理论上说，只要与我们业务相关的硬件资源，都可以被监控。例如：主机、交换机、路由器等等。但是监控的前提是能与他们进行通讯\n\n##### Zabbix 支持哪些通讯方式呢？\n\n1. **Agent**：通过专用的代理程序进行监控，与常见的master/agent模型类似。如果被监控对象支持对应的agent，推荐首选这种方式\n2. **ssh/tenet**：通过远程控制协议进行通讯，比如ssh或telnet\n3. **SNMP**:通过SNMP协议与被监控对象进行通讯，`SNMP`协议全称 Simple Network Mnaagement Protocol,被译为“简单网络管理协议”，通常来说，我们无法在路由器、交换机这种硬件上安装agent，但是这些硬件往往都支持SNMP协议、SNMP是一种比较久远的、通行的协议，大部分网络设备都支持这种协议，其实SNMP协议的工作方式也可以理解为master/agent的工作方式，只不过实在这些设备中内置了SNMP的agent而已，所以大部分网络设备都支持这种协议\n4. **IPMI**：通过IPMI接口进行监控，我们可以通过标准的IPMI硬件接口，监控被监控对象的物理特征，比如电压、温度、风扇状态、电源状态等等\n5. **JMX**：通过JMX进行监控，JMX全称 Java Management Extensions ，也就是Java管理扩展，监控JVM虚拟机时，使用这种方法也是非常不错的选择\n\n##### Zabbix 监控流程\n\n###### zabbix 核心组件\n\n+ **zabbix agent**：部署在被监控主机上，负责被监控主机的数据，并将数据发送给zabbix server\n+ **zabbix server**：负责接收agent发送的报告信息，并且负责组织配置信息，统计信息，操作数据等\n+ **zabbix database**：用于储存所有zabbix的配置信息、监控数据的数据库\n+ **zabbix web**：zabbix的web界面，管理员通过web界面管理zabbix配置以及查看zabbix相关监控信息\n+ **zabbix proxy**：可选组件，用于分布式监控环境中，zabbix proxy代表server端，完成局部区域内的信息收集，最终统一发往server端\n\n如下图1-1：\n\n![1576653301515](C:\\Users\\Lenovo\\AppData\\Local\\Temp\\1576653301515.png)\n\n+ 我们将zabbix agent 部署到被监控主机上，由agent采集数据，报告给负责控制中心主机，中心主机也就是master/agent模型中的master，负责监控的中心主机被称为zabbix serevr，zabbix server将从agent端接收到信息储存于zabbix的数据库中，我们把zabbix的数据库端称为zabbix database，如果管理员需要查看各种监控信息，则需要zabbix的GUI，zabbix的GUI是一种Web GUI，我们称之为zabbix web，zabbix web是使用PHP编写的，所以，如果想要使用zabbix web展示相关监控信息，需要依赖LAMP环境，不管 是zabbix server，或是zabbix web，他们都需要连接到zabbix database获取相关数据\n\n如下图1-2：\n\n![1576654020563](C:\\Users\\Lenovo\\AppData\\Local\\Temp\\1576654020563.png)\n\n+ 当监控规模变得庞大时，我们可能有成千上万台设备需要监控，这时我们是否需要部署多套zabbix系统进行监控呢？如果部署多套zabbix监控系统。那么监控压力就会被分摊，但是，把这些监控的对象将会被尽量平均的分配到不同的监控系统中，这个时候，我们就无法通过统一的监控入口，去监控这些对象了。虽然分摊了监控压力，但是也增加了监控工作的复杂度？其实，zabbix天生就有处理这种问题的能力，因为zabbix支持这种分布式监控，我们可以把成千上万台的监控对象分成不同的区域，每个区域中设置一台代理主机，区域内的每个监控对象的信息被agent采集，提交给代理 主机，在这个区域内，代理主机的作用就好比zabbix server，我们称为这些代理主机为zabbix proxy，zabbix proxy再将收集到的信息统一提交给真正的zabbix server处理。这样，zabbix proxy不仅分摊了zabbix server的压力，同时，我们还能够通过统一的监控入口，监控所有的对象\n\n##### zabbix 工作模式\n\n>  我们都知道，agent端采集完数据主动发送给server端，这种模式我饿称之为**主动模式**，也就说对于agent端来说是主动的。\n\n> 其实，agent端也可以不主动发送数据，而是等待server过来拉取数据，这种模式我们叫**被动模式**\n\n> 其实不管是主动模式还是被动模式，都是对于agent端来说的。而且，主动模式与被动模式可以同时存在，并不冲突。\n>\n> 管理员可以在agent端使用一个名为`zabbix_sender`的工具，测试是否能够从server端发送数据。\n>\n> 管理员也可以在server端使用一个名为`zabbix_get`的工具，测试是否能从agent端拉取数据\n\n#### Zabbix 服务器搭建部署\n\n##### 安装描述\n\n简单地概念刚刚已经描述过，zabbix的几个常用的重要组件，在安装zabbix时，其实是在安装这些组件。\n\n由于我们的监控规模也不大，所以此处将不会安装zabbix proxy ，我们需要安装如下组件：\n\n1. **Zabbix server**\n2. **Zabbix database**\n3. **Zabbix web**\n4. **Zabbix agent**\n\n好了，接下来一个一个聊\n\n安装的`zabbix server`版本为`3.0`\n\n因为zabbix3.X依赖的php版本 不能低于php5.4，而在centos6.8中，php默认版本为5.3\n\n如果你想要使用centos6.X的操作系统。同时想要更加方便的升级php，可以使用Remi源升级PHP，\n\n但是为了更加方便的使用yum源安装相关软件包，此处使用centos7安装`zabbix3.0.7`\n\n##### 安装 zabbix server\n\n为了方便安装，配置zabbix的官方yum源\n\n```shell\nhttp://repo.zabbix.com/\n```\n\n![1576671582254](C:\\Users\\Lenovo\\AppData\\Local\\Temp\\1576671582254.png)我们配置一下zabbix3.0的yum源\n\n![1576672159548](.\\1577188749508.png)\n\n首先进入`/ect/yum.repo.d/`文件夹\n\n查看一下 是否有`zabbix.repo`，如果没有创建一个并编写，如下图格式\n\n![1576672281247](.\\1577189089487.png)\n\n```shell\n[zabbix]\nname=zabbix\nbaseurl=http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/\ngpgcheck=0\nenabled=1\n```\n\n同时，我们配置了**base**源与**epel**源，因为安装过程中会用到这些yum源。\n\n没有配置好的话，会报出如下图的错：\n\n![1577235080635](.\\1577235080635.png)\n\n所以在下载之前，先配置好base源和epel源\n\n这里选择的是redhat7下x86_64的zabbix3.0版本的包。但是安装zabbix-server-mysql时报错，原因是缺少libiksemel.so.3()(64bit)和fping包。这是因为yum安装zabbix不仅需要配置zabbix包源，还需要配置好epel源和base源，base源我们有自带就不用说了。\n\n这时我们需要配置epel源\n\n```shell\nyum -y install epel-release\n```\n\n```shell\nls /etc/yum.repo.d/\n```\n\n这里看到多了epel.repo这个文件，表明epel配置成功。\n\n然后我们继续安装zabbix-server-mysql\n\n```shell\nyum -y install zabbix-server-mysql\n\n错误：软件包：zabbix-server-mysql-3.0.25-1.el7.x86_64 (zabbix)\n需要：libiksemel.so.3()(64bit)\n```\n\n我们发现这里还有缺少依赖包libiksemel.so.3()(64bit)的问题。\n\n我们通过下载zabbix源来解决这个问题\n\n```shell\nyum -y install http://repo.zabbix.com/zabbix/3.0/rhel/7/x86_64/zabbix-release-3.0-1.el7.noarch.rpm\n```\n\n最后安装zabbix-server-mysql成功 \n\n```shell\nyum -y install zabbix-server-mysql zabbix-get --skip-broken\n\n已安装:\nzabbix-server-mysql.x86_64 0:3.0.25-1.el7\n```\n\n准备工作完毕，剩下的就是安装各个组件了，我们一个一个安装\n\n先安装**zabbix server**\n\n由于我们使用mysql作为数据库，所以，在安装zabbix3.X的版本的server端时，需要安装zabbix-server-mysql包，在3.X的zabbix版本中，并没有单独的zabbix server端程序包，安装zabbix-server-mysql包即为了安装了server端包，同时，我们可以在服务器安装zabbix_get包，以便想agent端发起测试采集数据请求，所以，我们在server端安装如下;\n\n![1576672693413](/1577189265125.png)\n\n```shell\nyum install zabbix-server-mysql zabbix-get\n```\n\n\n\n在安装的时候，我遇到了一个问题，\n\n密密麻麻的英文很头疼，而且好多都不认识咋办？其实也不用全都理解，全都认识，认识关键字就行。例如：You could try using --skip-broken to work around the problem ，大概意思就是 ‘你可以使用 “--skip-broken” 来解决这个问题‘，所以只需要在代码后面加上就可以了\n\n```shell\nyum install zabbix-server-mysql zabbix-get --skip-broken\n```\n\n还有一种解决办法，就是更新一下你的yum，因为你的yum版本低了。\n\n```shell\nyum -y update\n```\n\n安装完成后，输入\n\n```shell\nrpm -ql zabbix-server-mysql\n```\n\n如果出现下图这样的情况，那就是你安装失败了。需要重新安装\n\n![1577235250247](.\\1577235250247.png)\n\n![1577235296648](.\\1577235296648.png)\n\n重新安装还有可能碰到一种问题，\n\n连上你同桌的WiFi就好了，因为你的网络丝毫不给你一分薄面！\n\n之后重新下载一下就好了，出现Complete就是现在好了，如下图\n\n后面出现类似的问题，同样的方法解决就好了\n\n![1577235650800](.\\1577235650800.png)\n\n之后继续配置，输入\n\n```shell\nrpm -ql zabbix-server-mysql\n```\n\n![1576745154362](.\\1577235837540.png)\n\n输入这行命令，看见create.sql.gz那就进入到这个文件夹\n\n```shell\ncd /usr/share/doc/zabbix-server-mysql-3.0.28\n```\n\n之后解压create.sql.gz这个压缩文件，即可获得初始化sql脚本\n\n```shell\ngunzip create.sql.gz\n```\n\n之后输入\n\n```shell\nll create.sql \n```\n\n![1577235977026](.\\1577235977026.png)\n\n是这样的效果，就是正确的！\n\n但是需要注意的是，此sql脚本中sql只会在对应的数据库中初始化zabbix所需要的数据库表，但是不会创建zabbix数据库，所以，创建zabbix数据库这一步骤，还是需要我们手动进行的。所以，此处我们先动手创建zabbix的数据库，过程如下：\n\n进入数据库\n\n![1576746216482](.\\1577236249847.png)\n\n```shell\ncreate database zabbix charset 'utf8';\n\ngrant all on zabbix.* to zabbix@'localhost' identified by '123456';\n\nflush privileges;\n```\n\nzabbix数据库初始化完成后，执行对应的sql初始化脚本，输入命令：\n\n![1576746644489](.\\1577237348431.png)\n\n```shell\nmysql -uroot -p -Dzabbix < /usr/share/doc/zabbix-server-mysql-3.0.28/create.sql\n# /usr/share/doc/zabbix-server-mysql-3.0.28/create.sql 对应的是你create.sql所在的地点\n```\n\n进入数据库，zabbix库，查看表，出现这些表，就是导入成功\n\n![1577237406737](.\\1577237406737.png)\n\n##### 配置zabbix server端并启动\n\nserver端已经安装完毕，并且数据库也已经初始化，现在我们开始配置server端，编辑zabbix server端的配置文件\n\n![1576747755163](.\\1577238444017.png)\n\n```shell\nvim /etc/zabbix/zabbix_server.conf \n```\n\n此处列出我们可能会经常修改的参数，如下：\n\n> `LIstenPort=10051`  \n>\n> 服务器默认端口\n\n> `SourceIP=`\n>\n> 通过SourceIP参数可以指定服务器的源IP，当server端㕛多个IP地址时，我们可以指定服务器端使用固定的IP于agent端进行通讯，为了安全起见，agent端会基于IP进行一定的访问控制，也就是说agent端只允许指定IP以server端的身份菜鸡被监控主机的数据，如果IP不对应，租不允许采集被监控主机的数据，所以，当server端 有多个IP时，我们可以通过SourceIP参数，指定server端 通过哪个IP采集被监控主机的数据\n\n> `LogType=file=file`\n>\n> 通过LogType参数，可以指定通过哪种方式记录日志，此参数可以设置为三种值，system、file、console，system表示将日志发往syslog，file表示使用指定的文件 作为日志 文件，console表示将日志发往控制台，默认为file\n\n> `LogFile=/var/log/zabbix/zabbix_server.log`\n>\n> 当LogType设置为file时，通过LogFile参数设置日志文件位置\n\n> `LogFileSize=0`\n>\n> 指明日志文件达到 多大是自动滚动，单位为MB，如果设置LogFileSize为50时，表示日志大小达到 50MB滚动一次，设置为0表示日志文件不寄回滚动，所有入职保存在一个文件中\n\n> `Debuglevel=3`\n>\n> 通过DebugLevel参数可以定义日志的详细程度，即为日志级别\n\n> `DBHost=localhost`\n>\n> 通过DBHost参数设置zabbix数据库 所在的服务器IP，由于此处zabbix于mysql安装在同一个服务器上，所以此处设置为localhost\n\n> `DBUser=zabbix`\n>\n> 通过DBUser指定zabbix数据库用户名\n\n> `DBPassword=`\n>\n> 通过DBPassword指定zabbix数据库用户的密码\n\n> `DBPort=3306`\n>\n> 通过DBPort指定zabbix所在数据库服务监听的端口号\n\n> `DBSocket=/var/lib/mysql/mysql.sock`\n>\n> 如果数据库服务于server端在同一台服务器上，可以通过DBSocket指定数据库本地套接字文件位置，但是需要注意，即使设置了mysql套接字文件的位置，还是需要配合DBHost参数，否则在登录zabbix控制台时，可能会出现警告，在zabbix server的log中，也可能会出现无法连接数据库的提示\n\n根绝上述的配置参数的解释，根据具体需求进行实际配置即可。\n\n\n\n配置完成后，启动zabbix服务端即可，输入\n\n```shell\nsystemctl start zabbix-server.service\n# 输入\nss -tnl\n# 查看10051端口是否被监听\n```\n\n启动后,10051端口已经被监听，如下图\n\n![1576807235896](.\\1577238791164.png)\n\n##### 安装zabbix web端\n\nzabbix web 可以安装在单独的主机上，只要能连接到zabbix database所在的数据库即可。但是此处为了方便，我们将zabbix web与mysql以及 zabbix server 安装在同一台服务器上。\n\n因为 zabbix web 需要lamp环境，所以，此处我们将会依赖到的环境先安装好。\n\n代码如下：\n\n```shell\nyum install httpd php php-mysql php-mbstring php-gd php-bcmath php-ldap php-xml\n```\n\n完成上述步骤后，安装zabbix web所需要的两个包，对应版本为3.0.7.\n\n```shell\nyum install zabbix-web zabbix-web-mysql --skip-broken\n```\n\n查看刚才安装完成的zabbix-web程序包，可以看到，zabbix-web的web应用存放在/usr/share/zabbix中。\n\n```shell\nrpm -ql zabbix-web\n```\n\n\n\n![1577239418954](.\\1577239418954.png)\n\nzabbix还是比较贴心的，针对httpd，zabbix-web包中已经包含了对应zabbix文档路径的配置文件。\n\n输入：\n\n![1577239584104](.\\1577239584104.png)\n\n```shell\nvim /etc/httpd/conf.d/zabbix.conf\n```\n\n![1577239635853](.\\1577239635853.png)\n\n以看到，针对zabbix web的文档路径，此文件中已经为我们准备了默认设置，如果不使用httpd的虚拟主机，只要把时区稍加改动即可直接使用\n\n而此处，我们使用httpd的虚拟主机访问zabbix web ，所以，将配置文件爱你中的内容改为如下配置，同时将时区修改为`亚洲上海`\n\n![1577233972645](.\\1577240031191.png)\n\n```shell\n<VirtualHost 39.106.84.122>\nservername zabbix.zhanglei.net\ndocumentroot /usr/share/zabbix\n\n        Alias /zabbix /usr/share/zabbix\n\n        <Directory \"/usr/share/zabbix\">\n                Options FollowSymLinks\n                AllowOverride None\n                Require all granted\n\n                <IfModule mod_php5.c>\n                        php_value max_execution_time 300\n                        php_value memory_limit 128M\n                        php_value post_max_size 16M\n                        php_value upload_max_filesize 2M\n                        php_value max_input_time 300\n                        php_value max_input_vars 10000\n                        php_value always_populate_raw_post_data -1\n                        php_value date.timezone Asia/Shanghai\n                </IfModule>\n        </Directory>\n        \n        <Directory \"/usr/share/zabbix/conf\">\n                Require all denied\n        </Directory>\n\n        <Directory \"/usr/share/zabbix/app\"> \n                Require all denied\n        </Directory> \n\n        <Directory \"/usr/share/zabbix/include\"> \n                Require all denied\n        </Directory>\n    \n        <Directory \"/usr/share/zabbix/local\">\n                Require all denied\n        </Directory>\n\n</VirtualHost>\n```\n\n配置为完成后，启动httpd服务。\n\n![1577240159396](.\\1577240159396.png)\n\n好了，zabbix web安装配置完成\n\n访问 服务器IP/zabbix，就可以看到如下图的zabbix安装页面\n\n配置好之后，别忘记把把`/usr/share/zabbix`复制到`/var/www/html`里面\n\n之后，IP/zabbix/setup.php访问即可，就可出现如下图的样子。\n\n![1577246143181](.\\1577246280547.png)\n\n##### 初始化zabbix 配置\n\n完成上述安装步骤后就可以看到zabbix安装页面。点击下一步\n\n不出意外的话，你们也会这样。哈哈哈\n\n![1577246345326](.\\1577246345326.png)\n\n按照错误的这四个参数。修改/etc/php.ini文件中的配置就行\n\n```\nvim /etc/php.ini\n```\n\n`post_max_size=16M `\n\n `max_execution_time=300 `\n\n `max_input_time=300 `\n\n `date.timezone =Asia/Shanghai `\n\n找到这四个，直接把配置改了就好，和我的一样就行。\n\n如果嫌找的麻烦，直接搜索，输入‘/’\n\n```\n/你想搜索的关键字，或者全拼\n```\n\n改好之后重新启动下服务\n\n```\nsystemctl restart httpd.service\n```\n\n之后再重新加载下页面，就会像如下图：\n\n![1576830128835](.\\1577253128156.png)\n\n可以看到，zabbix检查的环境已经全部满足，所以点击下一步！\n\n![1576830195924](.\\1577253184977.png)\n\n此处zabbix需要配置数据库连接，此处配置数据库的类型，IP，端口，数据库名，用户密码等信息，端口填写0表示使用默认端口(3306端口)\n\n请确定概要信息无误，点击下一步\n\n![1577255082305](.\\1577255155455.png)\n\n![1577255204397](.\\1577255204397.png)\n\n![1577255235198](.\\1577255235198.png)\n\n之后，就会进入登录页面，默认账号admin，密码zabbix\n\n![1577255289782](.\\1577255289782.png)\n\n登录完成后，可以看到zabbix的仪表盘\n\n![1577071638902](.\\1577255350824.png)\n\n全是英文，作为一个爱国的知识青年，肯定看不爽，所以 ，我们可以把它调成中文版\n\n![1577071691484](.\\1577255416702.png)\n\n语言选择中文，点击 更新即可，蛋黄思思你可能无法在语言中看到中文的选项，如果无法找到中文选项，则代表你的配置文件中的中文选项显示属性为false\n\n![1577071800394](.\\1577255448186.png)\n\n当然了，如果你没有这项选择，那么你可以修改下如下文件\n\n```\nvim /usr/share/zabbix/include/locales.ini.php\n```\n\n![1577071978581](.\\1577255626516.png)\n\n找到中文对应的值，将显示属性设置为true即可，如上图所示\n\n但是，你可能还会遇到中文乱码情况，如果遇到中文乱码，可以从Windows中挑选一个顺眼的中文字体，将对应字体放置到inux中 zabbix web的字体目录中，因为我们使用的是rpm包安装的zabbix web，所以zabbix web默认字体目录为 /usr/share/zabbix/fonts/，Windows中的字体文件后缀名如果为TTF，当我们把对应字体文件拷贝到zabbix字体目录是，需要修改其后缀名为小写的ttf（如果本来就是小写的则不用任何修改了），字体文件上传完毕后，修改/usr/share/zabbix/include/defines.inc.php配置文件，将下图中显示字体部分修改为刚才上传的字体文件对应的名称即可。\n\n好了，上述操作完成后，zabbix控制台即显示中文了。\n\n但是你可能会在访问zabbix控制台时，可能会发现如下提示：\n\n![1577255896543](.\\1577255896543.png)\n\n如果出现图中的提示，可能是由如下几个原因引起的：\n\n1、zabbix-server未正常启动\n\n2、已经开启selinux，但是没有正常设置对应权限\n\n3、zabbix-server未能正常连接数据库\n\n4、zabbix.conf.php文件中$ZBX_SERVER参数对应的主机名不能正常解析\n\n5、其他原因，需要查看zabbix server 日志\n\n如果在访问zabbix控制台时并没有出现上述提示，忽略上述描述即可。\n\n为了更加安全，我们不应该使用管理员的默认密码，所以，我们最好先修改管理员密码\n\n![1577073009933](.\\1577256061549.png)\n\n![1577073042611](.\\1577256113248.png)\n\n好了。基本配置已经配置完成了，我们以后的监控工作就要围绕着这个web界面展开了！\n\n##### Zaabix agent 安装\n\n现在万事具备，就差agent端了，agent端安装也非常方便，直接**被监控主机**上安装如下两个包即可。\n\n`当然了，在安装之前，指定要把上面的准备工作全都做好，否则会出错的哦！`\n\n此处被管理主机 centos7，已经配置好了对应的zabbix源，agent版本可以跟server端版本  不一致，没有关系，安装即可\n\n```shell\nyum install -y zabbix-agent zabbix-sender\n```\n\n我们查看一下zabbix-agent都安装了什么文件，当然，最重要的就是zabbix_agentd.conf这个配置文件了\n\n![1577073489994](.\\1577256337395.png)\n\n还记得我们在刚开始介绍zabbix时说过“主动模式”与“被动模式”吗？这两种模式的相关配置，都需要在zabbix_agentd.conf中定义，打开这个文件，我们来配置一下常用的agent端配置。首先，可以看到配置文件中有很多注释，打开配置文件，首先看到的就是\"通用参数配置段\"，我们可以在此配置段配置zabbix_agent进程的进程编号文件路径，存储日志方式，日志文件位置，日志滚动阈值等常用设定，细心如你一定已经发现，zabbix_agent配置文件的\"通用配置段\"中的参数大多数与zabbix_server配置文件中的常用参数意义相同，所以，此处不再过多赘述，如果没有特殊需要，保持默认即可。 \n\n![1577087246148](.\\1577272480659.png)\n\n此处先说说我们马上会用到的两个配置，如下图红框中的注释所描述的，“被动模式配置段”与“主动模式配置段”\n\n![1577273106840](.\\1577273106840.png)\n\n![1577087246148](.\\1577273063509.png)\n\n我们已经在最开始的概念介绍中，描述过，“主动模式”和“被动模式”都是对于agent端来说的，而且它们可以同时存在，，并不冲突。\n\n我们先来看看“被动模式”的相关配置参数。\n\n被动模式相关参数如下：\n\nSserver：用于指定允许哪台服务器拉去当前服务器的数据，当agent端工作于被动模式，则代表server端会主动拉取agent端数据，那么server端的IP必须与此参数的IP对应，此参数用于实现基于IP的访问控制，如果有多个IP，可以使用逗号隔开。\n\nListenPort：用于指定当agent端工作于被动模式所监听的端口号，默认端口号10050，也就是说，server端默认访问10050端口，从而拉取数据。\n\nListenIP：用于指定agent端工作于被动模式时所监听的IP地址，默认值为0.0.0.0，表示监听本机的所有IP地址。\n\nStartAgents：用于指定预生成的agent进程数量。\n\n\n\n主动模式\n\n主动模式的常用参数如下：\n\nServerActive：此参数用于指定当agent端工作于主动模式时，将信息主动推送到哪台server上，当时有多个IP时，可以用逗号隔开。\n\nHostname：此参数用于指定当前主机的主机名，server端通过此参数对应的主机名识别当前主机。\n\nRefreshActiveChescks：此参数用于指明agent端没多少秒主动将采集到的数据发往server端。\n\n\n\n此处，我们同时设置“被动模式”与“主动模式”的如下参数，其他保持默认即可，修改完成后保存退出。\n\nServer=47.96.230.50\n\nServerActive=47.96.230.50\n\nHostname=testzbx1.zsythink.net\n\n配置文件修改完成后，启动agent端进程\n\n![1577088553130](.\\1577273181431.png)\n\n好了，agent端也已经安装好了！\n\n#### 在 Zabbix 中添加主机\n\n在添加主机之前，我们先把工作场景描述清楚。然后再根据描述的工作场景进行演示\n\n假设，我们想要使用zabbix监控一台linux服务器，那么，我们肯定要将这个服务器纳入zabbix的管理范围，而“添加主机”这个操作，就是将被监控的主机纳入zabbix管理范围的一个必须操作，如果我们有10台主机都需要呗zabbix监控呢？没错，这10台主机必须被添加到zabbix的监控列表中，在zabbix中，我们将被监控的对象称为“主机”，“主机”不一定是服务器，也可以是`路由器`，`交换机`等网络设备，而且，根据主机的属性、角色、特征的不同，我们还能够将主机分组。\n\n比如，我们有10台服务器，10台服务器中，有3台window服务器，有7台linux服务器，那么，我们还可以按照操作系统的不同，将他们分成两组，Windows服务器组于linux服务器组，或者我们不按照操作系统对主机进行分组，而是根据服务器的角色对主机分组。\n\n比如，一共10台服务器，3台是是提供ldap服务的，2台是提供web服务的，5台是提供数据库服务的，我们也可以把他们按照角色分成3组，ladp主机组、web主机组、db主机组，当然，我们也只是举个例子。\n\n实际应用中，具体怎样分组，是根据实际需求视情况而定的，那么，为什么要将主机分组呢？这是为了方便管理，因为同一类主机需要被监控的指标很有可能 都是相同的，所以将他们分为一组方便管理，当然了，这就是后话，我们后面再聊！\n\n上面一段话中，我们提到了两个zabbix的常用术语，“主机”与“主机组”，我们再来总结一遍：\n\n```shell\n1、host（主机）：需要zabbix监控的对象，被称为主机，主机必须属于某个主机组。\n\n2、hostgroup（主机组）：“主机组”也被称为“主机群组”，是由具有相同属性、特征、角色的多个主机组成的逻辑单元。\n```\n\n理解 上述两个术语，并且能够在zabbix中使用他们，就是我们所要达到的目的。\n\n那么我们来看看怎样在zabbix添加一台主机，在动手添加主机之前。先说明下我们的环境。\n\n我们已经将zabbix-server、zabbix-database、zabbix-web安装在了39.106.84.122上。\n\n同时，我们将zabbix-agent安装在了47.97.172.176上。\n\n所以此处，47.97.172.176就是被监控的对象 ，我们需要将176添加为zabbix主机。\n\n首先呢，打开我们的zabbix web 控制台，看看都有那些“主机组”。\n\n点击“配置“----”主机群组“，可以看到，系统默认已经为我们准备了一些主机组，如果这些主机组不满足我们的需要，我们也可以创建新的主机组\n\n点击下图中的“创建主机群组”按钮，即可创建主机组，但是，我们还不用深入研究主机组，此处只是让大家了解一下，对主机有一个初步的认识。\n\n![1577320713453](.\\1577320713453.png)\n\n同样，点击`配置`-----`主机`，即可查看已经被加入zabbix主机列表的主机，可以看到，zabbix默认将zabbix server添加为了一台主机，以便 可以自己监控自己，但是此处，我们需要添加一台我们自己的主机，就是47.97.172.176，\n\n点击`创建主机`，点击创建主机之前，可以选择左侧的`群组`下来菜单，以确定将要创建的主机所在的主机组，当然，我们也可以先不选主机组，直接点击`创建主机`按钮。\n\n![1577322582225](.\\1577322582225.png)\n\n点击`创建主机`按钮之后，即可看见类似如下界面，为了更好的描述每个步骤，具体解释参靠下图后面的注释列表。\n\n![1577322873679](.\\1577322873679.png)\n\n`1`、我们可以在主机名的文本框中填写被监控主机的主机名称。\n\n`2`、**可见名称**一般使用剪短的、易读的、见名知义的名称表示主机即可。\n\n`3`、我们可以选择将要创建的主机属于哪个主机组，当然，如果没有合适的主机组，我们也可以直接在创建主机时，直接创建新的主机组，我上面说过，每个主机必须存在于某个主机组中，所以，主机组是必须的\n\n`4`、如果在三的`3`的位置上没有对应的、可用的、合适的主机组，我们可以直接在**新的群组**中创建当前主机需要的主机组。\n\n`5`、选择通过哪种接口监控当前主机，可选的方式有IPMI接口、JMX接口、SNMP接口、agent接口，我们说过，\"主机\"在zabbix中，可以是服务器，路由器，交换机等等硬件设备，有的硬件设备只支持某种接口，所以，当我们添加主机时，会让我们选择通过哪种合适的接口监控它，具体各接口的适用场景我们已经在第一篇介绍zabbix概念的文章中描述过，此处不再赘述，当然，如果一台主机能被多种接口所监控，也可以同时配置多个接口监控这台主机，但是当前，我们需要监控的主机是一台Linux服务器，而且已经安装了对应的agent端，所以，此处，我们只使用agent接口对当前主机进行监控，而使用agent接口时，可以通过IP连接到对应agent，也可以使用主机名连接到对应agent，而此处，我们选择使用IP地址连接到对应的agent，IP地址就是我们将要添加的主机的IP，47.97.172.176  ，对应端口为默认的10050，如果你想要使用主机名连接到对应的agent，那么需要保证主机名能够被正常解析到47.97.172.176上，此处不再赘述，如果有多个IP可以连接到对应agent，可以点击\"添加\"，添加一条新的IP。 \n\n`6`、对将要添加的主机进行描述，添加响应的描述信息即可。\n\n`7`、表示是否使用zabbix proxy监控当前主机，虽然上图中。此处翻译为“由agent代理程序检测”，但是实际是用于指定zabbix proxy的，与zabbix agent并没有关系，但是因为我们没有配置zabbix_proxy，所以此处保持默认即可\n\n好了，按照上述界面中的配置进行设置以后，点击\"添加\"按钮，即可简单的添加一台主机，可以看到，47.97.172.176已经被添加到了主机列表中。 \n\n![1577323895393](.\\1577323895393.png)\n\n而且，如果此时我们再次查看主机组，已经发现，TestHosts主机组已经被添加了，而且其中的成员已经包含了testzbx1主机。 \n\n![1577324037183](.\\1577324037183.png)\n\n回到主机列表，可以看到我们刚才添加的testzbx1主机，但是testzbx1主机的\"可用性\"对应的4中接口都是灰色的。 \n\n![1577324150680](.\\1577324150680.png)\n\n![1577324218152](.\\1577324218152.png)\n\n第二个图我们可以看见，何金存的主机已经成功的卑微监控了，\n\n上图中，而ZBX就代表agent接口，虽然我们在添加主机时，配置了通过agent监控对应主机，但是，由于我们并没有配置监控主机的任何指标，所以，ZBX仍然是灰色的，也就是说，我们现在只是将192.168.1.107加入了zabbix的监控范围，但是并没有对它进行任何实际的监控，因为我们还没有配置任何\"监控项\"，至于怎样配置监控项，且听下回分解。 \n\n#### 在Zabbix中添加监控项\n\n上面呢已经描述了zabbix添加主机，但是，我们还并没有对主机进行任何指标的实际监控那么现在，我们就来说说，具体怎样监控我们想要监控的指标。\n\n首先，打开我们zabbix控制台，点击`配置`---`主机`，可以看到我们上次创建的主机，虽然我们为对应的被监控主机安装了agent，但是主机对应的ZBX仍然显示灰色，代表我们还没有任何监控项被检测到，那么现在，我们来为“何金存”主机添加一个监控项。\n\n![1577330551358](/img/1577330551358.png)\n\n\n点击“王炎”主机上的`监控项`，如下图所示位置。\n\n![1577330697290](./img/1577330697290.png)\n\n进入监控项配置界面后，可以根据一些条件，筛选出已经存在的一些控制项，但是我们并没有任何监控项，所以此处 ，我们直接点击`创建监控项`按钮。以便新建监控项。\n\n![1577330817857](./img/1577330817857.png)\n\n假如，现在我们想要监控“何金存”这台主机的CPU的上下文切换此处，那么我们可以在此界面进行如下配置\n\n![1577330929623](./img/1577330929623.png)\n\n首先，在名称文本框中设置监控项的名称，我们此处监控的指标cpu上下文切换次数，所以，命名次监控项为“cpu context swiyches”\n\n因为我们在“何金存”这台主机上安装了zabbix agent，所以，此处类型保持默认，选择zabbix客户端。\n\n在键值一栏中，我们可以选择对应的key，也就是说，我们通过哪个key，这些key都是zabbix自带的key，这些key一般都是系统级别的通用的监控项所能够用到的key，如果这些“键”不能满足我们的需求，我们则需要自定义key，这是后话，后面再聊，此处，我们选择system/cpu.switches\n\n![1577331010819](./img/1577331010819.png)\n\n![1577331030747](./img/1577331030747.png)\n\n![1577331048400](./img/1577331048400.png)\n\n![1577331062325](./img/1577331062325.png)\n\n![1577331117868](./img/1577331117868.png)\n\n选择完可以看见，key的值已经自动填充到了“键值”的文本框中\n\n![1577331507814](C:\\Users\\Lenovo\\AppData\\Local\\Temp\\1577331507814.png)\n\n说到这，我们可以通过命令行，来看下对应的“键”返回信息到底是什么样子的？\n\n之前的我介绍过zabbix概念是已经说过：管理员可以在server端使用另一个名为zabbix_get的工具，测试是否能够从agent端拉取数据。\n\n![1577340453222](./img/1577340453222.png)\n\n我们就是通过agent接口监控数据的，agent监听在10050端口上，此处保持默认即可。\n\n而我们刚才也看到了，通过zabbix_get获取到的system.cpu.switches的数据，都是一些十进制的整数，所以，信息类型选择数字，数据类型选择十进制。\n\n数据更新间隔表示每个多长时间获取一次监控项对应的数据，为了演示方便，能尽快获取到数据，我们设定位每隔30秒获取一次监控信息，此处表示每隔30秒获取一次47.97.172.176主机的cpu上下文切换次数 ，但是需要注意，在生产环境中，如果不是特别重要的、敏感的、迅速变化的数据，不要获取的这么频繁，因为如果我们的监控项变得特别多时，获取信息的时间间隔过于频繁会带来巨大的监控压力，同时对数据库的写入也是一种考验。\n\n当然，我们也可以灵活的定义时间间隔，比如，周一到周五我们的业务量比较少，可以10分钟获取一次数据，而周六周日的业务量会剧增，为了实时监控，可以设置5分钟获取一次数据，这里只是举个例子，如果有类似的需求，可以通过“自定义事件间隔”配置段，添加不同时间段的不同检测频率。\n\n![1577341079573](./img/1577341079573.png)\n\n因为我们每隔30秒就获取一次数据，那么这些数据都会变成历史，存入数据库中，通过上图中的开始数据文本框，可以设置历史数据的保存时长。\n\n上图中，我们设置历史数据保存8天，趋势数据是什么意思呢？趋势数据就是每个小时收集到历史数。\n\n从上图中，还可以看到有一个趋势数据保存天数，趋势数据是什么意思呢？趋势数据就是每个小时收集到的历史数据中的最大值、最小值，平均值以及每个小时收集的到的历史数据的数据量，所以，趋势数据每小时收集一次，数据量不会特别大，一般情况下，历史数据的保留时间都比趋势数据保留时间短很多，因为历史数据比较多，如果我们监控的主机非常多，而且监控的频率 特别频繁，那么数据库的压力则会变得非常大。\n\n继续往下看，可以看到储存值于查看两个下拉框。\n\n![1577341472464](./img/1577341472464.png)\n\n我们点开储存值下拉框，可以看到三个选项，不变、差量（每秒频率）、差量（简单变化）\n\n![1577341562547](./img/1577341562547.png)\n\n那么这些值都是什么意思呢？\n\n`不变`：表示获取到的值是什么样子的，就在数据库中存储为什么样子。\n\n`差量（简单变化）`：表示本次收集的信息值 减去 上一次收集到的信息值 得出的差值\n\n`差量（每秒速率）`：表示本次收集到的值 减去 上次收集到的值以后，再除于两次收集信息的间隔时间。\n\n而此处，我们监控的指标为cpu上下文切换次数，这是一个不断增长的整数值，所以，我们选择**差量（每秒速率）**最合适\n\n这样发=我们就能 够监控到不同时间段内cpu上下文切换的频率了。\n\n那么查看值 是什么意思呢？查看值可以改变监控数据的展示方式，以便监控人员更容易理解，此处我们保持默认即可，在实际用到是我们在做解释。\n\n![1577341940128](./img/1577341940128.png)\n\n新的应用集  与  应用集  是什么意思呢？\n\n![1577341996936](./img/1577341996936.png)\n\n我们可以把“应用集”理解为同一类型的监控项的集合，“应用集”英文原词为application，application为一组item（监控项）的集合，比如，我们有3个监控项，他们分别监控“磁盘使用率”，“磁盘写入速率”，“磁盘读取速率”，虽然他们监控指标不同，但是他们都是监控“磁盘”的监控项 ，所以，我们可以把他们归类为“磁盘”应用集，同理，如果有2个监控项，一个是监控nginx连接数量的，一个是监控nginx请求数量的，虽然他们监控的指标不同，但是他们都是监控nginx相关指标的，所以，我们可以把他们归为nginx应用集。\n\n但是，由于我们没有创建过任何应用集，所以上图中，应用集选择框中没有任何可选择应用集，如果没有可选的合适的应用集，我们可以直接在“新的应用”文本框中填入要创建的应用集名称，那么对应应用集会自动被创建当前监控项 也会自动归类为这个应用集。\n\n![1577342433074](./img/1577342433074.png)\n\n继续聊，\"填入主机资产纪录栏位\"我们后面再聊。 \n\n描述信息栏填写关于这个监控项的相关描述。 \n\n\"已启用\"默认被勾选，表示此监控项被创建后，立即生效，即创建此监控项后立即开始监控。 \n\n好了，监控项的配置我们已经解释的七七八八了，示例配置如下，点击添加按钮, 注：为了更快的获取演示效果，此处将数据更新间隔设置为5秒，但是生产环境中请仔细考虑具体设置为多少秒比较适合生产环境的需求。 \n\n![1577349633413](./img/1577349633413.png)\n\n![1577342566211](./img/1577342566211.png)\n\n点击添加按钮以后，可以看到，何金存主机的第一个监控项已经被添加，而且处于已用状态。\n\n![1577342709516](./img/1577342709516.png)\n\n点击监控项旁边的“应用集”\n\n![1577342794093](./img/1577342794093.png)\n\n可以看到，应用集中已经存在了cpu应用集，而且这个应用集中已经存在一个监控项，就是我们刚才创建的\"cpu context switches\"监控项。 \n\n![1577343339745](./img/1577343339745.png)\n\n从对应的主机组中找到对应的主机，\n\n![1577344402324](./img/1577344402324.png)\n\n点击过滤按钮之后，应该可以看到我们刚才创建的监控项，已经存在了部分数据，如果你刚刚创建完监控项，不要着急立马查看“监控项”数据，因为他可能需要一段时间收集数据。\n\n但是，如果超出正常收集数据的时间后，很长时间以内仍然无法收集到数据，那么有可能 是因为agent端与server端时间不同步引起的，请确定你的agent端与server端的时间是同步的。\n\n过程中出现了一个问题\n\n![1577348901660](./img/1577348901660.png)\n\n![1577348933997](./img/1577348933997.png)\n\n这个问题，检查两处，如果其他问题也是首先想这两处。\n\n1、防火墙是否关闭\n\n2、进入/etc/zabbix/zabbix_agentd.conf配置文件，找到`Server`，把“127.0.0.1”改成监控服务器IP\n\n![1577349156980](./img/1577349156980.png)\n\n好，接着往下看啊\n\n可以看到，“cpu context switches”这个监控项已经存在数据，我们点击对应的“图形”连接\n\n![1577349426989](./img/1577349426989.png)\n\n点击上图中的`图形`连接，可以看到如下界面，zabbix已经监控到了对应的cpu上下文切换频率，并且绘制出了对应的“图形”\n\n![1577351286152](./img/1577351286152.png)\n\n如果没有图形就按照下图来操作即可。操作完之后还是没有图形，那就是没数据。\n\n![1577349916527](./img/1577349916527.png)\n\n![1577349869444](./img/1577349869444.png)\n\n![1577349793762](./img/1577349793762.png)\n\n我们已经为主机添加了第一个监控项，并且已经成功监控到了对应的数据，好了，我们已经入门了。 \n\n本文参考网站  --  <http://www.zsythink.net/archives/447> \n","tags":["运维"]},{"title":"JS的静态作用域、子程序引用环境与参数传递类型","url":"/2017/01/11/pout/前端技术/JS的静态作用域、子程序引用环境与参数传递类型/","content":"#### 静态作用域\n\n我们先来看下面这个小程序：\n\n```\n //JS版本：\n function sub1() {\n        var x;\n        function sub2() { alert(x); }\n        function sub3() { var x; x=3; sub4(sub2); }\n        function sub4(subx) { var x; x=4; subx(); }\n        x = 1;\n        sub3();\n    }\n\n    sub1();\n    \n #Python版本\ndef sub1():\n    def sub2():\n        print x\n    def sub3():\n        x=3\n        sub4(sub2)\n    def sub4(subx):\n        x=4\n        subx()\n    x = 1\n    sub3()\n\nsub1()   \n```\n\n不用亲自运行，实际上输出结果都是1，这可能不难猜到，但是需要解释一番，鉴于Python和JS在这一点上表现的类似，我就以JS来分析。\n\n我们知道，JS是静态作用域的，所谓静态作用域就是作用域在编译时确定，所以sub2中引用的x，实际上和x=3以及x=4的x没有任何关系，指向第二行的var x;\n\n#### 子程序的引用环境\n\n实际上这里面还有一个子程序(注：子程序和函数不是很一样，但我们可以认为子程序包括函数，也约等于函数)的概念，sub2、sub3、sub4都是子程序，对于允许嵌套子程序的语言，应该如何使用执行传递的子程序的引用环境？\n\n* 浅绑定：如果这样的话，应该输出4，这对动态作用域的语言来说比较自然。\n* 深绑定：也就是输出1的情况，这对静态作用域的语言来说比较自然。\n* Ad hoc binding: 这是第三种，将子程序作为实际参数传递到调用语句的环境。\n\n#### 参数传递类型\n\n参数传递类型我们普遍认为有按值传递和按引用传递两种，实际上不止。\n\n下面是一张图：\n\n![](https://www.10000h.top/images/call.png)\n\n这张图对应的第一种传递方式，叫做Pass-by-Value(In mode)，第二种是Pass-by-Result(Out mode)，第三种是Pass-by-Value-Result(Inout mode),图上说的比较明白，实际上如果有result就是说明最后把结果再赋值给参数。\n\n第二种和第三种编程语言用的少，原因如下：\n>Potential problem: sub(p1, p1)   \nWith the two corresponding formal parameters having different names, whichever formal parameter is copied back last will represent current value of p1\n\n","tags":["javascript"]},{"title":"CentOS7下安装和配置redis","url":"/2016/10/04/centos/CentOS7下安装和配置redis/","content":"Redis是一个高性能的，开源key-value型数据库。是构建高性能，可扩展的Web应用的完美解决方案，可以内存存储亦可持久化存储。因为要使用跨进程，跨服务级别的数据缓存，在对比多个方案后，决定使用Redis。顺便整理下Redis的安装过程，以便查阅。\n\n\n 1 . 下载Redis\n目前，最新的Redist版本为3.0，使用wget下载，命令如下：\n```\n\n# wget http://download.redis.io/releases/redis-3.0.4.tar.gz\n\n```\n 2 . 解压Redis\n下载完成后，使用tar命令解压下载文件：\n```\n\n# tar -xzvf redis-3.0.4.tar.gz\n```\n3 . 编译安装Redis\n切换至程序目录，并执行make命令编译：\n```\n# cd redis-3.0.4\n# make\n```\n执行安装命令\n```\n# make install\n```\nmake install安装完成后，会在/usr/local/bin目录下生成下面几个可执行文件，它们的作用分别是：\n\n* redis-server：Redis服务器端启动程序\n* redis-cli：Redis客户端操作工具。也可以用telnet根据其纯文本协议来操作\n* redis-benchmark：Redis性能测试工具\n* redis-check-aof：数据修复工具\n* redis-check-dump：检查导出工具\n\n备注\n\n有的机器会出现类似以下错误：\n```\nmake[1]: Entering directory `/root/redis/src'\nYou need tcl 8.5 or newer in order to run the Redis test\n……\n```\n这是因为没有安装tcl导致，yum安装即可：\n```\nyum install tcl\n```\n4 . 配置Redis\n复制配置文件到/etc/目录：\n```\n# cp redis.conf /etc/\n```\n为了让Redis后台运行，一般还需要修改redis.conf文件：\n```\nvi /etc/redis.conf\n```\n修改daemonize配置项为yes，使Redis进程在后台运行：\n```\ndaemonize yes\n```\n5 . 启动Redis\n配置完成后，启动Redis：\n```\n# cd /usr/local/bin\n# ./redis-server /etc/redis.conf\n```\n检查启动情况：\n```\n# ps -ef | grep redis\n```\n看到类似下面的一行，表示启动成功：\n```\nroot     18443     1  0 13:05 ?        00:00:00 ./redis-server *:6379 \n```\n6 . 添加开机启动项\n让Redis开机运行可以将其添加到rc.local文件，也可将添加为系统服务service。本文使用rc.local的方式，添加service请参考：Redis 配置为 Service 系统服务 。\n\n为了能让Redis在服务器重启后自动启动，需要将启动命令写入开机启动项：\n```\necho \"/usr/local/bin/redis-server /etc/redis.conf\" >>/etc/rc.local\n```\n7 . Redis配置参数\n在 前面的操作中，我们用到了使Redis进程在后台运行的参数，下面介绍其它一些常用的Redis启动参数：\n```\ndaemonize：是否以后台daemon方式运行\npidfile：pid文件位置\nport：监听的端口号\ntimeout：请求超时时间\nloglevel：log信息级别\nlogfile：log文件位置\ndatabases：开启数据库的数量\nsave * *：保存快照的频率，第一个*表示多长时间，第三个*表示执行多少次写操作。在一定时间内执行一定数量的写操作时，自动保存快照。可设置多个条件。\nrdbcompression：是否使用压缩\ndbfilename：数据快照文件名（只是文件名）\ndir：数据快照的保存目录（仅目录）\nappendonly：是否开启appendonlylog，开启的话每次写操作会记一条log，这会提高数据抗风险能力，但影响效率。\nappendfsync：appendonlylog如何同步到磁盘。三个选项，分别是每次写都强制调用fsync、每秒启用一次fsync、不调用fsync等待系统自己同步\n```\n","tags":["redis"]},{"title":"腾讯云北美服务器搭建ShadowSocks代理","url":"/2016/08/08/pout/后端技术/腾讯云北美服务器搭建ShadowSocks代理/","content":"\n注：本教程适合centos系列和red hat系列\n\n登陆SSH \n新的VPS可以先升级\n\n```\nyum -y update\n```\n\n有些VPS 没有wget \n这种要先装\n\n```\nyum -y install wget\n```\n\n输入以下命令：（可以复制）\n\n```\nwget --no-check-certificate https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.sh\nchmod +x shadowsocks.sh\n./shadowsocks.sh 2>&1 | tee shadowsocks.log\n```\n\n第一行是下载命令，下载东西，第二行是修改权限，第三行是安装命令\n\n下面是按照配置图\n\n```\n配置：\n密码：（默认是teddysun.com）\n端口：默认是8989\n然后按任意键安装，退出按 Ctrl+c\n```\n\n安装完成会有一个配置\n\n```\nCongratulations, shadowsocks install completed!Your Server IP:  ***** VPS的IP地址Your Server Port:  *****  你刚才设置的端口Your Password:  ****  你刚才设置的密码Your Local IP:  127.0.0.1 Your Local Port:  1080 Your Encryption Method:  aes-256-cfb Welcome to visit:https://teddysun.com/342.htmlEnjoy it!\n```\n\n然后即可以使用\n\n卸载方法：\n\n使用 root 用户登录，运行以下命令：\n\n```\n./shadowsocksR.sh uninstall\n```\n\n安装完成后即已后台启动 ShadowsocksR ，运行：\n\n```\n/etc/init.d/shadowsocks status\n```\n","tags":["ShadowSocks"]},{"title":"centOS7.2搭建nginx环境以及负载均衡","url":"/2016/08/03/centos/centOS7-2搭建nginx环境以及负载均衡/","content":" 之所以要整理出这篇文章，是因为1是搭建环境的过程中会遇到大大小小各种问题，2是网上目前也没有关于centos7.2搭建nginx环境的问题整理，因此在这里记录。\n\n前置工作就不赘述了，首先`ssh root@115.29.102.81` (换成你们自己的公网IP)登陆进入到自己的服务器命令行，之后开始基本的安装：\n\n**1.添加资源**\n\n添加CentOS 7 Nginx yum资源库,打开终端,使用以下命令(没有换行):\n\n```\nsudo rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpm\n\n```\n\n**2.安装Nginx**\n\n在你的CentOS 7 服务器中使用yum命令从Nginx源服务器中获取来安装Nginx：\n>*这里有一个需要注意的地方，尽量不要用网上的下载源码包然后再传到服务器上的方式进行安装，因为nginx已经不算是简单的Linux了，做了很多扩展，这个时候如果你用源码包安装会出现各种各样的问题，尽量用已经封装好的rpm\\yum进行安装*\n```\nsudo yum install -y nginx\n```\nNginx将完成安装在你的CentOS 7 服务器中。\n\n**3.启动Nginx**\n\n刚安装的Nginx不会自行启动。运行Nginx:\n```\nsudo systemctl start nginx.service\n```\n如果一切进展顺利的话，现在你可以通过你的域名或IP来访问你的Web页面来预览一下Nginx的默认页面\n\n>当然，这里一般很可能会无法访问的。\n\n我们先不急于解决我们的问题，先看看nginx的基本配置：\n\n\nNginx配置信息\n```\n网站文件存放默认目录\n\n/usr/share/nginx/html\n网站默认站点配置\n\n/etc/nginx/conf.d/default.conf\n自定义Nginx站点配置文件存放目录,自己在这里也可以定义别的名字的.conf，这个的作用以后再说。\n\n/etc/nginx/conf.d/\nNginx全局配置\n\n/etc/nginx/nginx.conf\n在这里你可以改变设置用户运行Nginx守护程序进程一样,和工作进程的数量得到了Nginx正在运行,等等。\n```\nLinux查看公网IP\n\n您可以运行以下命令来显示你的服务器的公共IP地址:(这个其实没用，不是公网IP)\n```\nip addr show eth0 | grep inet | awk '{ print $2; }' | sed 's/\\/.*$//'\n```\n___\n好了，这个时候我们再来看看可能遇到的问题：无法在公网访问。\n\n这个时候首先看看配置文件default.conf对不对，一个正确的例子：\n(域名要先进行解析到响应的IP)\n```\nserver {\n    listen       80;\n    server_name  nginx.310058.cn;\n\n    #charset koi8-r;\n    #access_log  /var/log/nginx/log/host.access.log  main;\n\n    location / {\n        root   /usr/share/nginx/html;\n        index  index.html index.htm;\n    }\n\n    #error_page  404              /404.html;\n\n    # redirect server error pages to the static page /50x.html\n    #\n    error_page   500 502 503 504  /50x.html;\n    location = /50x.html {\n        root   /usr/share/nginx/html;\n    }\n\n    # proxy the PHP scripts to Apache listening on 127.0.0.1:80\n    #\n    #location ~ \\.php$ {\n    #    proxy_pass   http://127.0.0.1;\n    #}\n\n    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000\n    #\n    #location ~ \\.php$ {\n    #    root           html;\n    #    fastcgi_pass   127.0.0.1:9000;\n    #    fastcgi_index  index.php;\n    #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;\n    #    include        fastcgi_params;\n    #}\n\n    # deny access to .htaccess files, if Apache's document root\n    # concurs with nginx's one\n    #\n    #location ~ /\\.ht {\n    #    deny  all;\n    #}\n}\n```\n\n确定文件没问题了，看看这个时候是不是开启了nginx进程：\n\n```\n ps -ef | grep nginx\n```\n\n应该会输出一个或者多个进程，如果没有的话就开启或者重启试试看。\n\n这个时候接下来再试试在服务器上：\n```\nping  115.29.102.81\ntelnet 115.29.102.81 80\nwget nginx.310058.cn\n```\n如果有的命令没有就直接yum安装下:\n```\nyum -y install telnet\n```\n如果都可以的话，之后在本机尝试以上三行。如果没有命令也要安装下：\n```\nbrew install wget\n```\n\n发现很可能本机telnet不通，而服务器telnet通。\n这个时候就是**防火墙**的问题。\n\n####centos7.2防火墙\n\n由于centos 7版本以后默认使用firewalld后，网上关于iptables的设置方法已经不管用了，所以根本就别想用配置iptables做啥，根本没用。\n\n查看下防火墙状态：\n```\n[root@iZ28dcsp7egZ conf.d]# systemctl status firewalld  \n● firewalld.service - firewalld - dynamic firewall daemon\n   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)\n   Active: active (running) since Wed 2016-08-03 12:06:44 CST; 2h 49min ago\n Main PID: 424 (firewalld)\n   CGroup: /system.slice/firewalld.service\n           └─424 /usr/bin/python -Es /usr/sbin/firewalld --nofork --nopid\n\nAug 03 12:06:41 iZ28dcsp7egZ systemd[1]: Starting firewalld - dynamic firewall daemon...\nAug 03 12:06:44 iZ28dcsp7egZ systemd[1]: Started firewalld - dynamic firewall daemon.\n```\n\n增加80端口的权限：\n```\nfirewall-cmd --zone=public --add-port=80/tcp --permanent  \n```\n \n 别忘了更新防火墙的配置：\n```\nfirewall-cmd --reload\n```\n这个时候再`restart  nginx.service` 一下就会发现应该好了。\n\n\nnginx 停止：\n\n```\nservice nginx restart\n也可以重启nginx\n\nkill -QUIT 进程号  \n#从容停止\n\nkill -TERM 进程号\n#或者\nkill -INT 进程号\n#快速停止\n\np-kill -9 nginx\n强制停止\n\nnginx -t \n#验证配置文件 前提是进入相应的配置的目录（自己实际测试的时候发现没有进入相应的配置目录也是可以的）\n\nnginx -s reload\n#重启\n\nkill -HUP 进程号\n#重启的另外一种方式\n```\n\n官方文档地址：\nhttps://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Security_Guide/sec-Using_Firewalls.html#sec-Introduction_to_firewalld\n\n附1:一个简单的负载均衡的实现:\nweight默认是1，自己也可以更改。\n```\nupstream mypro {\n\t\t\t\tip_hash;\n                server 111.13.100.92 weight=2;\n                server 183.232.41.1;\n                server 42.156.140.7;\n                }\n\n        server {\n                listen 8090;\n                location / {\n                proxy_pass http://mypro;\n                }\n        }\n\n```\n\n\n附2:防火墙基本学习：\n\n``` \n\n1、firewalld简介\nfirewalld是centos7的一大特性，最大的好处有两个：支持动态更新，不用重启服务；第二个就是加入了防火墙的“zone”概念\n \nfirewalld有图形界面和工具界面，由于我在服务器上使用，图形界面请参照官方文档，本文以字符界面做介绍\n \nfirewalld的字符界面管理工具是 firewall-cmd \n \nfirewalld默认配置文件有两个：/usr/lib/firewalld/ （系统配置，尽量不要修改）和 /etc/firewalld/ （用户配置地址）\n \nzone概念：\n硬件防火墙默认一般有三个区，firewalld引入这一概念系统默认存在以下区域（根据文档自己理解，如果有误请指正）：\ndrop：默认丢弃所有包\nblock：拒绝所有外部连接，允许内部发起的连接\npublic：指定外部连接可以进入\nexternal：这个不太明白，功能上和上面相同，允许指定的外部连接\ndmz：和硬件防火墙一样，受限制的公共连接可以进入\nwork：工作区，概念和workgoup一样，也是指定的外部连接允许\nhome：类似家庭组\ninternal：信任所有连接\n对防火墙不算太熟悉，还没想明白public、external、dmz、work、home从功能上都需要自定义允许连接，具体使用上的区别还需高人指点\n \n2、安装firewalld\nroot执行 # yum install firewalld firewall-config\n \n3、运行、停止、禁用firewalld\n启动：# systemctl start  firewalld\n查看状态：# systemctl status firewalld 或者 firewall-cmd --state\n停止：# systemctl disable firewalld\n禁用：# systemctl stop firewalld\n \n4、配置firewalld\n查看版本：$ firewall-cmd --version\n查看帮助：$ firewall-cmd --help\n查看设置：\n                显示状态：$ firewall-cmd --state\n                查看区域信息: $ firewall-cmd --get-active-zones\n                查看指定接口所属区域：$ firewall-cmd --get-zone-of-interface=eth0\n拒绝所有包：# firewall-cmd --panic-on\n取消拒绝状态：# firewall-cmd --panic-off\n查看是否拒绝：$ firewall-cmd --query-panic\n \n更新防火墙规则：# firewall-cmd --reload\n                            # firewall-cmd --complete-reload\n    两者的区别就是第一个无需断开连接，就是firewalld特性之一动态添加规则，第二个需要断开连接，类似重启服务\n \n将接口添加到区域，默认接口都在public\n# firewall-cmd --zone=public --add-interface=eth0\n永久生效再加上 --permanent 然后reload防火墙\n \n设置默认接口区域\n# firewall-cmd --set-default-zone=public\n立即生效无需重启\n \n打开端口（貌似这个才最常用）\n查看所有打开的端口：\n# firewall-cmd --zone=dmz --list-ports\n加入一个端口到区域：\n# firewall-cmd --zone=dmz --add-port=8080/tcp\n若要永久生效方法同上\n \n打开一个服务，类似于将端口可视化，服务需要在配置文件中添加，/etc/firewalld 目录下有services文件夹，这个不详细说了，详情参考文档\n# firewall-cmd --zone=work --add-service=smtp\n \n移除服务\n# firewall-cmd --zone=work --remove-service=smtp\n \n还有端口转发功能、自定义复杂规则功能、lockdown，由于还没用到，以后再学习\n\n```\n","tags":["nginx"]}]